{
    "models": [
        {
            "model_name": "albert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\albert.md",
            "release_date": "2019-09-26",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[ALBERT](https://huggingface.co/papers/1909.11942) is designed to address memory limitations of scaling and training of [BERT](./bert). It adds two parameter reduction techniques. The first, factorized embedding parametrization, splits the larger vocabulary embedding matrix into two smaller matrices so you can grow the hidden size without adding a lot more parameters. The second, cross-layer parameter sharing, allows layer to share parameters which keeps the number of learnable parameters lower.\n\nALBERT was created to address problems like -- GPU/TPU memory limitations, longer training times, and unexpected model degradation in BERT. ALBERT uses two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT:\n\n- **Factorized embedding parameterization:** The large vocabulary embedding matrix is decomposed into two smaller matrices, reducing memory consumption.\n- **Cross-layer parameter sharing:** Instead of learning separate parameters for each...",
            "tasks": [],
            "display_name": "ALBERT"
        },
        {
            "model_name": "bart",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bart.md",
            "release_date": "2019-10-29",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BART](https://huggingface.co/papers/1910.13461) is a sequence-to-sequence model that combines the pretraining objectives from BERT and GPT. It's pretrained by corrupting text in different ways like deleting words, shuffling sentences, or masking tokens and learning how to fix it. The encoder encodes the corrupted document and the corrupted text is fixed by the decoder. As it learns to recover the original text, BART gets really good at both understanding and generating language.\n\nYou can find all the original BART checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=bart) organization.\n\nThe example below demonstrates how to predict the `[MASK]` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"facebook/bart-large\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Plants create  through a process known as...",
            "tasks": [],
            "display_name": "BART"
        },
        {
            "model_name": "bert-generation",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bert-generation.md",
            "release_date": "2019-07-29",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BertGeneration](https://huggingface.co/papers/1907.12461) leverages pretrained BERT checkpoints for sequence-to-sequence tasks with the [`EncoderDecoderModel`] architecture. BertGeneration adapts the [`BERT`] for generative tasks.\n\nYou can find all the original BERT checkpoints under the [BERT](https://huggingface.co/collections/google/bert-release-64ff5e7a4be99045d1896dbc) collection.\n\n> [!TIP]\n> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n>\n> Click on the BertGeneration models in the right sidebar for more examples of how to apply BertGeneration to different sequence generation tasks.\n\nThe example below demonstrates how to use BertGeneration with [`EncoderDecoderModel`] for sequence-to-sequence tasks.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"text2text-generation\",\n    model=\"google/roberta2roberta_L-24_discofuse\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Plants create energy...",
            "tasks": [],
            "display_name": "BertGeneration"
        },
        {
            "model_name": "bert-japanese",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bert-japanese.md",
            "release_date": "2019-03-24",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "There are models with two different tokenization methods:\n\n- Tokenize with MeCab and WordPiece. This requires some extra dependencies, [fugashi](https://github.com/polm/fugashi) which is a wrapper around [MeCab](https://taku910.github.io/mecab/).\n- Tokenize into characters.\n\nTo use *MecabTokenizer*, you should `pip install transformers[\"ja\"]` (or `pip install -e .[\"ja\"]` if you install\nfrom source) to install dependencies.\n\nSee [details on cl-tohoku repository](https://github.com/cl-tohoku/bert-japanese).\n\nExample of using a model with MeCab and WordPiece tokenization:\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> bertjapanese = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n\n>>> ## Input Japanese Text\n>>> line = \"\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u3002\"\n\n>>> inputs = tokenizer(line, return_tensors=\"pt\")\n\n>>> print(tokenizer.decode(inputs[\"input_ids\"][0]))\n[CLS] \u543e\u8f29 \u306f \u732b \u3067 \u3042\u308b \u3002 [SEP]\n\n>>>...",
            "tasks": [],
            "display_name": "BertJapanese"
        },
        {
            "model_name": "bert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bert.md",
            "release_date": "2018-10-11",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BERT](https://huggingface.co/papers/1810.04805) is a bidirectional transformer pretrained on unlabeled text to predict masked tokens in a sentence and to predict whether one sentence follows another. The main idea is that by randomly masking some tokens, the model can train on text to the left and right, giving it a more thorough understanding. BERT is also very versatile because its learned language representations can be adapted for other NLP tasks by fine-tuning an additional layer or head.\n\nYou can find all the original BERT checkpoints under the [BERT](https://huggingface.co/collections/google/bert-release-64ff5e7a4be99045d1896dbc) collection.\n\n> [!TIP]\n> Click on the BERT models in the right sidebar for more examples of how to apply BERT to different language tasks.\n\nThe example below demonstrates how to predict the `[MASK]` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   ...",
            "tasks": [],
            "display_name": "BERT"
        },
        {
            "model_name": "bertweet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bertweet.md",
            "release_date": "2020-05-20",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BERTweet](https://huggingface.co/papers/2005.10200) shares the same architecture as [BERT-base](./bert), but it's pretrained like [RoBERTa](./roberta) on English Tweets. It performs really well on Tweet-related tasks like part-of-speech tagging, named entity recognition, and text classification.\n\nYou can find all the original BERTweet checkpoints under the [VinAI Research](https://huggingface.co/vinai?search_models=BERTweet) organization.\n\n> [!TIP]\n> Refer to the [BERT](./bert) docs for more examples of how to apply BERTweet to different language tasks.\n\nThe example below demonstrates how to predict the `` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"vinai/bertweet-base\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Plants create  through a process known as photosynthesis.\")\n```\n\n\n\n\n```py\nimport torch\nfrom transformers import AutoModelForMaskedLM,...",
            "tasks": [],
            "display_name": "BERTweet"
        },
        {
            "model_name": "blenderbot",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\blenderbot.md",
            "release_date": "2020-04-28",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that\nscaling neural models in the number of parameters and the size of the data they are trained on gives improved results,\nwe show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of\nskills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to\ntheir partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent\npersona. We show that large scale models can learn these skills when given...",
            "tasks": [],
            "display_name": "Blenderbot"
        },
        {
            "model_name": "camembert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\camembert.md",
            "release_date": "2019-11-10",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[CamemBERT](https://huggingface.co/papers/1911.03894) is a language model based on [RoBERTa](./roberta), but trained specifically on French text from the OSCAR dataset, making it more effective for French language tasks.\n\nWhat sets CamemBERT apart is that it learned from a huge, high quality collection of French data, as opposed to mixing lots of languages. This helps it really understand French better than many multilingual models.\n\nCommon applications of CamemBERT include masked language modeling (Fill-mask prediction), text classification (sentiment analysis), token classification (entity recognition) and sentence pair classification (entailment tasks).\n\nYou can find all the original CamemBERT checkpoints under the [ALMAnaCH](https://huggingface.co/almanach/models?search=camembert) organization.\n\n> [!TIP]\n> This model was contributed by the [ALMAnaCH (Inria)](https://huggingface.co/almanach) team.\n>\n> Click on the CamemBERT models in the right sidebar for more examples of how to...",
            "tasks": [],
            "display_name": "CamemBERT"
        },
        {
            "model_name": "ctrl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ctrl.md",
            "release_date": "2019-09-11",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "CTRL model was proposed in [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://huggingface.co/papers/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and\nRichard Socher. It's a causal (unidirectional) transformer pre-trained using language modeling on a very large corpus\nof ~140 GB of text data with the first token reserved as a control code (such as Links, Books, Wikipedia etc.).\n\nThe abstract from the paper is the following:\n\n*Large-scale language models show promising text generation capabilities, but users cannot easily control particular\naspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model,\ntrained to condition on control codes that govern style, content, and task-specific behavior. Control codes were\nderived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while\nproviding more explicit control...",
            "tasks": [],
            "display_name": "CTRL"
        },
        {
            "model_name": "deberta",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deberta.md",
            "release_date": "2020-06-05",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[DeBERTa](https://huggingface.co/papers/2006.03654) improves the pretraining efficiency of BERT and RoBERTa with two key ideas, disentangled attention and an enhanced mask decoder. Instead of mixing everything together like BERT, DeBERTa separates a word's *content* from its *position* and processes them independently. This gives it a clearer sense of what's being said and where in the sentence it's happening.\n\nThe enhanced mask decoder replaces the traditional softmax decoder to make better predictions.\n\nEven with less training data than RoBERTa, DeBERTa manages to outperform it on several benchmarks.\n\nYou can find all the original DeBERTa checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=deberta) organization.\n\n> [!TIP]\n> Click on the DeBERTa models in the right sidebar for more examples of how to apply DeBERTa to different language tasks.\n\nThe example below demonstrates how to classify text with [`Pipeline`], [`AutoModel`], and from the command...",
            "tasks": [],
            "display_name": "DeBERTa"
        },
        {
            "model_name": "dialogpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dialogpt.md",
            "release_date": "2019-11-01",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "DialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://huggingface.co/papers/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,\nJianfeng Gao, Jingjing Liu, Bill Dolan. It's a GPT2 Model trained on 147M conversation-like exchanges extracted from\nReddit.\n\nThe abstract from the paper is the following:\n\n*We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained\ntransformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning\nfrom 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human\nboth in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems\nthat leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong...",
            "tasks": [],
            "display_name": "DialoGPT"
        },
        {
            "model_name": "distilbert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\distilbert.md",
            "release_date": "2019-10-02",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[DistilBERT](https://huggingface.co/papers/1910.01108) is pretrained by knowledge distillation to create a smaller model with faster inference and requires less compute to train. Through a triple loss objective during pretraining, language modeling loss, distillation loss, cosine-distance loss, DistilBERT demonstrates similar performance to a larger transformer language model.\n\nYou can find all the original DistilBERT checkpoints under the [DistilBERT](https://huggingface.co/distilbert) organization.\n\n> [!TIP]\n> Click on the DistilBERT models in the right sidebar for more examples of how to apply DistilBERT to different language tasks.\n\nThe example below demonstrates how to classify text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n\n```py\nfrom transformers import pipeline\n\nclassifier = pipeline(\n    task=\"text-classification\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    dtype=torch.float16,\n    device=0\n)\n\nresult = classifier(\"I love using...",
            "tasks": [],
            "display_name": "DistilBERT"
        },
        {
            "model_name": "dpr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dpr.md",
            "release_date": "2020-04-10",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was\nintroduced in [Dense Passage Retrieval for Open-Domain Question Answering](https://huggingface.co/papers/2004.04906) by\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.\n\nThe abstract from the paper is the following:\n\n*Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional\nsparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can\nbe practically implemented using dense representations alone, where embeddings are learned from a small number of\nquestions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,\nour dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage\nretrieval accuracy, and...",
            "tasks": [],
            "display_name": "DPR"
        },
        {
            "model_name": "electra",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\electra.md",
            "release_date": "2020-03-23",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[ELECTRA](https://huggingface.co/papers/2003.10555) modifies the pretraining objective of traditional masked language models like BERT. Instead of just masking tokens and asking the model to predict them, ELECTRA trains two models, a generator and a discriminator. The generator replaces some tokens with plausible alternatives and the discriminator (the model you'll actually use) learns to detect which tokens are original and which were replaced. This training approach is very efficient and scales to larger models while using considerably less compute.\n\nThis approach is super efficient because ELECTRA learns from every single token in the input, not just the masked ones. That's why even the small ELECTRA models can match or outperform much larger models while using way less computing resources.\n\nYou can find all the original ELECTRA checkpoints under the [ELECTRA](https://huggingface.co/collections/google/electra-release-64ff6e8b18830fabea30a1ab) release.\n\n> [!TIP]\n> Click on the right...",
            "tasks": [],
            "display_name": "ELECTRA"
        },
        {
            "model_name": "encoder-decoder",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\encoder-decoder.md",
            "release_date": "2017-06-12",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[`EncoderDecoderModel`](https://huggingface.co/papers/1706.03762) initializes a sequence-to-sequence model with any pretrained autoencoder and pretrained autoregressive model. It is effective for sequence generation tasks as demonstrated in [Text Summarization with Pretrained Encoders](https://huggingface.co/papers/1908.08345) which uses [`BertModel`] as the encoder and decoder.\n\n> [!TIP]\n> This model was contributed by [thomwolf](https://huggingface.co/thomwolf).\n>\n> Click on the Encoder Decoder models in the right sidebar for more examples of how to apply Encoder Decoder to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\n    \"summarization\",\n    model=\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\",\n    device=0\n)\n\ntext = \"Plants create energy through a process known as photosynthesis. This involves capturing sunlight and...",
            "tasks": [],
            "display_name": "Encoder Decoder Models"
        },
        {
            "model_name": "flaubert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\flaubert.md",
            "release_date": "2019-12-11",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The FlauBERT model was proposed in the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://huggingface.co/papers/1912.05372) by Hang Le et al. It's a transformer model pretrained using a masked language\nmodeling (MLM) objective (like BERT).\n\nThe abstract from the paper is the following:\n\n*Language models have become a key step to achieve state-of-the art results in many different Natural Language\nProcessing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way\nto pre-train continuous word representations that can be fine-tuned for a downstream task, along with their\ncontextualization at the sentence level. This has been widely demonstrated for English using contextualized\nrepresentations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large...",
            "tasks": [],
            "display_name": "FlauBERT"
        },
        {
            "model_name": "fsmt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\fsmt.md",
            "release_date": "2019-07-15",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "FSMT (FairSeq MachineTranslation) models were introduced in [Facebook FAIR's WMT19 News Translation Task Submission](https://huggingface.co/papers/1907.06616) by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov.\n\nThe abstract of the paper is the following:\n\n*This paper describes Facebook FAIR's submission to the WMT19 shared news translation task. We participate in two\nlanguage pairs and four language directions, English  German and English  Russian. Following our submission from\nlast year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling\ntoolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,\nas well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific\ndata, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the\nhuman evaluation...",
            "tasks": [],
            "display_name": "FSMT"
        },
        {
            "model_name": "funnel",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\funnel.md",
            "release_date": "2020-06-05",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Funnel Transformer model was proposed in the paper [Funnel-Transformer: Filtering out Sequential Redundancy for\nEfficient Language Processing](https://huggingface.co/papers/2006.03236). It is a bidirectional transformer model, like\nBERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks\n(CNN) in computer vision.\n\nThe abstract from the paper is the following:\n\n*With the success of language pretraining, it is highly desirable to develop more efficient architectures of good\nscalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the\nmuch-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only\nrequire a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which\ngradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost....",
            "tasks": [],
            "display_name": "Funnel Transformer"
        },
        {
            "model_name": "gpt2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt2.md",
            "release_date": "2019-02-14",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) is a scaled up version of GPT, a causal transformer language model, with 10x more parameters and training data. The model was pretrained on a 40GB dataset to predict the next word in a sequence based on all the previous words. This approach enabled the model to perform many downstream tasks in a zero-shot setting. The blog post released by OpenAI can be found [here](https://openai.com/index/better-language-models/).\n\nThe model architecture uses a unidirectional (causal) attention mechanism where each token can only attend to previous tokens, making it particularly effective for text generation tasks.\n\nYou can find all the original GPT-2 checkpoints under the [OpenAI community](https://huggingface.co/openai-community?search_models=gpt) organization.\n\n> [!TIP]\n> Click on the GPT-2 models in the right sidebar for more examples of how to apply GPT-2 to different language...",
            "tasks": [],
            "display_name": "GPT-2"
        },
        {
            "model_name": "herbert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\herbert.md",
            "release_date": "2020-05-01",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The HerBERT model was proposed in [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://huggingface.co/papers/2005.00630) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and\nIreneusz Gawlik. It is a BERT-based Language Model trained on Polish Corpora using only MLM objective with dynamic\nmasking of whole words.\n\nThe abstract from the paper is the following:\n\n*In recent years, a series of Transformer-based models unlocked major improvements in general natural language\nunderstanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which\nallow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of\nlanguages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language\nunderstanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing\ndatasets for named entity recognition,...",
            "tasks": [],
            "display_name": "HerBERT"
        },
        {
            "model_name": "layoutlm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\layoutlm.md",
            "release_date": "2019-12-31",
            "transformers_date": "2020-11-16",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[LayoutLM](https://huggingface.co/papers/1912.13318) jointly learns text and the document layout rather than focusing only on text. It incorporates positional layout information and visual features of words from the document images.\n\nYou can find all the original LayoutLM checkpoints under the [LayoutLM](https://huggingface.co/collections/microsoft/layoutlm-6564539601de72cb631d0902) collection.\n\n> [!TIP]\n> Click on the LayoutLM models in the right sidebar for more examples of how to apply LayoutLM to different vision and language tasks.\n\nThe example below demonstrates question answering with the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, LayoutLMForQuestionAnswering\n\ntokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\nmodel = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", dtype=torch.float16)\n\ndataset = load_dataset(\"nielsr/funsd\",...",
            "tasks": [],
            "display_name": "LayoutLM"
        },
        {
            "model_name": "longformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\longformer.md",
            "release_date": "2020-04-10",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Longformer](https://huggingface.co/papers/2004.05150) is a transformer model designed for processing long documents. The self-attention operation usually scales quadratically with sequence length, preventing transformers from processing longer sequences. The Longformer attention mechanism overcomes this by scaling linearly with sequence length. It combines local windowed attention with task-specific global attention, enabling efficient processing of documents with thousands of tokens.\n\nYou can find all the original Longformer checkpoints under the [Ai2](https://huggingface.co/allenai?search_models=longformer) organization.\n\n> [!TIP]\n> Click on the Longformer models in the right sidebar for more examples of how to apply Longformer to different language tasks.\n\nThe example below demonstrates how to fill the `` token with [`Pipeline`], [`AutoModel`] and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n   ...",
            "tasks": [],
            "display_name": "Longformer"
        },
        {
            "model_name": "lxmert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\lxmert.md",
            "release_date": "2019-08-20",
            "transformers_date": "2020-11-16",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://huggingface.co/papers/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\n(one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\ncombination of masked language modeling, visual-language text alignment, ROI-feature regression, masked\nvisual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining\nconsists of multiple multi-modal datasets: MSCOCO, Visual-Genome + Visual-Genome Question Answering, VQA 2.0, and GQA.\n\nThe abstract from the paper is the following:\n\n*Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly,\nthe alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality\nEncoder Representations from...",
            "tasks": [],
            "display_name": "LXMERT"
        },
        {
            "model_name": "marian",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\marian.md",
            "release_date": "2018-04-01",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[MarianMT](https://huggingface.co/papers/1804.00344) is a machine translation model trained with the Marian framework which is written in pure C++. The framework includes its own custom auto-differentiation engine and efficient meta-algorithms to train encoder-decoder models like BART.\n\nAll MarianMT models are transformer encoder-decoders with 6 layers in each component, use static sinusoidal positional embeddings, don't have a layernorm embedding, and the model starts generating with the prefix `pad_token_id` instead of ``.\n\nYou can find all the original MarianMT checkpoints under the [Language Technology Research Group at the University of Helsinki](https://huggingface.co/Helsinki-NLP/models?search=opus-mt) organization.\n\n> [!TIP]\n> This model was contributed by [sshleifer](https://huggingface.co/sshleifer).\n>\n> Click on the MarianMT models in the right sidebar for more examples of how to apply MarianMT to translation tasks.\n\nThe example below demonstrates how to translate text...",
            "tasks": [],
            "display_name": "MarianMT"
        },
        {
            "model_name": "mbart",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mbart.md",
            "release_date": "2020-01-22",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[mBART](https://huggingface.co/papers/2001.08210) is a multilingual machine translation model that pretrains the entire translation model (encoder-decoder) unlike previous methods that only focused on parts of the model. The model is trained on a denoising objective which reconstructs the corrupted text. This allows mBART to handle the source language and the target text to translate to.\n\n[mBART-50](https://huggingface.co/paper/2008.00401) is pretrained on an additional 25 languages.\n\nYou can find all the original mBART checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=mbart) organization.\n\n> [!TIP]\n> Click on the mBART models in the right sidebar for more examples of applying mBART to different language tasks.\n\nThe example below demonstrates how to translate text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"translation\",\n   ...",
            "tasks": [],
            "display_name": "mBART"
        },
        {
            "model_name": "mobilebert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mobilebert.md",
            "release_date": "2020-04-06",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[MobileBERT](https://huggingface.co/papers/2004.02984) is a lightweight and efficient variant of BERT, specifically designed for resource-limited devices such as mobile phones. It retains BERT's architecture but significantly reduces model size and inference latency while maintaining strong performance on NLP tasks. MobileBERT achieves this through a bottleneck structure and carefully balanced self-attention and feedforward networks. The model is trained by knowledge transfer from a large BERT model with an inverted bottleneck structure.\n\nYou can find the original MobileBERT checkpoint under the [Google](https://huggingface.co/google/mobilebert-uncased) organization.\n> [!TIP]\n> Click on the MobileBERT models in the right sidebar for more examples of how to apply MobileBERT to different language tasks.\n\nThe example below demonstrates how to predict the `[MASK]` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import...",
            "tasks": [],
            "display_name": "MobileBERT"
        },
        {
            "model_name": "pegasus",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pegasus.md",
            "release_date": "2019-12-18",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Pegasus](https://huggingface.co/papers/1912.08777) is an encoder-decoder (sequence-to-sequence) transformer model pretrained on unlabeled text to perform abstractive summarization. Pegasus is trained jointly on two self-supervised objective functions, masked language modeling (MLM) and gap sentence generation (GSG). Whole sentences are masked and the model has to fill in the gaps in the document. It can be fine-tuned with good performance even on small datasets with only 1000 examples.\n\nYou can find all the original Pegasus checkpoints under the [Google](https://huggingface.co/google?search_models=pegasus) organization.\n\n> [!TIP]\n> Click on the Pegasus models in the right sidebar for more examples of how to apply Pegasus to different language tasks.\n\nThe example below demonstrates how to summarize text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"summarization\",\n   ...",
            "tasks": [],
            "display_name": "Pegasus"
        },
        {
            "model_name": "phobert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\phobert.md",
            "release_date": "2020-03-02",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The PhoBERT model was proposed in [PhoBERT: Pre-trained language models for Vietnamese](https://huggingface.co/papers/2003.00744) by Dat Quoc Nguyen, Anh Tuan Nguyen.\n\nThe abstract from the paper is the following:\n\n*We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual\nlanguage models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent\nbest pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple\nVietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and\nNatural language inference.*\n\nThis model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/PhoBERT).\n\n## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> phobert =...",
            "tasks": [],
            "display_name": "PhoBERT"
        },
        {
            "model_name": "prophetnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\prophetnet.md",
            "release_date": "2020-01-13",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\nZhang, Ming Zhou on 13 Jan, 2020.\n\nProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of just\nthe next token.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel\nself-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of\nthe optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by\nn-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time\nstep. The future n-gram prediction explicitly encourages the model to...",
            "tasks": [],
            "display_name": "ProphetNet"
        },
        {
            "model_name": "rag",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\rag.md",
            "release_date": "2020-05-22",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Retrieval-Augmented Generation (RAG)](https://huggingface.co/papers/2005.11401) combines a pretrained language model (parametric memory) with access to an external data source (non-parametric memory) by means of a pretrained neural retriever. RAG fetches relevant passages and conditions its generation on them during inference. This often makes the answers more factual and lets you update knowledge by changing the index instead of retraining the whole model.\n\nYou can find all the original RAG checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=rag) organization.\n\n> [!TIP]\n> This model was contributed by [ola13](https://huggingface.co/ola13).\n>\n> Click on the RAG models in the right sidebar for more examples of how to apply RAG to different language tasks.\n\nThe examples below demonstrates how to generate text with [`AutoModel`].\n\n\n\n\n```py\nimport torch\nfrom transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n\ntokenizer =...",
            "tasks": [],
            "display_name": "RAG"
        },
        {
            "model_name": "reformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\reformer.md",
            "release_date": "2020-01-13",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya.\n\nThe abstract from the paper is the following:\n\n*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can\nbe prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of\nTransformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its\ncomplexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual\nlayers instead of the standard residuals, which allows storing activations only once in the training process instead of\nN times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models\nwhile being much more memory-efficient and much faster on long...",
            "tasks": [],
            "display_name": "Reformer"
        },
        {
            "model_name": "roberta",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\roberta.md",
            "release_date": "2019-07-26",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[RoBERTa](https://huggingface.co/papers/1907.11692) improves BERT with new pretraining objectives, demonstrating [BERT](./bert) was undertrained and training design is important. The pretraining objectives include dynamic masking, sentence packing, larger batches and a byte-level BPE tokenizer.\n\nYou can find all the original RoBERTa checkpoints under the [Facebook AI](https://huggingface.co/FacebookAI) organization.\n\n> [!TIP]\n> Click on the RoBERTa models in the right sidebar for more examples of how to apply RoBERTa to different language tasks.\n\nThe example below demonstrates how to predict the `` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"FacebookAI/roberta-base\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Plants create  through a process known as photosynthesis.\")\n```\n\n\n\n\n```py\nimport torch\nfrom transformers import AutoModelForMaskedLM,...",
            "tasks": [],
            "display_name": "RoBERTa"
        },
        {
            "model_name": "squeezebert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\squeezebert.md",
            "release_date": "2020-06-19",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://huggingface.co/papers/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:\n\n*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social...",
            "tasks": [],
            "display_name": "SqueezeBERT"
        },
        {
            "model_name": "t5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\t5.md",
            "release_date": "2019-10-23",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[T5](https://huggingface.co/papers/1910.10683) is a encoder-decoder transformer available in a range of sizes from 60M to 11B parameters. It is designed to handle a wide range of NLP tasks by treating them all as text-to-text problems. This eliminates the need for task-specific architectures because T5 converts every NLP task into a text generation task.\n\nTo formulate every task as text generation, each task is prepended with a task-specific prefix (e.g., translate English to German: ..., summarize: ...). This enables T5 to handle tasks like translation, summarization, question answering, and more.\n\nYou can find all official T5 checkpoints under the [T5](https://huggingface.co/collections/google/t5-release-65005e7c520f8d7b4d037918) collection.\n\n> [!TIP]\n> Click on the T5 models in the right sidebar for more examples of how to apply T5 to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and how to translate with T5 from...",
            "tasks": [],
            "display_name": "T5"
        },
        {
            "model_name": "xlm-roberta",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlm-roberta.md",
            "release_date": "2019-11-05",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[XLM-RoBERTa](https://huggingface.co/papers/1911.02116) is a large multilingual masked language model trained on 2.5TB of filtered CommonCrawl data across 100 languages. It shows that scaling the model provides strong performance gains on high-resource and low-resource languages. The model uses the [RoBERTa](./roberta) pretraining objectives on the [XLM](./xlm) model.\n\nYou can find all the original XLM-RoBERTa checkpoints under the [Facebook AI community](https://huggingface.co/FacebookAI) organization.\n\n> [!TIP]\n> Click on the XLM-RoBERTa models in the right sidebar for more examples of how to apply XLM-RoBERTa to different cross-lingual tasks like classification, translation, and question answering.\n\nThe example below demonstrates how to predict the `` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"FacebookAI/xlm-roberta-base\",\n   ...",
            "tasks": [],
            "display_name": "XLM-RoBERTa"
        },
        {
            "model_name": "xlm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlm.md",
            "release_date": "2019-01-22",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[XLM](https://huggingface.co/papers/1901.07291) demonstrates cross-lingual pretraining with two approaches, unsupervised training on a single language and supervised training on more than one language with a cross-lingual language model objective. The XLM model supports the causal language modeling objective, masked language modeling, and translation language modeling (an extension of the [BERT](./bert)) masked language modeling objective to multiple language inputs).\n\nYou can find all the original XLM checkpoints under the [Facebook AI community](https://huggingface.co/FacebookAI?search_models=xlm-mlm) organization.\n\n> [!TIP]\n> Click on the XLM models in the right sidebar for more examples of how to apply XLM to different cross-lingual tasks like classification, translation, and question answering.\n\nThe example below demonstrates how to predict the `` token with [`Pipeline`], [`AutoModel`] and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import...",
            "tasks": [],
            "display_name": "XLM"
        },
        {
            "model_name": "xlnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlnet.md",
            "release_date": "2019-06-19",
            "transformers_date": "2020-11-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://huggingface.co/papers/1906.08237) by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nQuoc V. Le. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn\nbidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization\norder.\n\nThe abstract from the paper is the following:\n\n*With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves\nbetter performance than pretraining approaches based on autoregressive language modeling. However, relying on\ncorrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a\npretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables...",
            "tasks": [],
            "display_name": "XLNet"
        },
        {
            "model_name": "mt5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mt5.md",
            "release_date": "2020-10-22",
            "transformers_date": "2020-11-17",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[mT5](https://huggingface.co/papers/2010.11934) is a multilingual variant of [T5](./t5), training on 101 languages. It also incorporates a new \"accidental translation\" technique to prevent the model from incorrectly translating predictions into the wrong language.\n\nYou can find all the original [mT5] checkpoints under the [mT5](https://huggingface.co/collections/google/mt5-release-65005f1a520f8d7b4d039509) collection.\n\n> [!TIP]\n> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n>\n> Click on the mT5 models in the right sidebar for more examples of how to apply mT5 to different language tasks.\n\nThe example below demonstrates how to summarize text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"text2text-generation\",\n    model=\"csebuetnlp/mT5_multilingual_XLSum\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"\"\"Plants are remarkable...",
            "tasks": [],
            "display_name": "mT5"
        },
        {
            "model_name": "barthez",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\barthez.md",
            "release_date": "2020-10-23",
            "transformers_date": "2020-11-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BARThez](https://huggingface.co/papers/2010.12321) is a [BART](./bart) model designed for French language tasks. Unlike existing French BERT models, BARThez includes a pretrained encoder-decoder, allowing it to generate text as well. This model is also available as a multilingual variant, mBARThez, by continuing pretraining multilingual BART on a French corpus.\n\nYou can find all of the original BARThez checkpoints under the [BARThez](https://huggingface.co/collections/dascim/barthez-670920b569a07aa53e3b6887) collection.\n\n> [!TIP]\n> This model was contributed by [moussakam](https://huggingface.co/moussakam).\n> Refer to the [BART](./bart) docs for more usage examples.\n\nThe example below demonstrates how to predict the `` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"moussaKam/barthez\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Les plantes...",
            "tasks": [],
            "display_name": "BARThez"
        },
        {
            "model_name": "mpnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mpnet.md",
            "release_date": "2020-04-20",
            "transformers_date": "2020-12-09",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MPNet model was proposed in [MPNet: Masked and Permuted Pre-training for Language Understanding](https://huggingface.co/papers/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n\nMPNet adopts a novel pre-training method, named masked and permuted language modeling, to inherit the advantages of\nmasked language modeling and permuted language modeling for natural language understanding.\n\nThe abstract from the paper is the following:\n\n*BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models.\nSince BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and\nthus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids...",
            "tasks": [],
            "display_name": "MPNet"
        },
        {
            "model_name": "tapas",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\tapas.md",
            "release_date": "2020-04-05",
            "transformers_date": "2020-12-15",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The TAPAS model was proposed in [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://huggingface.co/papers/2004.02349)\nby Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno and Julian Martin Eisenschlos. It's a BERT-based model specifically\ndesigned (and pre-trained) for answering questions about tabular data. Compared to BERT, TAPAS uses relative position embeddings and has 7\ntoken types that encode tabular structure. TAPAS is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising\nmillions of tables from English Wikipedia and corresponding texts.\n\nFor question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for (optionally) performing aggregations (such as counting or summing) among selected cells. TAPAS has been fine-tuned on several datasets:\n\n- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n-...",
            "tasks": [],
            "display_name": "TAPAS"
        },
        {
            "model_name": "blenderbot-small",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\blenderbot-small.md",
            "release_date": "2020-04-28",
            "transformers_date": "2021-01-05",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[`BlenderbotSmallForConditionalGeneration`] are only used in combination with the checkpoint\n[facebook/blenderbot-90M](https://huggingface.co/facebook/blenderbot-90M). Larger Blenderbot checkpoints should\ninstead be used with [`BlenderbotModel`] and\n[`BlenderbotForConditionalGeneration`]\n\n## Overview\n\nThe Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\nJing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n\nThe abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that\nscaling neural models in the number of parameters and the size of the data they are trained on gives improved results,\nwe show that other ingredients are important for a high-performing chatbot. Good conversation requires a number...",
            "tasks": [],
            "display_name": "Blenderbot Small"
        },
        {
            "model_name": "led",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\led.md",
            "release_date": "2020-04-10",
            "transformers_date": "2021-01-05",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Longformer-Encoder-Decoder (LED)](https://huggingface.co/papers/2004.05150) is an encoder-decoder  transformer model for sequence-to-sequence tasks like summarization. It extends [Longformer](.longformer), an encoder-only model designed to handle long inputs, by adding a decoder layer. The decoder uses full self-attention on the encoded tokens and previously decoded locations. Because of Longformer's linear self-attention mechanism, LED is more efficient than standard encoder-decoder models when processing long sequences.\n\nYou can find all the original [LED] checkpoints under the [Ai2](https://huggingface.co/allenai/models?search=led) organization.\n\n> [!TIP]\n> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n>\n> Click on the LED models in the right sidebar for more examples of how to apply LED to different language tasks.\n\nThe example below demonstrates how to summarize text with [`Pipeline`], [`AutoModel`], and from the command...",
            "tasks": [],
            "display_name": "LED"
        },
        {
            "model_name": "convbert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\convbert.md",
            "release_date": "2020-08-06",
            "transformers_date": "2021-01-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://huggingface.co/papers/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\nYan.\n\nThe abstract from the paper is the following:\n\n*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads,...",
            "tasks": [],
            "display_name": "ConvBERT"
        },
        {
            "model_name": "wav2vec2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\wav2vec2.md",
            "release_date": "2020-06-20",
            "transformers_date": "2021-02-02",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://huggingface.co/papers/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k...",
            "tasks": [],
            "display_name": "Wav2Vec2"
        },
        {
            "model_name": "deberta-v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deberta-v2.md",
            "release_date": "2020-06-05",
            "transformers_date": "2021-02-19",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[DeBERTa-v2](https://huggingface.co/papers/2006.03654) improves on the original [DeBERTa](./deberta) architecture by using a SentencePiece-based tokenizer and a new vocabulary size of 128K. It also adds an additional convolutional layer within the first transformer layer to better learn local dependencies of input tokens. Finally, the position projection and content projection matrices are shared in the attention layer to reduce the number of parameters.\n\nYou can find all the original [DeBERTa-v2] checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=deberta-v2) organization.\n\n> [!TIP]\n> This model was contributed by [Pengcheng He](https://huggingface.co/DeBERTa).\n>\n> Click on the DeBERTa-v2 models in the right sidebar for more examples of how to apply DeBERTa-v2 to different language tasks.\n\nThe example below demonstrates how to classify text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline =...",
            "tasks": [],
            "display_name": "DeBERTa-v2"
        },
        {
            "model_name": "ibert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ibert.md",
            "release_date": "2021-01-05",
            "transformers_date": "2021-02-26",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://huggingface.co/papers/2101.01321) by\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney and Kurt Keutzer. It's a quantized version of RoBERTa running\ninference up to four times faster.\n\nThe abstract from the paper is the following:\n\n*Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language\nProcessing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive for\nefficient inference at the edge, and even at the data center. While quantization can be a viable solution for this,\nprevious work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot\nefficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM\nprocessors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models...",
            "tasks": [],
            "display_name": "I-BERT"
        },
        {
            "model_name": "m2m_100",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\m2m_100.md",
            "release_date": "2020-10-21",
            "transformers_date": "2021-03-06",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\nSiddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n\nThe abstract from the paper is the following:\n\n*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a\nsingle model able to translate between any pair of languages. However, much of this work is English-Centric by training\nonly on data which was translated from or to English. While this is supported by large sources of training data, it\ndoes not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation\nmodel that can translate directly between any pair of 100 languages. We build and open...",
            "tasks": [],
            "display_name": "M2M100"
        },
        {
            "model_name": "speech_to_text",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\speech_to_text.md",
            "release_date": "2020-10-11",
            "transformers_date": "2021-03-10",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Speech2Text model was proposed in [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It's a\ntransformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech\nTranslation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are\nfed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the\ntranscripts/translations autoregressively. Speech2Text has been fine-tuned on several datasets for ASR and ST:\n[LibriSpeech](http://www.openslr.org/12), [CoVoST 2](https://github.com/facebookresearch/covost), [MuST-C](https://ict.fbk.eu/must-c/).\n\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found...",
            "tasks": [],
            "display_name": "Speech2Text"
        },
        {
            "model_name": "big_bird",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\big_bird.md",
            "release_date": "2020-07-28",
            "transformers_date": "2021-03-30",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BigBird](https://huggingface.co/papers/2007.14062) is a transformer model built to handle sequence lengths up to 4096 compared to 512 for [BERT](./bert). Traditional transformers struggle with long inputs because attention gets really expensive as the sequence length grows. BigBird fixes this by using a sparse attention mechanism, which means it doesn\u2019t try to look at everything at once. Instead, it mixes in local attention, random attention, and a few global tokens to process the whole input. This combination gives it the best of both worlds. It keeps the computation efficient while still capturing enough of the sequence to understand it well. Because of this, BigBird is great at tasks involving long documents, like question answering, summarization, and genomic applications.\n\nYou can find all the original BigBird checkpoints under the [Google](https://huggingface.co/google?search_models=bigbird) organization.\n\n> [!TIP]\n> Click on the BigBird models in the right sidebar for more...",
            "tasks": [],
            "display_name": "BigBird"
        },
        {
            "model_name": "gpt_neo",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt_neo.md",
            "release_date": "2021-03-21",
            "transformers_date": "2021-03-30",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[GPT-Neo](https://zenodo.org/records/5297715) is an open-source alternative to GPT-2 and GPT-3 models, built with Mesh TensorFlow for TPUs. GPT-Neo uses local attention in every other layer for more efficiency. It is trained on the [Pile](https://huggingface.co/datasets/EleutherAI/pile), a diverse dataset consisting of 22 smaller high-quality datasets. The original github repository can be found [here](https://github.com/EleutherAI/gpt-neo/tree/v1.1)\n\nYou can find all the original GPT-Neo checkpoints under the [EleutherAI](https://huggingface.co/EleutherAI?search_models=gpt-neo) organization.\n\n> [!TIP]\n> Click on the GPT-Neo models in the right sidebar for more examples of how to apply GPT Neo to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"EleutherAI/gpt-neo-1.3B\",...",
            "tasks": [],
            "display_name": "Gpt Neo"
        },
        {
            "model_name": "vit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vit.md",
            "release_date": "2020-10-22",
            "transformers_date": "2021-04-01",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Vision Transformer (ViT)](https://huggingface.co/papers/2010.11929) is a transformer adapted for computer vision tasks. An image is split into smaller fixed-sized patches which are treated as a sequence of tokens, similar to words for NLP tasks. ViT requires less resources to pretrain compared to convolutional architectures and its performance on large datasets can be transferred to smaller downstream tasks.\n\nYou can find all the original ViT checkpoints under the [Google](https://huggingface.co/google?search_models=vit) organization.\n\n> [!TIP]\n> Click on the ViT models in the right sidebar for more examples of how to apply ViT to different computer vision tasks.\n\nThe example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"image-classification\",\n    model=\"google/vit-base-patch16-224\",\n    dtype=torch.float16,\n   ...",
            "tasks": [],
            "display_name": "Vision Transformer (ViT)"
        },
        {
            "model_name": "megatron-bert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\megatron-bert.md",
            "release_date": "2019-09-17",
            "transformers_date": "2021-04-08",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MegatronBERT model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\nParallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper and Bryan Catanzaro.\n\nThe abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in\nNatural Language Processing applications. However, very large models can be quite difficult to train due to memory\nconstraints. In this work, we present our techniques for training very large transformer models and implement a simple,\nefficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our\napproach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with the insertion of a few communication operations...",
            "tasks": [],
            "display_name": "MegatronBERT"
        },
        {
            "model_name": "cpm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\cpm.md",
            "release_date": "2020-12-01",
            "transformers_date": "2021-04-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The CPM model was proposed in [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://huggingface.co/papers/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,\nYusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,\nDaixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n\nThe abstract from the paper is the following:\n\n*Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3,\nwith 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even\nzero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus\nof GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the\nChinese Pre-trained Language Model (CPM)...",
            "tasks": [],
            "display_name": "CPM"
        },
        {
            "model_name": "deit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deit.md",
            "release_date": "2020-12-23",
            "transformers_date": "2021-04-13",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://huggingface.co/papers/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, Herv\u00e9 J\u00e9gou. The [Vision Transformer (ViT)](vit) introduced in [Dosovitskiy et al., 2020](https://huggingface.co/papers/2010.11929) has shown that one can match or even outperform existing convolutional neural\nnetworks using a Transformer encoder (BERT-like). However, the ViT models introduced in that paper required training on\nexpensive infrastructure for multiple weeks, using external data. DeiT (data-efficient image transformers) are more\nefficiently trained transformers for image classification, requiring far less data and far less computing resources\ncompared to the original ViT models.\n\nThe abstract from the paper is the following:\n\n*Recently, neural networks purely based on attention were shown to address image understanding tasks such as...",
            "tasks": [],
            "display_name": "DeiT"
        },
        {
            "model_name": "luke",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\luke.md",
            "release_date": "2020-10-02",
            "transformers_date": "2021-05-03",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto.\nIt is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps\nimprove performance on various downstream tasks involving reasoning about entities such as named entity recognition,\nextractive and cloze-style question answering, entity typing, and relation classification.\n\nThe abstract from the paper is the following:\n\n*Entity representations are useful in natural language tasks involving entities. In this paper, we propose new\npretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed\nmodel treats words and entities in a given text as independent tokens, and outputs contextualized representations of\nthem. Our model is trained using a new...",
            "tasks": [],
            "display_name": "LUKE"
        },
        {
            "model_name": "bigbird_pegasus",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bigbird_pegasus.md",
            "release_date": "2020-07-28",
            "transformers_date": "2021-05-07",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BigBirdPegasus](https://huggingface.co/papers/2007.14062) is an encoder-decoder (sequence-to-sequence) transformer model for long-input summarization. It extends the [BigBird](./big_bird) architecture with an additional pretraining objective borrowed from [Pegasus](./pegasus) called gap sequence generation (GSG). Whole sentences are masked and the model has to fill in the gaps in the document. BigBirdPegasus's ability to keep track of long contexts makes it effective at summarizing lengthy inputs, surpassing the performance of base Pegasus models.\n\nYou can find all the original BigBirdPegasus checkpoints under the [Google](https://huggingface.co/google/models?search=bigbird-pegasus) organization.\n\n> [!TIP]\n> This model was contributed by [vasudevgupta](https://huggingface.co/vasudevgupta).\n>\n> Click on the BigBirdPegasus models in the right sidebar for more examples of how to apply BigBirdPegasus to different language tasks.\n\nThe example below demonstrates how to summarize text with...",
            "tasks": [],
            "display_name": "BigBirdPegasus"
        },
        {
            "model_name": "clip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\clip.md",
            "release_date": "2021-02-26",
            "transformers_date": "2021-05-12",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[CLIP](https://huggingface.co/papers/2103.00020) is a is a multimodal vision and language model motivated by overcoming the fixed number of object categories when training a computer vision model. CLIP learns about images directly from raw text by jointly training on 400M (image, text) pairs. Pretraining on this scale enables zero-shot transfer to downstream tasks. CLIP uses an image encoder and text encoder to get visual features and text features. Both features are projected to a latent space with the same number of dimensions and their dot product gives a similarity score.\n\nYou can find all the original CLIP checkpoints under the [OpenAI](https://huggingface.co/openai?search_models=clip) organization.\n\n> [!TIP]\n> Click on the CLIP models in the right sidebar for more examples of how to apply CLIP to different image and language tasks.\n\nThe example below demonstrates how to calculate similarity scores between multiple text descriptions and an image with [`Pipeline`] or the...",
            "tasks": [],
            "display_name": "CLIP"
        },
        {
            "model_name": "roformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\roformer.md",
            "release_date": "2021-04-20",
            "transformers_date": "2021-05-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[RoFormer](https://huggingface.co/papers/2104.09864) introduces Rotary Position Embedding (RoPE) to encode token positions by rotating the inputs in 2D space. This allows a model to track absolute positions and model relative relationships. RoPE can scale to longer sequences, account for the natural decay of token dependencies, and works with the more efficient linear self-attention.\n\nYou can find all the RoFormer checkpoints on the [Hub](https://huggingface.co/models?search=roformer).\n\n> [!TIP]\n> Click on the RoFormer models in the right sidebar for more examples of how to apply RoFormer to different language tasks.\n\nThe example below demonstrates how to predict the `[MASK]` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\n# uncomment to install rjieba which is needed for the tokenizer\n# !pip install rjieba\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"fill-mask\",\n    model=\"junnyu/roformer_chinese_base\",\n   ...",
            "tasks": [],
            "display_name": "RoFormer"
        },
        {
            "model_name": "byt5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\byt5.md",
            "release_date": "2021-05-28",
            "transformers_date": "2021-06-01",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[ByT5](https://huggingface.co/papers/2105.13626) is tokenizer-free version of the [T5](./t5) model designed to works directly on raw UTF-8 bytes. This means it can process any language, more robust to noise like typos, and simpler to use because it doesn't require a preprocessing pipeline.\n\nYou can find all the original ByT5 checkpoints under the [Google](https://huggingface.co/google?search_models=byt5) organization.\n\n> [!TIP]\n> Refer to the [T5](./t5) docs for more examples of how to apply ByT5 to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"text2text-generation\",\n    model=\"google/byt5-small\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"translate English to French: The weather is nice today\")\n```\n\n\n\n\n```python\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM,...",
            "tasks": [],
            "display_name": "ByT5"
        },
        {
            "model_name": "visual_bert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\visual_bert.md",
            "release_date": "2019-08-09",
            "transformers_date": "2021-06-02",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[VisualBERT](https://huggingface.co/papers/1908.03557) is a vision-and-language model. It uses an approach called \"early fusion\", where inputs are fed together into a single Transformer stack initialized from [BERT](./bert). Self-attention implicitly aligns words with their corresponding image objects. It processes text with visual features from object-detector regions instead of raw pixels.\n\nYou can find all the original VisualBERT checkpoints under the [UCLA NLP](https://huggingface.co/uclanlp/models?search=visualbert) organization.\n\n> [!TIP]\n> This model was contributed by [gchhablani](https://huggingface.co/gchhablani).\n> Click on the VisualBERT models in the right sidebar for more examples of how to apply VisualBERT to different image and language tasks.\n\nThe example below demonstrates how to answer a question based on an image with the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nimport torchvision\nfrom PIL import Image\nimport numpy as np\nfrom transformers import AutoTokenizer,...",
            "tasks": [],
            "display_name": "VisualBERT"
        },
        {
            "model_name": "detr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\detr.md",
            "release_date": "2020-05-26",
            "transformers_date": "2021-06-09",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[DETR](https://huggingface.co/papers/2005.12872) consists of a convolutional backbone followed by an encoder-decoder Transformer which can be trained end-to-end for object detection. It greatly simplifies a lot of the complexity of models like Faster-R-CNN and Mask-R-CNN, which use things like region proposals, non-maximum suppression procedure and anchor generation. Moreover, DETR can also be naturally extended to perform panoptic segmentation, by simply adding a mask head on top of the decoder outputs.\n\nYou can find all the original DETR checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=detr) organization.\n\n> [!TIP]\n> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n>\n> Click on the DETR models in the right sidebar for more examples of how to apply DETR to different object detection and segmentation tasks.\n\nThe example below demonstrates how to perform object detection with the [`Pipeline`] or the [`AutoModel`]...",
            "tasks": [],
            "display_name": "DETR"
        },
        {
            "model_name": "hubert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\hubert.md",
            "release_date": "2021-06-14",
            "transformers_date": "2021-06-16",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[HuBERT](https://huggingface.co/papers/2106.07447) is a self-supervised speech model to cluster aligned target labels for BERT-like prediction loss and applying the prediction loss only over masked regions to force the model to learn both acoustic and language modeling over continuous inputs. It addresses the challenges of multiple sound units per utterance, no lexicon during pre-training, and variable-length sound units without explicit segmentation.\n\nYou can find all the original HuBERT checkpoints under the [HuBERT](https://huggingface.co/collections/facebook/hubert-651fca95d57549832161e6b6) collection.\n\n> [!TIP]\n> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n>\n> Click on the HuBERT models in the right sidebar for more examples of how to apply HuBERT to different audio tasks.\n\nThe example below demonstrates how to automatically transcribe speech into text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom...",
            "tasks": [],
            "display_name": "HuBERT"
        },
        {
            "model_name": "canine",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\canine.md",
            "release_date": "2021-03-11",
            "transformers_date": "2021-06-30",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[CANINE](https://huggingface.co/papers/2103.06874) is a tokenization-free Transformer. It skips the usual step of splitting text into subwords or wordpieces and processes text character by character. That means it works directly with raw Unicode, making it especially useful for languages with complex or inconsistent tokenization rules and even noisy inputs like typos. Since working with characters means handling longer sequences, CANINE uses a smart trick. The model compresses the input early on (called downsampling) so the transformer doesn't have to process every character individually. This keeps things fast and efficient.\n\nYou can find all the original CANINE checkpoints under the [Google](https://huggingface.co/google?search_models=canine) organization.\n\n> [!TIP]\n> Click on the CANINE models in the right sidebar for more examples of how to apply CANINE to different language tasks.\n\nThe example below demonstrates how to generate embeddings with [`Pipeline`], [`AutoModel`], and...",
            "tasks": [],
            "display_name": "CANINE"
        },
        {
            "model_name": "rembert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\rembert.md",
            "release_date": "2020-10-24",
            "transformers_date": "2021-07-24",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://huggingface.co/papers/2010.12821) by Hyung Won Chung, Thibault F\u00e9vry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\n\nThe abstract from the paper is the following:\n\n*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on\nstandard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\nfine-tuning stage even though the output...",
            "tasks": [],
            "display_name": "RemBERT"
        },
        {
            "model_name": "beit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\beit.md",
            "release_date": "2021-06-15",
            "transformers_date": "2021-08-04",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) by\nHangbo Bao, Li Dong and Furu Wei. Inspired by BERT, BEiT is the first paper that makes self-supervised pre-training of\nVision Transformers (ViTs) outperform supervised pre-training. Rather than pre-training the model to predict the class\nof an image (as done in the [original ViT paper](https://huggingface.co/papers/2010.11929)), BEiT models are pre-trained to\npredict visual tokens from the codebook of OpenAI's [DALL-E model](https://huggingface.co/papers/2102.12092) given masked\npatches.\n\nThe abstract from the paper is the following:\n\n*We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation\nfrom Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image\nmodeling task to pretrain vision Transformers. Specifically, each image has two views in our...",
            "tasks": [],
            "display_name": "BEiT"
        },
        {
            "model_name": "splinter",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\splinter.md",
            "release_date": "2021-01-02",
            "transformers_date": "2021-08-17",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Splinter model was proposed in [Few-Shot Question Answering by Pretraining Span Selection](https://huggingface.co/papers/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy. Splinter\nis an encoder-only transformer (similar to BERT) pretrained using the recurring span selection task on a large corpus\ncomprising Wikipedia and the Toronto Book Corpus.\n\nThe abstract from the paper is the following:\n\nIn several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order\nof 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred\ntraining examples are available, and observe that standard models perform poorly, highlighting the discrepancy between\ncurrent pretraining objectives and question answering. We propose a new pretraining scheme tailored for question\nanswering: recurring span selection. Given a passage with multiple sets of recurring spans, we...",
            "tasks": [],
            "display_name": "Splinter"
        },
        {
            "model_name": "layoutlmv2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\layoutlmv2.md",
            "release_date": "2020-12-29",
            "transformers_date": "2021-08-30",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://huggingface.co/papers/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,\nDinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. LayoutLMV2 improves [LayoutLM](layoutlm) to obtain\nstate-of-the-art results across several document image understanding benchmarks:\n\n- information extraction from scanned documents: the [FUNSD](https://guillaumejaume.github.io/FUNSD/) dataset (a\n  collection of 199 annotated forms comprising more than 30,000 words), the [CORD](https://github.com/clovaai/cord)\n  dataset (a collection of 800 receipts for training, 100 for validation and 100 for testing), the [SROIE](https://rrc.cvc.uab.es/?ch=13) dataset (a collection of 626 receipts for training and 347 receipts for testing)\n  and the [Kleister-NDA](https://github.com/applicaai/kleister-nda) dataset (a collection of non-disclosure\n ...",
            "tasks": [],
            "display_name": "LayoutLMV2"
        },
        {
            "model_name": "gptj",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gptj.md",
            "release_date": "2021-06-04",
            "transformers_date": "2021-08-31",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like\ncausal language model trained on [the Pile](https://pile.eleuther.ai/) dataset.\n\nThis model was contributed by [Stella Biderman](https://huggingface.co/stellaathena).\n\n## Usage tips\n\n- To load [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) in float32 one would need at least 2x model size\n  RAM: 1x for initial weights and another 1x to load the checkpoint. So for GPT-J it would take at least 48GB\n  RAM to just load the model. To reduce the RAM usage there are a few options. The `dtype` argument can be\n  used to initialize the model in half-precision on a CUDA device only. There is also a fp16 branch which stores the fp16 weights,\n  which could be used to further minimize the RAM usage:\n\n```python\n>>> from transformers import...",
            "tasks": [],
            "display_name": "GPT-J"
        },
        {
            "model_name": "speech-encoder-decoder",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\speech-encoder-decoder.md",
            "release_date": "2021-04-14",
            "transformers_date": "2021-09-01",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [`SpeechEncoderDecoderModel`] can be used to initialize a speech-to-text model\nwith any pretrained speech autoencoding model as the encoder (*e.g.* [Wav2Vec2](wav2vec2), [Hubert](hubert)) and any pretrained autoregressive model as the decoder.\n\nThe effectiveness of initializing speech-sequence-to-text-sequence models with pretrained checkpoints for speech\nrecognition and speech translation has *e.g.* been shown in [Large-Scale Self- and Semi-Supervised Learning for Speech\nTranslation](https://huggingface.co/papers/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli,\nAlexis Conneau.\n\nAn example of how to use a [`SpeechEncoderDecoderModel`] for inference can be seen in [Speech2Text2](speech_to_text_2).\n\n## Randomly initializing `SpeechEncoderDecoderModel` from model configurations.\n\n[`SpeechEncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default...",
            "tasks": [],
            "display_name": "Speech Encoder Decoder Models"
        },
        {
            "model_name": "fnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\fnet.md",
            "release_date": "2021-05-09",
            "transformers_date": "2021-09-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://huggingface.co/papers/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\n*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we...",
            "tasks": [],
            "display_name": "FNet"
        },
        {
            "model_name": "megatron_gpt2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\megatron_gpt2.md",
            "release_date": "2019-09-17",
            "transformers_date": "2021-10-01",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MegatronGPT2 model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\nParallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper and Bryan Catanzaro.\n\nThe abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in\nNatural Language Processing applications. However, very large models can be quite difficult to train due to memory\nconstraints. In this work, we present our techniques for training very large transformer models and implement a simple,\nefficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our\napproach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with the insertion of a few communication operations...",
            "tasks": [],
            "display_name": "MegatronGPT2"
        },
        {
            "model_name": "trocr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\trocr.md",
            "release_date": "2021-09-21",
            "transformers_date": "2021-10-13",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[TrOCR](https://huggingface.co/papers/2109.10282) is a text recognition model for both image understanding and text generation. It doesn't require separate models for image processing or character generation. TrOCR is a simple single end-to-end system that uses a transformer to handle visual understanding and text generation.\n\nYou can find all the original TrOCR checkpoints under the [Microsoft](https://huggingface.co/microsoft/models?search=trocr) organization.\n\n\n TrOCR architecture. Taken from the original paper. \n\n> [!TIP]\n> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n>\n> Click on the TrOCR models in the right sidebar for more examples of how to apply TrOCR to different image and text tasks.\n\nThe example below demonstrates how to perform optical character recognition (OCR) with the [`AutoModel`] class.\n\n\n\n\n```python\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nimport requests\nfrom PIL import Image\n\nprocessor =...",
            "tasks": [],
            "display_name": "TrOCR"
        },
        {
            "model_name": "vision-encoder-decoder",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vision-encoder-decoder.md",
            "release_date": "2021-09-21",
            "transformers_date": "2021-10-13",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [`VisionEncoderDecoderModel`] can be used to initialize an image-to-text model with any\npretrained Transformer-based vision model as the encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit), [Swin](swin))\nand any pretrained language model as the decoder (*e.g.* [RoBERTa](roberta), [GPT2](gpt2), [BERT](bert), [DistilBERT](distilbert)).\n\nThe effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for\nexample) [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/papers/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\nZhoujun Li, Furu Wei.\n\nAfter such a [`VisionEncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like any other models (see the examples below\nfor more information).\n\nAn example application is image captioning, in which the encoder is used to encode the image, after which an autoregressive language model...",
            "tasks": [],
            "display_name": "Vision Encoder Decoder Models"
        },
        {
            "model_name": "sew-d",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\sew-d.md",
            "release_date": "2021-09-14",
            "transformers_date": "2021-10-15",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "SEW-D (Squeezed and Efficient Wav2Vec with Disentangled attention) was proposed in [Performance-Efficiency Trade-offs\nin Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim,\nJing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n\nThe abstract from the paper is the following:\n\n*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition\n(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance\nand its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a\npre-trained model architecture with significant improvements along both performance and efficiency dimensions across a\nvariety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction...",
            "tasks": [],
            "display_name": "SEW-D"
        },
        {
            "model_name": "sew",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\sew.md",
            "release_date": "2021-09-14",
            "transformers_date": "2021-10-15",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "SEW (Squeezed and Efficient Wav2Vec) was proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training\nfor Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q.\nWeinberger, Yoav Artzi.\n\nThe abstract from the paper is the following:\n\n*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition\n(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance\nand its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a\npre-trained model architecture with significant improvements along both performance and efficiency dimensions across a\nvariety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a...",
            "tasks": [],
            "display_name": "SEW"
        },
        {
            "model_name": "bartpho",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bartpho.md",
            "release_date": "2021-09-20",
            "transformers_date": "2021-10-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BARTpho](https://huggingface.co/papers/2109.09701) is a large-scale Vietnamese sequence-to-sequence model. It offers a word-based and syllable-based version. This model is built on the [BART](./bart) large architecture with its denoising pretraining.\n\nYou can find all the original checkpoints under the [VinAI](https://huggingface.co/vinai/models?search=bartpho) organization.\n\n> [!TIP]\n> This model was contributed by [dqnguyen](https://huggingface.co/dqnguyen).\n> Check out the right sidebar for examples of how to apply BARTpho to different language tasks.\n\nThe example below demonstrates how to summarize text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   task=\"summarization\",\n   model=\"vinai/bartpho-word\",\n   dtype=torch.float16,\n   device=0\n)\n\ntext = \"\"\"\nQuang t\u1ed5ng h\u1ee3p hay g\u1ecdi t\u1eaft l\u00e0 quang h\u1ee3p l\u00e0 qu\u00e1 tr\u00ecnh thu nh\u1eadn v\u00e0 chuy\u1ec3n h\u00f3a n\u0103ng l\u01b0\u1ee3ng \u00e1nh s\u00e1ng M\u1eb7t tr\u1eddi c\u1ee7a th\u1ef1c v\u1eadt,\nt\u1ea3o v\u00e0 m\u1ed9t s\u1ed1 vi khu\u1ea9n \u0111\u1ec3 t\u1ea1o...",
            "tasks": [],
            "display_name": "BARTpho"
        },
        {
            "model_name": "unispeech-sat",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\unispeech-sat.md",
            "release_date": "2021-10-12",
            "transformers_date": "2021-10-26",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The UniSpeech-SAT model was proposed in [UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware\nPre-Training](https://huggingface.co/papers/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen,\nShujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu .\n\nThe abstract from the paper is the following:\n\n*Self-supervised learning (SSL) is a long-standing goal for speech processing, since it utilizes large-scale unlabeled\ndata and avoids extensive human labeling. Recent years witness great successes in applying self-supervised learning in\nspeech recognition, while limited exploration was attempted in applying SSL for modeling speaker characteristics. In\nthis paper, we aim to improve the existing SSL framework for speaker representation learning. Two methods are\nintroduced for enhancing the unsupervised speaker information extraction. First, we apply the multi-task learning to\nthe current SSL framework, where we integrate the utterance-wise...",
            "tasks": [],
            "display_name": "UniSpeech-SAT"
        },
        {
            "model_name": "unispeech",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\unispeech.md",
            "release_date": "2021-01-19",
            "transformers_date": "2021-10-26",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The UniSpeech model was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://huggingface.co/papers/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael\nZeng, Xuedong Huang .\n\nThe abstract from the paper is the following:\n\n*In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both\nunlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive\nself-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture\ninformation more correlated with phonetic structures and improve the generalization across languages and domains. We\nevaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The\nresults show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for...",
            "tasks": [],
            "display_name": "UniSpeech"
        },
        {
            "model_name": "segformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\segformer.md",
            "release_date": "2021-05-31",
            "transformers_date": "2021-10-28",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) is a semantic segmentation model that combines a hierarchical Transformer encoder (Mix Transformer, MiT) with a lightweight all-MLP decoder. It avoids positional encodings and complex decoders and achieves state-of-the-art performance on benchmarks like ADE20K and Cityscapes. This simple and lightweight design is more efficient and scalable.\n\nThe figure below illustrates the architecture of SegFormer.\n\n\n\nYou can find all the original SegFormer checkpoints under the [NVIDIA](https://huggingface.co/nvidia/models?search=segformer) organization.\n\n> [!TIP]\n> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n>\n> Click on the SegFormer models in the right sidebar for more examples of how to apply SegFormer to different vision tasks.\n\nThe example below demonstrates semantic segmentation with [`Pipeline`] or the [`AutoModel`]...",
            "tasks": [],
            "display_name": "SegFormer"
        },
        {
            "model_name": "layoutxlm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\layoutxlm.md",
            "release_date": "2021-04-18",
            "transformers_date": "2021-11-03",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "LayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://huggingface.co/papers/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\nZhang, Furu Wei. It's a multilingual extension of the [LayoutLMv2 model](https://huggingface.co/papers/2012.14740) trained\non 53 languages.\n\nThe abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document\nunderstanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In\nthis paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to\nbridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also\nintroduce a multilingual form understanding benchmark dataset named XFUN, which includes form understanding...",
            "tasks": [],
            "display_name": "LayoutXLM"
        },
        {
            "model_name": "imagegpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\imagegpt.md",
            "release_date": "2020-06-17",
            "transformers_date": "2021-11-18",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt) by Mark\nChen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever. ImageGPT (iGPT) is a GPT-2-like\nmodel trained to predict the next pixel value, allowing for both unconditional and conditional image generation.\n\nThe abstract from the [paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V1_ICML.pdf) is the following:\n\n*Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models\ncan learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels,\nwithout incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels,\nwe find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and\nlow-data classification. On CIFAR-10, we achieve 96.3%...",
            "tasks": [],
            "display_name": "ImageGPT"
        },
        {
            "model_name": "vision-text-dual-encoder",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vision-text-dual-encoder.md",
            "release_date": "2021-11-15",
            "transformers_date": "2021-11-30",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [`VisionTextDualEncoderModel`] can be used to initialize a vision-text dual encoder model with\nany pretrained vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.* [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both the vision and text encoder to project the output embeddings\nto a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a\ndownstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\ntraining and then can be used for zero-shot vision tasks such image-classification or retrieval.\n\nIn [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://huggingface.co/papers/2111.07991) it is shown how\nleveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\nnew zero-shot...",
            "tasks": [],
            "display_name": "VisionTextDualEncoder"
        },
        {
            "model_name": "mluke",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mluke.md",
            "release_date": "2021-10-15",
            "transformers_date": "2021-12-07",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension\nof the [LUKE model](https://huggingface.co/papers/2010.01057) trained on the basis of XLM-RoBERTa.\n\nIt is based on XLM-RoBERTa and adds entity embeddings, which helps improve performance on various downstream tasks\ninvolving reasoning about entities such as named entity recognition, extractive question answering, relation\nclassification, cloze-style knowledge completion.\n\nThe abstract from the paper is the following:\n\n*Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual\nalignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining\nand do not explicitly use entities in downstream tasks. In this study, we explore the...",
            "tasks": [],
            "display_name": "mLUKE"
        },
        {
            "model_name": "perceiver",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\perceiver.md",
            "release_date": "2021-07-30",
            "transformers_date": "2021-12-08",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Perceiver IO model was proposed in [Perceiver IO: A General Architecture for Structured Inputs &\nOutputs](https://huggingface.co/papers/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,\nCatalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M.\nBotvinick, Andrew Zisserman, Oriol Vinyals, Jo\u00e3o Carreira.\n\nPerceiver IO is a generalization of [Perceiver](https://huggingface.co/papers/2103.03206) to handle arbitrary outputs in\naddition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to\nclassification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with audio.\nThis is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO is\nlinear in the input and output size and the bulk of the processing occurs in the latent space, allowing us to...",
            "tasks": [],
            "display_name": "Perceiver"
        },
        {
            "model_name": "wavlm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\wavlm.md",
            "release_date": "2021-10-26",
            "transformers_date": "2021-12-16",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://huggingface.co/papers/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\nMichael Zeng, Furu Wei.\n\nThe abstract from the paper is the following:\n\n*Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been\nattempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker\nidentity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is\nchallenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks.\nWavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker...",
            "tasks": [],
            "display_name": "WavLM"
        },
        {
            "model_name": "wav2vec2_phoneme",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\wav2vec2_phoneme.md",
            "release_date": "2021-09-23",
            "transformers_date": "2021-12-17",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al.,\n2021)](https://huggingface.co/papers/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech\nrecognition systems without any labeled data. However, in many cases there is labeled data available for related\nlanguages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer\nlearning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by\nmapping phonemes of the training languages to the target language using articulatory features. Experiments show that\nthis simple method significantly outperforms prior work which introduced task-specific architectures and used only part\nof a monolingually pretrained...",
            "tasks": [],
            "display_name": "Wav2Vec2Phoneme"
        },
        {
            "model_name": "nystromformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nystromformer.md",
            "release_date": "2021-02-07",
            "transformers_date": "2022-01-11",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Nystr\u00f6mformer model was proposed in [*Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention*](https://huggingface.co/papers/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh.\n\nThe abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard...",
            "tasks": [],
            "display_name": "Nystr\u00f6mformer"
        },
        {
            "model_name": "vit_mae",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vit_mae.md",
            "release_date": "2021-11-11",
            "transformers_date": "2022-01-18",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[ViTMAE](https://huggingface.co/papers/2111.06377) is a self-supervised vision model that is pretrained by masking large portions of an image (~75%). An encoder processes the visible image patches and a decoder reconstructs the missing pixels from the encoded patches and mask tokens. After pretraining, the encoder can be reused for downstream tasks like image classification or object detection \u2014 often outperforming models trained with supervised learning.\n\n\n\nYou can find all the original ViTMAE checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=vit-mae) organization.\n\n> [!TIP]\n> Click on the ViTMAE models in the right sidebar for more examples of how to apply ViTMAE to vision tasks.\n\nThe example below demonstrates how to reconstruct the missing pixels with the [`ViTMAEForPreTraining`] class.\n\n\n\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import infer_device, ViTImageProcessor, ViTMAEForPreTraining\n\ndevice =...",
            "tasks": [],
            "display_name": "ViTMAE"
        },
        {
            "model_name": "vilt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vilt.md",
            "release_date": "2021-02-05",
            "transformers_date": "2022-01-19",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The ViLT model was proposed in [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://huggingface.co/papers/2102.03334)\nby Wonjae Kim, Bokyung Son, Ildoo Kim. ViLT incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design\nfor Vision-and-Language Pre-training (VLP).\n\nThe abstract from the paper is the following:\n\n*Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks.\nCurrent approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision\n(e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we\nfind it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more\ncomputation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive\npower of the...",
            "tasks": [],
            "display_name": "ViLT"
        },
        {
            "model_name": "swin",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\swin.md",
            "release_date": "2021-03-25",
            "transformers_date": "2022-01-21",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Swin Transformer](https://huggingface.co/papers/2103.14030) is a hierarchical vision transformer. Images are processed in patches and windowed self-attention is used to capture local information. These windows are shifted across the image to allow for cross-window connections, capturing global information more efficiently. This hierarchical approach with shifted windows allows the Swin Transformer to process images effectively at different scales and achieve linear computational complexity relative to image size, making it a versatile backbone for various vision tasks like image classification and object detection.\n\nYou can find all official Swin Transformer checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=swin) organization.\n\n> [!TIP]\n> Click on the Swin Transformer models in the right sidebar for more examples of how to apply Swin Transformer to different image tasks.\n\nThe example below demonstrates how to classify an image with [`Pipeline`] or the...",
            "tasks": [],
            "display_name": "Swin Transformer"
        },
        {
            "model_name": "yoso",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\yoso.md",
            "release_date": "2021-11-18",
            "transformers_date": "2022-01-26",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The YOSO model was proposed in [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714)  \nby Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. YOSO approximates standard softmax self-attention\nvia a Bernoulli sampling scheme based on Locality Sensitive Hashing (LSH). In principle, all the Bernoulli random variables can be sampled with\na single hash.\n\nThe abstract from the paper is the following:\n\n*Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is\nthe self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically\non the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling\nattention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to...",
            "tasks": [],
            "display_name": "YOSO"
        },
        {
            "model_name": "xglm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xglm.md",
            "release_date": "2021-12-20",
            "transformers_date": "2022-01-28",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The XGLM model was proposed in [Few-shot Learning with Multilingual Language Models](https://huggingface.co/papers/2112.10668)\nby Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal,\nShruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo,\nJeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n\nThe abstract from the paper is the following:\n\n*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language\ntasks without fine-tuning. While these models are known to be able to jointly represent many different languages,\ntheir training data is dominated by English, potentially limiting their cross-lingual generalization.\nIn this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages,\nand study their few- and zero-shot learning...",
            "tasks": [],
            "display_name": "XGLM"
        },
        {
            "model_name": "xlm-roberta-xl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlm-roberta-xl.md",
            "release_date": "2021-05-02",
            "transformers_date": "2022-01-29",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[XLM-RoBERTa-XL](https://huggingface.co/papers/2105.00572) is a 3.5B parameter multilingual masked language model pretrained on 100 languages. It shows that by scaling model capacity, multilingual models demonstrates strong performance on high-resource languages and can even zero-shot low-resource languages.\n\nYou can find all the original XLM-RoBERTa-XL checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=xlm) organization.\n\n> [!TIP]\n> Click on the XLM-RoBERTa-XL models in the right sidebar for more examples of how to apply XLM-RoBERTa-XL to different cross-lingual tasks like classification, translation, and question answering.\n\nThe example below demonstrates how to predict the `` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"facebook/xlm-roberta-xl\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Bonjour, je suis un...",
            "tasks": [],
            "display_name": "XLM-RoBERTa-XL"
        },
        {
            "model_name": "convnext",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\convnext.md",
            "release_date": "2022-01-10",
            "transformers_date": "2022-02-07",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ConvNeXT model was proposed in [A ConvNet for the 2020s](https://huggingface.co/papers/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\nConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them.\n\nThe abstract from the paper is the following:\n\n*The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model.\nA vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers\n(e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide\nvariety of vision tasks. However, the effectiveness of such...",
            "tasks": [],
            "display_name": "ConvNeXT"
        },
        {
            "model_name": "poolformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\poolformer.md",
            "release_date": "2021-11-22",
            "transformers_date": "2022-02-17",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The PoolFormer model was proposed in [MetaFormer is Actually What You Need for Vision](https://huggingface.co/papers/2111.11418)  by Sea AI Labs. Instead of designing complicated token mixer to achieve SOTA performance, the target of this work is to demonstrate the competence of transformer models largely stem from the general architecture MetaFormer.\n\nThe abstract from the paper is the following:\n\n*Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an...",
            "tasks": [],
            "display_name": "PoolFormer"
        },
        {
            "model_name": "plbart",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\plbart.md",
            "release_date": "2021-03-10",
            "transformers_date": "2022-02-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The PLBART model was proposed in [Unified Pre-training for Program Understanding and Generation](https://huggingface.co/papers/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\nThis is a BART-like model which can be used to perform code-summarization, code-generation, and code-translation tasks. The pre-trained model `plbart-base` has been trained using multilingual denoising task\non Java, Python and English.\n\nAccording to the abstract\n\n*Code summarization and generation empower conversion between programming language (PL) and natural language (NL),\nwhile code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks.\nPLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding.\nExperiments on code summarization in the...",
            "tasks": [],
            "display_name": "PLBart"
        },
        {
            "model_name": "data2vec",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\data2vec.md",
            "release_date": "2022-02-07",
            "transformers_date": "2022-03-01",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://huggingface.co/papers/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.\nData2Vec proposes a unified framework for self-supervised learning across different data modalities - text, audio and images.\nImportantly, predicted targets for pre-training are contextualized latent representations of the inputs, rather than modality-specific, context-independent targets.\n\nThe abstract from the paper is the following:\n\n*While the general idea of self-supervised learning is identical across modalities, the actual algorithms and\nobjectives differ widely because they were developed with a single modality in mind. To get us closer to general\nself-supervised learning, we present data2vec, a framework that uses the same learning method for either speech,\nNLP or computer vision. The core idea is to predict latent...",
            "tasks": [],
            "display_name": "Data2Vec"
        },
        {
            "model_name": "maskformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\maskformer.md",
            "release_date": "2021-07-13",
            "transformers_date": "2022-03-02",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight\nbreaking changes to fix it in the future. If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title).\n\n\n\n## Overview\n\nThe MaskFormer model was proposed in [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://huggingface.co/papers/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov. MaskFormer addresses semantic segmentation with a mask classification paradigm instead of performing classic pixel-level classification.\n\nThe abstract from the paper is the following:\n\n*Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and...",
            "tasks": [],
            "display_name": "MaskFormer"
        },
        {
            "model_name": "dit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dit.md",
            "release_date": "2022-03-04",
            "transformers_date": "2022-03-10",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[DiT](https://huggingface.co/papers/2203.02378) is an image transformer pretrained on large-scale unlabeled document images. It learns to predict the missing visual tokens from a corrupted input image. The pretrained DiT model can be used as a backbone in other models for visual document tasks like document image classification and table detection.\n\n\n\nYou can find all the original DiT checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=dit) organization.\n\n> [!TIP]\n> Refer to the [BEiT](./beit) docs for more examples of how to apply DiT to different vision tasks.\n\nThe example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"image-classification\",\n    model=\"microsoft/dit-base-finetuned-rvlcdip\",\n    dtype=torch.float16,\n   ...",
            "tasks": [],
            "display_name": "DiT"
        },
        {
            "model_name": "resnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\resnet.md",
            "release_date": "2015-12-10",
            "transformers_date": "2022-03-14",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://huggingface.co/papers/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`. This is generally known as \"ResNet v1.5\".\n\nResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\n\nThe abstract from the paper is the following:\n\n*Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the...",
            "tasks": [],
            "display_name": "ResNet"
        },
        {
            "model_name": "glpn",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\glpn.md",
            "release_date": "2022-01-19",
            "transformers_date": "2022-03-22",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight\nbreaking changes to fix it in the future. If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title).\n\n\n\n## Overview\n\nThe GLPN model was proposed in [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://huggingface.co/papers/2201.07436)  by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\nGLPN combines [SegFormer](segformer)'s hierarchical mix-Transformer with a lightweight decoder for monocular depth estimation. The proposed decoder shows better performance than the previously proposed decoders, with considerably\nless computational complexity.\n\nThe abstract from the paper is the following:\n\n*Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has...",
            "tasks": [],
            "display_name": "GLPN"
        },
        {
            "model_name": "decision_transformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\decision_transformer.md",
            "release_date": "2021-06-02",
            "transformers_date": "2022-03-23",
            "modality": "reinforcement",
            "modality_name": "Reinforcement Learning",
            "modality_color": "#EF4444",
            "description": "The Decision Transformer model was proposed in [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://huggingface.co/papers/2106.01345)  \nby Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n\nThe abstract from the paper is the following:\n\n*We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem.\nThis allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances\n in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that\n casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or\n compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked\n Transformer. By conditioning an autoregressive model on the desired return (reward), past...",
            "tasks": [],
            "display_name": "Decision Transformer"
        },
        {
            "model_name": "dpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dpt.md",
            "release_date": "2021-03-24",
            "transformers_date": "2022-03-28",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The DPT model was proposed in [Vision Transformers for Dense Prediction](https://huggingface.co/papers/2103.13413) by Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\nDPT is a model that leverages the [Vision Transformer (ViT)](vit) as backbone for dense prediction tasks like semantic segmentation and depth estimation.\n\nThe abstract from the paper is the following:\n\n*We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally...",
            "tasks": [],
            "display_name": "DPT"
        },
        {
            "model_name": "regnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\regnet.md",
            "release_date": "2020-03-30",
            "transformers_date": "2022-04-07",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The RegNet model was proposed in [Designing Network Design Spaces](https://huggingface.co/papers/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Doll\u00e1r.\n\nThe authors design search spaces to perform Neural Architecture Search (NAS). They first start from a high dimensional search space and iteratively reduce the search space by empirically applying constraints based on the best-performing models sampled by the current search space.\n\nThe abstract from the paper is the following:\n\n*In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the...",
            "tasks": [],
            "display_name": "RegNet"
        },
        {
            "model_name": "yolos",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\yolos.md",
            "release_date": "2021-06-01",
            "transformers_date": "2022-05-02",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[YOLOS](https://huggingface.co/papers/2106.00666) uses a [Vision Transformer (ViT)](./vit) for object detection with minimal modifications and region priors. It can achieve performance comparable to specialized object detection models and frameworks with knowledge about 2D spatial structures.\n\nYou can find all the original YOLOS checkpoints under the [HUST Vision Lab](https://huggingface.co/hustvl/models?search=yolos) organization.\n\n\n\n YOLOS architecture. Taken from the original paper.\n\n> [!TIP]\n> This model wasa contributed by [nielsr](https://huggingface.co/nielsr).\n> Click on the YOLOS models in the right sidebar for more examples of how to apply YOLOS to different object detection tasks.\n\nThe example below demonstrates how to detect objects with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\ndetector = pipeline(\n    task=\"object-detection\",\n    model=\"hustvl/yolos-base\",\n    dtype=torch.float16,\n   ...",
            "tasks": [],
            "display_name": "YOLOS"
        },
        {
            "model_name": "flava",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\flava.md",
            "release_date": "2021-12-08",
            "transformers_date": "2022-05-11",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://huggingface.co/papers/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.\n\nThe paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks.\n\nThe abstract from the paper is the following:\n\n*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\nat once -- a true vision and language foundation...",
            "tasks": [],
            "display_name": "FLAVA"
        },
        {
            "model_name": "opt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\opt.md",
            "release_date": "2022-05-02",
            "transformers_date": "2022-05-12",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[OPT](https://huggingface.co/papers/2205.01068) is a suite of open-source decoder-only pre-trained transformers whose parameters range from 125M to 175B. OPT models are designed for causal language modeling and aim to enable responsible and reproducible research at scale. OPT-175B is comparable in performance to GPT-3 with only 1/7th the carbon footprint.\n\nYou can find all the original OPT checkpoints under the [OPT](https://huggingface.co/collections/facebook/opt-66ed00e15599f02966818844) collection.\n\n> [!TIP]\n> This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ), [ybelkada](https://huggingface.co/ybelkada), and [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n>\n> Click on the OPT models in the right sidebar for more examples of how to apply OPT to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline...",
            "tasks": [],
            "display_name": "OPT"
        },
        {
            "model_name": "wav2vec2-conformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\wav2vec2-conformer.md",
            "release_date": "2020-10-11",
            "transformers_date": "2022-05-17",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Wav2Vec2-Conformer was added to an updated version of [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n\nThe official results of the model can be found in Table 3 and Table 4 of the paper.\n\nThe Wav2Vec2-Conformer weights were released by the Meta AI team within the [Fairseq library](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md#pre-trained-models).\n\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/wav2vec).\n\nNote: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert) - it's pretrained on 4.5M hours of audio. We especially recommend using it for fine-tuning tasks, e.g. as per [this...",
            "tasks": [],
            "display_name": "Wav2Vec2-Conformer"
        },
        {
            "model_name": "cvt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\cvt.md",
            "release_date": "2021-03-29",
            "transformers_date": "2022-05-18",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Convolutional Vision Transformer (CvT)](https://huggingface.co/papers/2103.15808) is a model that combines the strengths of convolutional neural networks (CNNs) and Vision transformers for the computer vision tasks. It introduces convolutional layers into the vision transformer architecture, allowing it to capture local patterns in images while maintaining the global context provided by self-attention mechanisms.\n\nYou can find all the CvT checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=cvt) organization.\n\n> [!TIP]\n> This model was contributed by [anujunj](https://huggingface.co/anugunj).\n>\n> Click on the CvT models in the right sidebar for more examples of how to apply CvT to different computer vision tasks.\n\nThe example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"image-classification\",\n    model=\"microsoft/cvt-13\",\n  ...",
            "tasks": [],
            "display_name": "Convolutional Vision Transformer (CvT)"
        },
        {
            "model_name": "gpt_neox",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt_neox.md",
            "release_date": "2022-04-14",
            "transformers_date": "2022-05-24",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "We introduce [GPT-NeoX-20B](https://huggingface.co/papers/2204.06745), a 20 billion parameter autoregressive language model trained on the Pile, whose weights will\nbe made freely and openly available to the public through a permissive license. It is, to the best of our knowledge,\nthe largest dense autoregressive model that has publicly available weights at the time of submission. In this work,\nwe describe GPT-NeoX-20B's architecture and training and evaluate its performance on a range of language-understanding,\nmathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and\ngains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source\nthe training and evaluation code, as well as the model weights, at [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\n\nDevelopment of the model was led by Sid Black, Stella Biderman and Eric Hallahan, and the model was...",
            "tasks": [],
            "display_name": "GPT-NeoX"
        },
        {
            "model_name": "layoutlmv3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\layoutlmv3.md",
            "release_date": "2022-04-18",
            "transformers_date": "2022-05-24",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The LayoutLMv3 model was proposed in [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://huggingface.co/papers/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\nLayoutLMv3 simplifies [LayoutLMv2](layoutlmv2) by using patch embeddings (as in [ViT](vit)) instead of leveraging a CNN backbone, and pre-trains the model on 3 objectives: masked language modeling (MLM), masked image modeling (MIM)\nand word-patch alignment (WPA).\n\nThe abstract from the paper is the following:\n\n*Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified...",
            "tasks": [],
            "display_name": "LayoutLMv3"
        },
        {
            "model_name": "levit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\levit.md",
            "release_date": "2021-04-02",
            "transformers_date": "2022-06-01",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The LeViT model was proposed in [LeViT: Introducing Convolutions to Vision Transformers](https://huggingface.co/papers/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, Matthijs Douze. LeViT improves the [Vision Transformer (ViT)](vit) in performance and efficiency by a few architectural differences such as activation maps with decreasing resolutions in Transformers and the introduction of an attention bias to integrate positional information.\n\nThe abstract from the paper is the following:\n\n*We design a family of image classification architectures that optimize the trade-off between accuracy\nand efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures,\nwhich are competitive on highly parallel processing hardware. We revisit principles from the extensive\nliterature on convolutional neural networks to apply them to transformers, in particular activation maps\nwith decreasing resolutions. We...",
            "tasks": [],
            "display_name": "LeViT"
        },
        {
            "model_name": "bloom",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bloom.md",
            "release_date": "2022-11-09",
            "transformers_date": "2022-06-09",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The [BLOOM](https://huggingface.co/papers/2211.05100) model has been proposed with its various versions through the [BigScience Workshop](https://bigscience.huggingface.co/). BigScience is inspired by other open science initiatives where researchers have pooled their time and resources to collectively achieve a higher impact.\nThe architecture of BLOOM is essentially similar to GPT3 (auto-regressive model for next token prediction), but has been trained on 46 different languages and 13 programming languages.\nSeveral smaller versions of the models have been trained on the same dataset. BLOOM is available in the following versions:\n\n- [bloom-560m](https://huggingface.co/bigscience/bloom-560m)\n- [bloom-1b1](https://huggingface.co/bigscience/bloom-1b1)\n- [bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)\n- [bloom-3b](https://huggingface.co/bigscience/bloom-3b)\n- [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1)\n- [bloom](https://huggingface.co/bigscience/bloom) (176B...",
            "tasks": [],
            "display_name": "BLOOM"
        },
        {
            "model_name": "longt5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\longt5.md",
            "release_date": "2021-12-15",
            "transformers_date": "2022-06-13",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://huggingface.co/papers/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\nThe abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from...",
            "tasks": [],
            "display_name": "LongT5"
        },
        {
            "model_name": "codegen",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\codegen.md",
            "release_date": "2022-03-25",
            "transformers_date": "2022-06-24",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://huggingface.co/papers/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\n\nCodeGen is an autoregressive language model for program synthesis trained sequentially on [The Pile](https://pile.eleuther.ai/), BigQuery, and BigPython.\n\nThe abstract from the paper is the following:\n\n*Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in...",
            "tasks": [],
            "display_name": "CodeGen"
        },
        {
            "model_name": "groupvit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\groupvit.md",
            "release_date": "2022-02-22",
            "transformers_date": "2022-06-28",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The GroupViT model was proposed in [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://huggingface.co/papers/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\nInspired by [CLIP](clip), GroupViT is a vision-language model that can perform zero-shot semantic segmentation on any given vocabulary categories.\n\nThe abstract from the paper is the following:\n\n*Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid...",
            "tasks": [],
            "display_name": "GroupViT"
        },
        {
            "model_name": "mobilevit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mobilevit.md",
            "release_date": "2021-10-05",
            "transformers_date": "2022-06-29",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[MobileViT](https://huggingface.co/papers/2110.02178) is a lightweight vision transformer for mobile devices that merges CNNs's efficiency and inductive biases with transformers global context modeling. It treats transformers as convolutions, enabling global information processing without the heavy computational cost of standard ViTs.\n\n\n   \n\n\nYou can find all the original MobileViT checkpoints under the [Apple](https://huggingface.co/apple/models?search=mobilevit) organization.\n\n> [!TIP]\n>\n> - This model was contributed by [matthijs](https://huggingface.co/Matthijs).\n>\n> Click on the MobileViT models in the right sidebar for more examples of how to apply MobileViT to different vision tasks.\n\nThe example below demonstrates how to do [Image Classification] with [`Pipeline`] and the [`AutoModel`] class.\n\n\n\n\n```python\n\nimport torch\nfrom transformers import pipeline\n\nclassifier = pipeline(\n   task=\"image-classification\",\n   model=\"apple/mobilevit-small\",\n   dtype=torch.float16,...",
            "tasks": [],
            "display_name": "MobileViT"
        },
        {
            "model_name": "mvp",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mvp.md",
            "release_date": "2022-06-24",
            "transformers_date": "2022-06-29",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://huggingface.co/papers/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n\nAccording to the abstract,\n\n- MVP follows a standard Transformer encoder-decoder architecture.\n- MVP is supervised pre-trained using labeled datasets.\n- MVP also has task-specific soft prompts to stimulate the model's capacity in performing a certain task.\n- MVP is specially designed for natural language generation and can be adapted to a wide range of generation tasks, including but not limited to summarization, data-to-text generation, open-ended dialogue system, story generation, question answering, question generation, task-oriented dialogue system, commonsense generation, paraphrase generation, text style transfer, and text simplification. Our model can also be adapted to natural language understanding tasks such as sequence classification and (extractive) question...",
            "tasks": [],
            "display_name": "MVP"
        },
        {
            "model_name": "nllb",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nllb.md",
            "release_date": "2022-07-11",
            "transformers_date": "2022-07-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[NLLB: No Language Left Behind](https://huggingface.co/papers/2207.04672) is a multilingual translation model. It's trained on data using data mining techniques tailored for low-resource languages and supports over 200 languages. NLLB features a conditional compute architecture using a Sparsely Gated Mixture of Experts.\n\nYou can find all the original NLLB checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=nllb) organization.\n\n> [!TIP]\n> This model was contributed by [Lysandre](https://huggingface.co/lysandre).\n> Click on the NLLB models in the right sidebar for more examples of how to apply NLLB to different translation tasks.\n\nThe example below demonstrates how to translate text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\", dtype=torch.float16, device=0)\npipeline(\"UN...",
            "tasks": [],
            "display_name": "NLLB"
        },
        {
            "model_name": "owlvit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\owlvit.md",
            "release_date": "2022-05-12",
            "transformers_date": "2022-07-22",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in [Simple Open-Vocabulary Object Detection with Vision Transformers](https://huggingface.co/papers/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text.\n\nThe abstract from the paper is the following:\n\n*Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively...",
            "tasks": [],
            "display_name": "OWL-ViT"
        },
        {
            "model_name": "swinv2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\swinv2.md",
            "release_date": "2021-11-18",
            "transformers_date": "2022-07-27",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Swin Transformer V2](https://huggingface.co/papers/2111.09883) is a 3B parameter model that focuses on how to scale a vision model to billions of parameters. It introduces techniques like residual-post-norm combined with cosine attention for improved training stability, log-spaced continuous position bias to better handle varying image resolutions between pre-training and fine-tuning, and a new pre-training method (SimMIM) to reduce the need for large amounts of labeled data. These improvements enable efficiently training very large models (up to 3 billion parameters) capable of processing high-resolution images.\n\nYou can find official Swin Transformer V2 checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=swinv2) organization.\n\n> [!TIP]\n> Click on the Swin Transformer V2 models in the right sidebar for more examples of how to apply Swin Transformer V2 to vision tasks.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   ...",
            "tasks": [],
            "display_name": "Swin Transformer V2"
        },
        {
            "model_name": "videomae",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\videomae.md",
            "release_date": "2022-03-23",
            "transformers_date": "2022-08-04",
            "modality": "video",
            "modality_name": "Video Models",
            "modality_color": "#EC4899",
            "description": "The VideoMAE model was proposed in [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://huggingface.co/papers/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\nVideoMAE extends masked auto encoders ([MAE](vit_mae)) to video, claiming state-of-the-art performance on several video classification benchmarks.\n\nThe abstract from the paper is the following:\n\n*Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking and reconstruction. These simple designs turn out to be effective for overcoming information leakage caused by the temporal correlation during video reconstruction. We obtain three important findings on SSVP: (1)...",
            "tasks": [],
            "display_name": "VideoMAE"
        },
        {
            "model_name": "donut",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\donut.md",
            "release_date": "2021-11-30",
            "transformers_date": "2022-08-12",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Donut (Document Understanding Transformer)](https://huggingface.co/papers/2111.15664) is a visual document understanding model that doesn't require an Optical Character Recognition (OCR) engine. Unlike traditional approaches that extract text using OCR before processing, Donut employs an end-to-end Transformer-based architecture to directly analyze document images. This eliminates OCR-related inefficiencies making it more accurate and adaptable to diverse languages and formats.\n\nDonut features vision encoder ([Swin](./swin)) and a text decoder ([BART](./bart)). Swin converts document images into embeddings and BART processes them into meaningful text sequences.\n\nYou can find all the original Donut checkpoints under the [Naver Clova Information Extraction](https://huggingface.co/naver-clova-ix) organization.\n\n> [!TIP]\n> Click on the Donut models in the right sidebar for more examples of how to apply Donut to different language and vision tasks.\n\nThe examples below demonstrate how to...",
            "tasks": [],
            "display_name": "Donut"
        },
        {
            "model_name": "pegasus_x",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pegasus_x.md",
            "release_date": "2022-08-08",
            "transformers_date": "2022-09-02",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[PEGASUS-X](https://huggingface.co/papers/2208.04347) is an encoder-decoder (sequence-to-sequence) transformer model for long-input summarization. It extends the [Pegasus](./pegasus) model with staggered block-local attention, global encoder tokens, and additional pretraining on long text sequences, enabling it to handle inputs of up to 16,000 tokens. PEGASUS-X matches the performance of much larger models while using fewer parameters.\n\nYou can find all the original PEGASUS-X checkpoints under the [Google](https://huggingface.co/google/models?search=pegasus-x) organization.\n\n> [!TIP]\n> This model was contributed by [zphang](https://huggingface.co/zphang).\n>\n> Click on the PEGASUS-X models in the right sidebar for more examples of how to apply PEGASUS-X to different language tasks.\n\nThe example below demonstrates how to summarize text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   ...",
            "tasks": [],
            "display_name": "PEGASUS-X"
        },
        {
            "model_name": "ernie",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ernie.md",
            "release_date": "2019-04-19",
            "transformers_date": "2022-09-09",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[ERNIE1.0](https://huggingface.co/papers/1904.09223), [ERNIE2.0](https://ojs.aaai.org/index.php/AAAI/article/view/6428),\n[ERNIE3.0](https://huggingface.co/papers/2107.02137), [ERNIE-Gram](https://huggingface.co/papers/2010.12148), [ERNIE-health](https://huggingface.co/papers/2110.07244) are a series of powerful models proposed by baidu, especially in Chinese tasks.\n\nERNIE (Enhanced Representation through kNowledge IntEgration) is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking.\n\nOther ERNIE models released by baidu can be found at [Ernie 4.5](./ernie4_5), and [Ernie 4.5 MoE](./ernie4_5_moe).\n\n> [!TIP]\n> This model was contributed by [nghuyong](https://huggingface.co/nghuyong), and the official code can be found in [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) (in PaddlePaddle).\n>\n> Click on the ERNIE models in the right sidebar for more examples of how to apply ERNIE to different...",
            "tasks": [],
            "display_name": "ERNIE"
        },
        {
            "model_name": "deformable_detr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deformable_detr.md",
            "release_date": "2020-10-08",
            "transformers_date": "2022-09-14",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Deformable DETR](https://huggingface.co/papers/2010.04159) improves on the original [DETR](./detr) by using a deformable attention module. This mechanism selectively attends to a small set of key sampling points around a reference. It improves training speed and improves accuracy.\n\n\n\n Deformable DETR architecture. Taken from the original paper.\n\nYou can find all the available Deformable DETR checkpoints under the [SenseTime](https://huggingface.co/SenseTime) organization.\n\n> [!TIP]\n> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n>\n> Click on the Deformable DETR models in the right sidebar for more examples of how to apply Deformable DETR to different object detection and segmentation tasks.\n\nThe example below demonstrates how to perform object detection with the [`Pipeline`] and the [`AutoModel`] class.\n\n\n\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipeline = pipeline(\n    \"object-detection\", \n    model=\"SenseTime/deformable-detr\",\n   ...",
            "tasks": [],
            "display_name": "Deformable DETR"
        },
        {
            "model_name": "gpt_neox_japanese",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt_neox_japanese.md",
            "release_date": "2022-07-27",
            "transformers_date": "2022-09-14",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "GPT-NeoX-Japanese, a Japanese language model based on [GPT-NeoX](./gpt_neox).\nJapanese uses three types of characters (hiragana, katakana, kanji) and has a huge vocabulary. This model uses [BPEEncoder V2](https://github.com/tanreinama/Japanese-BPEEncoder_V2), a sub-word tokenizer to handle the different characters.\n\nThe model also removes some bias parameters for better performance.\n\nYou can find all the original GPT-NeoX-Japanese checkpoints under the [ABEJA](https://huggingface.co/abeja/models?search=gpt-neo-x) organization.\n\n> [!TIP]\n> This model was contributed by [Shinya Otani](https://github.com/SO0529), [Takayoshi Makabe](https://github.com/spider-man-tm), [Anuj Arora](https://github.com/Anuj040), and [Kyo Hattori](https://github.com/go5paopao) from [ABEJA, Inc.](https://www.abejainc.com/).\n>\n> Click on the GPT-NeoX-Japanese models in the right sidebar for more examples of how to apply GPT-NeoX-Japanese to different language tasks.\n\nThe example below demonstrates how to...",
            "tasks": [],
            "display_name": "GPT-NeoX-Japanese"
        },
        {
            "model_name": "conditional_detr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\conditional_detr.md",
            "release_date": "2021-08-13",
            "transformers_date": "2022-09-22",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The Conditional DETR model was proposed in [Conditional DETR for Fast Training Convergence](https://huggingface.co/papers/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang. Conditional DETR presents a conditional cross-attention mechanism for fast DETR training. Conditional DETR converges 6.7\u00d7 to 10\u00d7 faster than DETR.\n\nThe abstract from the paper is the following:\n\n*The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach,...",
            "tasks": [],
            "display_name": "Conditional DETR"
        },
        {
            "model_name": "vit_msn",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vit_msn.md",
            "release_date": "2022-04-14",
            "transformers_date": "2022-09-22",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ViTMSN model was proposed in [Masked Siamese Networks for Label-Efficient Learning](https://huggingface.co/papers/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,\nPascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. The paper presents a joint-embedding architecture to match the prototypes\nof masked patches with that of the unmasked patches. With this setup, their method yields excellent performance in the low-shot and extreme low-shot\nregimes.\n\nThe abstract from the paper is the following:\n\n*We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our\napproach matches the representation of an image view containing randomly masked patches to the representation of the original\nunmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the\nunmasked patches are processed by the network. As a result,...",
            "tasks": [],
            "display_name": "ViTMSN"
        },
        {
            "model_name": "esm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\esm.md",
            "release_date": "2019-04-19",
            "transformers_date": "2022-09-30",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This page provides code and pre-trained weights for Transformer protein language models from Meta AI's Fundamental\nAI Research Team, providing the state-of-the-art ESMFold and ESM-2, and the previously released ESM-1b and ESM-1v.\nTransformer protein language models were introduced in the paper [Biological structure and function emerge from scaling\nunsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott,\nC. Lawrence Zitnick, Jerry Ma, and Rob Fergus.\nThe first version of this paper was [preprinted in 2019](https://www.biorxiv.org/content/10.1101/622803v1?versioned=true).\n\nESM-2 outperforms all tested single-sequence protein language models across a range of structure prediction tasks,\nand enables atomic resolution structure prediction.\nIt was released with the paper [Language models of protein sequences at the scale of evolution enable...",
            "tasks": [],
            "display_name": "ESM"
        },
        {
            "model_name": "markuplm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\markuplm.md",
            "release_date": "2021-10-16",
            "transformers_date": "2022-09-30",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://huggingface.co/papers/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).\n\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\nfor information extraction from web pages (basically named-entity recognition on web pages)\n\nThe abstract from the paper is the...",
            "tasks": [],
            "display_name": "MarkupLM"
        },
        {
            "model_name": "time_series_transformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\time_series_transformer.md",
            "release_date": "2022-12-01",
            "transformers_date": "2022-09-30",
            "modality": "timeseries",
            "modality_name": "Time Series Models",
            "modality_color": "#F97316",
            "description": "The Time Series Transformer model is a vanilla encoder-decoder Transformer for time series forecasting.\nThis model was contributed by [kashif](https://huggingface.co/kashif).\n\n## Usage tips\n\n- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer without any head on top, and [`TimeSeriesTransformerForPrediction`]\nadds a distribution head on top of the former, which can be used for time-series forecasting. Note that this is a so-called probabilistic forecasting model, not a\npoint forecasting model. This means that the model learns a distribution, from which one can sample. The model doesn't directly output values.\n- [`TimeSeriesTransformerForPrediction`] consists of 2 blocks: an encoder, which takes a `context_length` of time series values as input (called `past_values`),\nand a decoder, which predicts a `prediction_length` of time series values into the future (called `future_values`). During training, one needs to provide\npairs of (`past_values`...",
            "tasks": [],
            "display_name": "Time Series Transformer"
        },
        {
            "model_name": "whisper",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\whisper.md",
            "release_date": "2022-12-06",
            "transformers_date": "2022-10-05",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[Whisper](https://huggingface.co/papers/2212.04356) is a encoder-decoder (sequence-to-sequence) transformer pretrained on 680,000 hours of labeled audio data. This amount of pretraining data enables zero-shot performance on audio tasks in English and many other languages. The decoder allows Whisper to map the encoders learned speech representations to useful outputs, such as text, without additional fine-tuning. Whisper just works out of the box.\n\nYou can find all the original Whisper checkpoints under the [Whisper](https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013) collection.\n\n\n> [!TIP]\n> Click on the Whisper models in the right sidebar for more examples of how to apply Whisper to different audio tasks.\n\nThe example below demonstrates how to automatically transcribe speech into text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"automatic-speech-recognition\",\n   ...",
            "tasks": [],
            "display_name": "Whisper"
        },
        {
            "model_name": "lilt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\lilt.md",
            "release_date": "2022-02-28",
            "transformers_date": "2022-10-12",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The LiLT model was proposed in [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://huggingface.co/papers/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\nLiLT allows to combine any pre-trained RoBERTa text encoder with a lightweight Layout Transformer, to enable [LayoutLM](layoutlm)-like document understanding for many\nlanguages.\n\nThe abstract from the paper is the following:\n\n*Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured...",
            "tasks": [],
            "display_name": "LiLT"
        },
        {
            "model_name": "table-transformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\table-transformer.md",
            "release_date": "2021-09-30",
            "transformers_date": "2022-10-18",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://huggingface.co/papers/2110.00061) by\nBrandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\nas well as table structure recognition and functional analysis. The authors train 2 [DETR](detr) models, one for table detection and one for table structure recognition, dubbed Table Transformers.\n\nThe abstract from the paper is the following:\n\n*Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents.\nHowever, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more\ncomprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains...",
            "tasks": [],
            "display_name": "Table Transformer"
        },
        {
            "model_name": "clipseg",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\clipseg.md",
            "release_date": "2021-12-18",
            "transformers_date": "2022-11-08",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://huggingface.co/papers/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero-shot and one-shot image segmentation.\n\nThe abstract from the paper is the following:\n\n*Image segmentation is usually addressed by training a\nmodel for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive\nas it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system\nthat can generate image segmentations based on arbitrary\nprompts at test time. A prompt can be either a text or an\nimage. This approach enables us to create a unified model\n(trained once) for three common segmentation tasks, which\ncome with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.\nWe build upon the CLIP model as a...",
            "tasks": [],
            "display_name": "CLIPSeg"
        },
        {
            "model_name": "roc_bert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\roc_bert.md",
            "release_date": "2022-05-27",
            "transformers_date": "2022-11-08",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[RoCBert](https://aclanthology.org/2022.acl-long.65.pdf) is a pretrained Chinese [BERT](./bert) model designed against adversarial attacks like typos and synonyms. It is pretrained with a contrastive learning objective to align normal and adversarial text examples. The examples include different semantic, phonetic, and visual features of Chinese. This makes RoCBert more robust against manipulation.\n\nYou can find all the original RoCBert checkpoints under the [weiweishi](https://huggingface.co/weiweishi) profile.\n\n> [!TIP]\n> This model was contributed by [weiweishi](https://huggingface.co/weiweishi).\n>\n> Click on the RoCBert models in the right sidebar for more examples of how to apply RoCBert to different Chinese language tasks.\n\nThe example below demonstrates how to predict the [MASK] token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   task=\"fill-mask\",\n  ...",
            "tasks": [],
            "display_name": "RoCBert"
        },
        {
            "model_name": "mobilenet_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mobilenet_v2.md",
            "release_date": "2018-01-13",
            "transformers_date": "2022-11-14",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[MobileNet V2](https://huggingface.co/papers/1801.04381) improves performance on mobile devices with a more efficient architecture. It uses inverted residual blocks and linear bottlenecks to start with a smaller representation of the data, expands it for processing, and shrinks it again to reduce the number of computations. The model also removes non-linearities to maintain accuracy despite its simplified design. Like [MobileNet V1](./mobilenet_v1), it uses depthwise separable convolutions for efficiency.\n\nYou can all the original MobileNet checkpoints under the [Google](https://huggingface.co/google?search_models=mobilenet) organization.\n\n> [!TIP]\n> Click on the MobileNet V2 models in the right sidebar for more examples of how to apply MobileNet to different vision tasks.\n\nThe examples below demonstrate how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   ...",
            "tasks": [],
            "display_name": "MobileNet V2"
        },
        {
            "model_name": "switch_transformers",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\switch_transformers.md",
            "release_date": "2021-01-11",
            "transformers_date": "2022-11-15",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Switch Transformers](https://huggingface.co/papers/2101.03961) is a sparse T5 model where the MLP layer is replaced by a Mixture-of-Experts (MoE). A routing mechanism associates each token with an expert and each expert is a dense MLP. Sparsity enables better scaling and the routing mechanism allows the model to select relevant weights on the fly which increases model capacity.\n\nYou can find all the original Switch Transformers checkpoints under the [Switch Transformer](https://huggingface.co/collections/google/switch-transformers-release-6548c35c6507968374b56d1f) collection.\n\n> [!TIP]\n> This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and [ArthurZ](https://huggingface.co/ArthurZ).\n>\n> Click on the Switch Transformers models in the right sidebar for more examples of how to apply Switch Transformers to different natural language tasks.\n\nThe example below demonstrates how to predict the masked token with [`Pipeline`], [`AutoModel`], and from the command...",
            "tasks": [],
            "display_name": "Switch Transformers"
        },
        {
            "model_name": "dinat",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dinat.md",
            "release_date": "2022-09-29",
            "transformers_date": "2022-11-18",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "DiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://huggingface.co/papers/2209.15001)\nby Ali Hassani and Humphrey Shi.\n\nIt extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to capture global context,\nand shows significant performance improvements over it.\n\nThe abstract from the paper is the following:\n\n*Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities,\ndomains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have\nalso gained significant attention, thanks to their performance and easy integration into existing frameworks.\nThese models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA)\nor Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity,\nlocal attention weakens two of the most desirable properties of self...",
            "tasks": [],
            "display_name": "Dilated Neighborhood Attention Transformer"
        },
        {
            "model_name": "audio-spectrogram-transformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\audio-spectrogram-transformer.md",
            "release_date": "2021-04-05",
            "transformers_date": "2022-11-21",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://huggingface.co/papers/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\nThe Audio Spectrogram Transformer applies a [Vision Transformer](vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results\nfor audio classification.\n\nThe abstract from the paper is the following:\n\n*In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In...",
            "tasks": [],
            "display_name": "Audio Spectrogram Transformer"
        },
        {
            "model_name": "mobilenet_v1",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mobilenet_v1.md",
            "release_date": "2017-04-17",
            "transformers_date": "2022-11-21",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[MobileNet V1](https://huggingface.co/papers/1704.04861) is a family of efficient convolutional neural networks optimized for on-device or embedded vision tasks. It achieves this efficiency by using depth-wise separable convolutions instead of standard convolutions. The architecture allows for easy trade-offs between latency and accuracy using two main hyperparameters, a width multiplier (alpha) and an image resolution multiplier.\n\nYou can all the original MobileNet checkpoints under the [Google](https://huggingface.co/google?search_models=mobilenet) organization.\n\n> [!TIP]\n> Click on the MobileNet V1 models in the right sidebar for more examples of how to apply MobileNet to different vision tasks.\n\nThe example below demonstrates how to classify an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"image-classification\",\n    model=\"google/mobilenet_v1_1.0_224\",\n    dtype=torch.float16,\n   ...",
            "tasks": [],
            "display_name": "MobileNet V1"
        },
        {
            "model_name": "chinese_clip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\chinese_clip.md",
            "release_date": "2022-11-02",
            "transformers_date": "2022-12-01",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Chinese-CLIP model was proposed in [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://huggingface.co/papers/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\nChinese-CLIP is an implementation of CLIP (Radford et al., 2021) on a large-scale dataset of Chinese image-text pairs. It is capable of performing cross-modal retrieval and also playing as a vision backbone for vision tasks like zero-shot image classification, open-domain object detection, etc. The original Chinese-CLIP code is released [at this link](https://github.com/OFA-Sys/Chinese-CLIP).\n\nThe abstract from the paper is the following:\n\n*The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain...",
            "tasks": [],
            "display_name": "Chinese-CLIP"
        },
        {
            "model_name": "timesformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\timesformer.md",
            "release_date": "2021-02-09",
            "transformers_date": "2022-12-02",
            "modality": "video",
            "modality_name": "Video Models",
            "modality_color": "#EC4899",
            "description": "The TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://huggingface.co/papers/2102.05095) by Facebook Research.\nThis work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\n\nThe abstract from the paper is the following:\n\n*We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically...",
            "tasks": [],
            "display_name": "TimeSformer"
        },
        {
            "model_name": "biogpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\biogpt.md",
            "release_date": "2022-10-19",
            "transformers_date": "2022-12-05",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[BioGPT](https://huggingface.co/papers/2210.10341) is a generative Transformer model based on [GPT-2](./gpt2) and pretrained on 15 million PubMed abstracts. It is designed for biomedical language tasks.\n\nYou can find all the original BioGPT checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=biogpt) organization.\n\n> [!TIP]\n> Click on the BioGPT models in the right sidebar for more examples of how to apply BioGPT to different language tasks.\n\nThe example below demonstrates how to generate biomedical text with [`Pipeline`], [`AutoModel`], and also from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\ngenerator = pipeline(\n    task=\"text-generation\",\n    model=\"microsoft/biogpt\",\n    dtype=torch.float16,\n    device=0,\n)\nresult = generator(\"Ibuprofen is best used for\", truncation=True, max_length=50, do_sample=True)[0][\"generated_text\"]\nprint(result)\n```\n\n\n\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM,...",
            "tasks": [],
            "display_name": "BioGPT"
        },
        {
            "model_name": "bit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bit.md",
            "release_date": "2019-12-24",
            "transformers_date": "2022-12-07",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The BiT model was proposed in [Big Transfer (BiT): General Visual Representation Learning](https://huggingface.co/papers/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\nBiT is a simple recipe for scaling up pre-training of [ResNet](resnet)-like architectures (specifically, ResNetv2). The method results in significant improvements for transfer learning.\n\nThe abstract from the paper is the following:\n\n*Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across...",
            "tasks": [],
            "display_name": "Big Transfer (BiT)"
        },
        {
            "model_name": "gpt-sw3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt-sw3.md",
            "release_date": "2022-06-25",
            "transformers_date": "2022-12-12",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)\nby Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey \u00d6hman,\nFredrik Carlsson, Magnus Sahlgren.\n\nSince that first paper the authors have extended their work and trained new models on their new 1.2TB corpora named The Nordic Pile.\n\nGPT-Sw3 is a collection of large decoder-only pretrained transformer language models that were developed by AI Sweden\nin collaboration with RISE and the WASP WARA for Media and Language. GPT-Sw3 has been trained on a dataset containing\n320B tokens in Swedish, Norwegian, Danish, Icelandic, English, and programming code. The model was pretrained using a\ncausal language modeling (CLM) objective utilizing the NeMo Megatron GPT implementation.\n\nThis model was contributed by [AI Sweden Models](https://huggingface.co/AI-Sweden-Models).\n\n## Usage...",
            "tasks": [],
            "display_name": "GPT-Sw3"
        },
        {
            "model_name": "swin2sr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\swin2sr.md",
            "release_date": "2022-09-22",
            "transformers_date": "2022-12-16",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://huggingface.co/papers/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\nSwin2SR improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps between pre-training\nand fine-tuning, and hunger on data.\n\nThe abstract from the paper is the following:\n\n*Compression plays an important role on the efficient transmission and storage of images and videos through band-limited systems such as streaming services, virtual reality or videogames. However, compression unavoidably leads to artifacts and the loss of the original information, which may severely degrade the visual quality. For these reasons, quality enhancement of compressed images has become a popular research topic. While most state-of-the-art image restoration...",
            "tasks": [],
            "display_name": "Swin2SR"
        },
        {
            "model_name": "roberta-prelayernorm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\roberta-prelayernorm.md",
            "release_date": "2019-04-01",
            "transformers_date": "2022-12-19",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The RoBERTa-PreLayerNorm model was proposed in [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://huggingface.co/papers/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\nIt is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n\nThe abstract from the paper is the following:\n\n*fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs.*\n\nThis model was contributed by [andreasmaden](https://huggingface.co/andreasmadsen).\nThe original code can be found [here](https://github.com/princeton-nlp/DinkyTrain).\n\n## Usage tips\n\n- The implementation...",
            "tasks": [],
            "display_name": "RoBERTa-PreLayerNorm"
        },
        {
            "model_name": "blip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\blip.md",
            "release_date": "2022-01-28",
            "transformers_date": "2022-12-21",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[BLIP](https://huggingface.co/papers/2201.12086) (Bootstrapped Language-Image Pretraining) is a vision-language pretraining (VLP) framework designed for *both* understanding and generation tasks. Most existing pretrained models are only good at one or the other. It uses a captioner to generate captions and a filter to remove the noisy captions. This increases training data quality and more effectively uses the messy web data.\n\nYou can find all the original BLIP checkpoints under the [BLIP](https://huggingface.co/collections/Salesforce/blip-models-65242f40f1491fbf6a9e9472) collection.\n\n> [!TIP]\n> This model was contributed by [ybelkada](https://huggingface.co/ybelkada).\n>\n> Click on the BLIP models in the right sidebar for more examples of how to apply BLIP to different vision language tasks.\n\nThe example below demonstrates how to visual question answering with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n  ...",
            "tasks": [],
            "display_name": "BLIP"
        },
        {
            "model_name": "git",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\git.md",
            "release_date": "2022-05-27",
            "transformers_date": "2023-01-03",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The GIT model was proposed in [GIT: A Generative Image-to-text Transformer for Vision and Language](https://huggingface.co/papers/2205.14100) by\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. GIT is a decoder-only Transformer\nthat leverages [CLIP](clip)'s vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on\nimage captioning and visual question answering benchmarks.\n\nThe abstract from the paper is the following:\n\n*In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition...",
            "tasks": [],
            "display_name": "GIT"
        },
        {
            "model_name": "altclip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\altclip.md",
            "release_date": "2022-11-12",
            "transformers_date": "2023-01-04",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[AltCLIP](https://huggingface.co/papers/2211.06679) replaces the [CLIP](./clip) text encoder with a multilingual XLM-R encoder and aligns image and text representations with teacher learning and contrastive learning.\n\nYou can find all the original AltCLIP checkpoints under the [AltClip](https://huggingface.co/collections/BAAI/alt-clip-diffusion-66987a97de8525205f1221bf) collection.\n\n> [!TIP]\n> Click on the AltCLIP models in the right sidebar for more examples of how to apply AltCLIP to different tasks.\n\nThe examples below demonstrates how to calculate similarity scores between an image and one or more captions with the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import AltCLIPModel, AltCLIPProcessor\n\nmodel = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\", dtype=torch.bfloat16)\nprocessor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\nurl =...",
            "tasks": [],
            "display_name": "AltCLIP"
        },
        {
            "model_name": "mask2former",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mask2former.md",
            "release_date": "2021-12-02",
            "transformers_date": "2023-01-16",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The Mask2Former model was proposed in [Masked-attention Mask Transformer for Universal Image Segmentation](https://huggingface.co/papers/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. Mask2Former is a unified framework for panoptic, instance and semantic segmentation and features significant performance and efficiency improvements over [MaskFormer](maskformer).\n\nThe abstract from the paper is the following:\n\n*Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice\nof semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining...",
            "tasks": [],
            "display_name": "Mask2Former"
        },
        {
            "model_name": "upernet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\upernet.md",
            "release_date": "2018-07-26",
            "transformers_date": "2023-01-16",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://huggingface.co/papers/1807.10221)\nby Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a general framework to effectively segment\na wide range of concepts from images, leveraging any vision backbone like [ConvNeXt](convnext) or [Swin](swin).\n\nThe abstract from the paper is the following:\n\n*Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing...",
            "tasks": [],
            "display_name": "UPerNet"
        },
        {
            "model_name": "oneformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\oneformer.md",
            "release_date": "2022-11-10",
            "transformers_date": "2023-01-19",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The OneFormer model was proposed in [OneFormer: One Transformer to Rule Universal Image Segmentation](https://huggingface.co/papers/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal image segmentation framework that can be trained on a single panoptic dataset to perform semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference.\n\n\n\nThe abstract from the paper is the following:\n\n*Universal Image Segmentation is not a new concept. Past attempts to unify image segmentation in the last decades include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best...",
            "tasks": [],
            "display_name": "OneFormer"
        },
        {
            "model_name": "bridgetower",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bridgetower.md",
            "release_date": "2022-06-17",
            "transformers_date": "2023-01-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The BridgeTower model was proposed in [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://huggingface.co/papers/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The goal of this model is to build a\nbridge between each uni-modal encoder and the cross-modal encoder to enable comprehensive and detailed interaction at each layer of the cross-modal encoder thus achieving remarkable performance on various downstream tasks with almost negligible additional performance and computational costs.\n\nThis paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference.\n\nThe abstract from the paper is the following:\n\n*Vision-Language (VL) models with the TWO-TOWER architecture have dominated visual-language representation learning in recent years.\nCurrent VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep...",
            "tasks": [],
            "display_name": "BridgeTower"
        },
        {
            "model_name": "speecht5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\speecht5.md",
            "release_date": "2021-10-14",
            "transformers_date": "2023-02-03",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://huggingface.co/papers/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nThe abstract from the paper is the following:\n\n*Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder....",
            "tasks": [],
            "display_name": "SpeechT5"
        },
        {
            "model_name": "blip-2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\blip-2.md",
            "release_date": "2023-01-30",
            "transformers_date": "2023-02-09",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://huggingface.co/papers/2301.12597) by\nJunnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer\nencoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://huggingface.co/papers/2204.14198), an 80 billion parameter model, by 8.7%\non zero-shot VQAv2 with 54x fewer trainable parameters.\n\nThe abstract from the paper is the following:\n\n*The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders...",
            "tasks": [],
            "display_name": "BLIP-2"
        },
        {
            "model_name": "xmod",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xmod.md",
            "release_date": "2022-05-12",
            "transformers_date": "2023-02-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](https://huggingface.co/papers/2205.06266) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\nX-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (_language adapters_) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.\n\nThe abstract from the paper is the following:\n\n*Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the...",
            "tasks": [],
            "display_name": "X-MOD"
        },
        {
            "model_name": "clap",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\clap.md",
            "release_date": "2022-11-12",
            "transformers_date": "2023-02-16",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[CLAP (Contrastive Language-Audio Pretraining)](https://huggingface.co/papers/2211.06687) is a multimodal model that combines audio data with natural language descriptions through contrastive learning.\n\nIt incorporates feature fusion and keyword-to-caption augmentation to process variable-length audio inputs and to improve performance. CLAP doesn't require task-specific training data and can learn meaningful audio representations through natural language.\n\nYou can find all the original CLAP checkpoints under the [CLAP](https://huggingface.co/collections/laion/clap-contrastive-language-audio-pretraining-65415c0b18373b607262a490) collection.\n\n> [!TIP]\n> This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and [ArthurZ](https://huggingface.co/ArthurZ).\n>\n> Click on the CLAP models in the right sidebar for more examples of how to apply CLAP to different audio retrieval and classification tasks.\n\nThe example below demonstrates how to extract text embeddings with the...",
            "tasks": [],
            "display_name": "CLAP"
        },
        {
            "model_name": "efficientnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\efficientnet.md",
            "release_date": "2019-05-28",
            "transformers_date": "2023-02-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946)\nby Mingxing Tan and Quoc V. Le. EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models.\n\nThe abstract from the paper is the following:\n\n*Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and...",
            "tasks": [],
            "display_name": "EfficientNet"
        },
        {
            "model_name": "align",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\align.md",
            "release_date": "2021-02-11",
            "transformers_date": "2023-03-01",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[ALIGN](https://huggingface.co/papers/2102.05918) is pretrained on a noisy 1.8 billion alt\u2011text and image pair dataset to show that scale can make up for the noise. It uses a dual\u2011encoder architecture, [EfficientNet](./efficientnet) for images and [BERT](./bert) for text, and a contrastive loss to align similar image\u2013text embeddings together while pushing different embeddings apart. Once trained, ALIGN can encode any image and candidate captions into a shared vector space for zero\u2011shot retrieval or classification without requiring extra labels. This scale\u2011first approach reduces dataset curation costs and powers state\u2011of\u2011the\u2011art image\u2013text retrieval and zero\u2011shot ImageNet classification.\n\nYou can find all the original ALIGN checkpoints under the [Kakao Brain](https://huggingface.co/kakaobrain?search_models=align) organization.\n\n> [!TIP]\n> Click on the ALIGN models in the right sidebar for more examples of how to apply ALIGN to different vision and text related tasks.\n\nThe example below...",
            "tasks": [],
            "display_name": "ALIGN"
        },
        {
            "model_name": "informer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\informer.md",
            "release_date": "2020-12-14",
            "transformers_date": "2023-03-08",
            "modality": "timeseries",
            "modality_name": "Time Series Models",
            "modality_color": "#F97316",
            "description": "The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://huggingface.co/papers/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\n\nThis method introduces a Probabilistic Attention mechanism to select the \"active\" queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention.\n\nThe abstract from the paper is the following:\n\n*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe...",
            "tasks": [],
            "display_name": "Informer"
        },
        {
            "model_name": "mgp-str",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mgp-str.md",
            "release_date": "2022-09-08",
            "transformers_date": "2023-03-13",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The MGP-STR model was proposed in [Multi-Granularity Prediction for Scene Text Recognition](https://huggingface.co/papers/2209.03592) by Peng Wang, Cheng Da, and Cong Yao. MGP-STR is a conceptually **simple** yet **powerful** vision Scene Text Recognition (STR) model, which is built upon the [Vision Transformer (ViT)](vit). To integrate linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed to inject information from the language modality into the model in an implicit way.\n\nThe abstract from the paper is the following:\n\n*Scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this challenging problem, numerous innovative methods have been successively proposed and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet powerful vision STR model, which...",
            "tasks": [],
            "display_name": "MGP-STR"
        },
        {
            "model_name": "convnextv2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\convnextv2.md",
            "release_date": "2023-01-02",
            "transformers_date": "2023-03-14",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ConvNeXt V2 model was proposed in [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://huggingface.co/papers/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\nConvNeXt V2 is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, and a successor of [ConvNeXT](convnext).\n\nThe abstract from the paper is the following:\n\n*Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked  autoencoders (MAE). However, we found that simply combining these two...",
            "tasks": [],
            "display_name": "ConvNeXt V2"
        },
        {
            "model_name": "llama",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llama.md",
            "release_date": "2023-02-27",
            "transformers_date": "2023-03-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Llama](https://huggingface.co/papers/2302.13971) is a family of large language models ranging from 7B to 65B parameters. These models are focused on efficient inference (important for serving language models) by training a smaller model on more tokens rather than training a larger model on fewer tokens. The Llama model is based on the GPT architecture, but it uses pre-normalization to improve training stability, replaces ReLU with SwiGLU to improve performance, and replaces absolute positional embeddings with rotary positional embeddings (RoPE) to better handle longer sequence lengths.\n\nYou can find all the original Llama checkpoints under the [Huggy Llama](https://huggingface.co/huggyllama) organization.\n\n> [!TIP]\n> Click on the Llama models in the right sidebar for more examples of how to apply Llama to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom...",
            "tasks": [],
            "display_name": "Llama"
        },
        {
            "model_name": "pix2struct",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pix2struct.md",
            "release_date": "2022-10-07",
            "transformers_date": "2023-03-22",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://huggingface.co/papers/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\n\nThe abstract from the paper is the following:\n\n> Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness...",
            "tasks": [],
            "display_name": "Pix2Struct"
        },
        {
            "model_name": "nllb-moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nllb-moe.md",
            "release_date": "2022-07-11",
            "transformers_date": "2023-03-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://huggingface.co/papers/2207.04672) by Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\nLoic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\nNecip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n\nThe abstract of the paper is the following:\n\n*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.\nHowever, such efforts have...",
            "tasks": [],
            "display_name": "NLLB-MOE"
        },
        {
            "model_name": "gpt_bigcode",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt_bigcode.md",
            "release_date": "2023-01-09",
            "transformers_date": "2023-04-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://huggingface.co/papers/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del R\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\n\nThe abstract from the paper is the following:\n\n*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This...",
            "tasks": [],
            "display_name": "GPTBigCode"
        },
        {
            "model_name": "cpmant",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\cpmant.md",
            "release_date": "2022-09-16",
            "transformers_date": "2023-04-12",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "CPM-Ant is an open-source Chinese pre-trained language model (PLM) with 10B parameters. It is also the first milestone of the live training process of CPM-Live. The training process is cost-effective and environment-friendly. CPM-Ant also achieves promising results with delta tuning on the CUGE benchmark. Besides the full model, we also provide various compressed versions to meet the requirements of different hardware configurations. [See more](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)\n\nThis model was contributed by [OpenBMB](https://huggingface.co/openbmb). The original code can be found [here](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live).\n\n## Resources\n\n- A tutorial on [CPM-Live](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live).\n\n## CpmAntConfig\n\n[[autodoc]] CpmAntConfig\n    - all\n\n## CpmAntTokenizer\n\n[[autodoc]] CpmAntTokenizer\n    - all\n\n## CpmAntModel\n\n[[autodoc]] CpmAntModel\n    - all\n\n## CpmAntForCausalLM\n\n[[autodoc]] CpmAntForCausalLM\n ...",
            "tasks": [],
            "display_name": "CPMAnt"
        },
        {
            "model_name": "sam",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\sam.md",
            "release_date": "2023-04-05",
            "transformers_date": "2023-04-19",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "SAM (Segment Anything Model) was proposed in [Segment Anything](https://huggingface.co/papers/2304.02643) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n\nThe model can be used to predict segmentation masks of any object of interest given an input image.\n\n![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png)\n\nThe abstract from the paper is the following:\n\n*We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its...",
            "tasks": [],
            "display_name": "SAM"
        },
        {
            "model_name": "focalnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\focalnet.md",
            "release_date": "2022-03-22",
            "transformers_date": "2023-04-23",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The FocalNet model was proposed in [Focal Modulation Networks](https://huggingface.co/papers/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\nFocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision.\nThe authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation.\n\nThe abstract from the paper is the following:\n\n*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts...",
            "tasks": [],
            "display_name": "FocalNet"
        },
        {
            "model_name": "rwkv",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\rwkv.md",
            "release_date": "2022-08-17",
            "transformers_date": "2023-05-09",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The RWKV model (version 4) was proposed in [this repo](https://github.com/BlinkDL/RWKV-LM)\n\nIt suggests a tweak in the traditional Transformer attention to make it linear. This way, the model can be used as recurrent network: passing inputs for timestamp 0 and timestamp 1 together is the same as passing inputs at timestamp 0, then inputs at timestamp 1 along with the state of timestamp 0 (see example below).\n\nThis can be more efficient than a regular Transformer and can deal with sentence of any length (even if the model uses a fixed context length for training).\n\nThis model was contributed by [sgugger](https://huggingface.co/sgugger).\nThe original code can be found [here](https://github.com/BlinkDL/RWKV-LM).\n\n## Usage example\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, RwkvConfig, RwkvModel\n\nmodel = RwkvModel.from_pretrained(\"sgugger/rwkv-430M-pile\")\ntokenizer = AutoTokenizer.from_pretrained(\"sgugger/rwkv-430M-pile\")\n\ninputs = tokenizer(\"This is an example.\",...",
            "tasks": [],
            "display_name": "RWKV"
        },
        {
            "model_name": "swiftformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\swiftformer.md",
            "release_date": "2023-03-27",
            "transformers_date": "2023-05-12",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The SwiftFormer model was proposed in [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://huggingface.co/papers/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.\n\nThe SwiftFormer paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications. A series of models called 'SwiftFormer' is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2\u00d7 faster compared to MobileViT-v2.\n\nThe abstract from the paper is the following:\n\n*Self-attention has become a defacto choice for capturing global context in various vision...",
            "tasks": [],
            "display_name": "SwiftFormer"
        },
        {
            "model_name": "autoformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\autoformer.md",
            "release_date": "2021-06-24",
            "transformers_date": "2023-05-30",
            "modality": "timeseries",
            "modality_name": "Time Series Models",
            "modality_color": "#F97316",
            "description": "The Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://huggingface.co/papers/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\n\nThis model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\n\nThe abstract from the paper is the following:\n\n*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long...",
            "tasks": [],
            "display_name": "Autoformer"
        },
        {
            "model_name": "mobilevitv2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mobilevitv2.md",
            "release_date": "2022-06-06",
            "transformers_date": "2023-06-02",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The MobileViTV2 model was proposed in [Separable Self-attention for Mobile Vision Transformers](https://huggingface.co/papers/2206.02680) by Sachin Mehta and Mohammad Rastegari.\n\nMobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-attention in MobileViT with separable self-attention.\n\nThe abstract from the paper is the following:\n\n*Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires O(k2) time complexity with respect to the number of tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on...",
            "tasks": [],
            "display_name": "MobileViTV2"
        },
        {
            "model_name": "encodec",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\encodec.md",
            "release_date": "2022-10-24",
            "transformers_date": "2023-06-14",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://huggingface.co/papers/2210.13438) by Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n\nThe abstract from the paper is the following:\n\n*We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while...",
            "tasks": [],
            "display_name": "EnCodec"
        },
        {
            "model_name": "bort",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bort.md",
            "release_date": "2020-10-20",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we do not accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n\n\n## Overview\n\nThe BORT model was proposed in [Optimal Subarchitecture Extraction for BERT](https://huggingface.co/papers/2010.10499) by\nAdrian de Wynter and Daniel J. Perry. It is an optimal subset of architectural parameters for the BERT, which the\nauthors refer to as \"Bort\".\n\nThe abstract from the paper is the following:\n\n*We extract an optimal subset of architectural parameters for the BERT architecture from Devlin et al. (2018) by\napplying recent breakthroughs in algorithms for neural architecture search. This optimal subset, which we refer to as\n\"Bort\", is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of 5.5% the\noriginal BERT-large architecture,...",
            "tasks": [],
            "display_name": "BORT"
        },
        {
            "model_name": "deplot",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deplot.md",
            "release_date": "2022-12-20",
            "transformers_date": "2023-06-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://huggingface.co/papers/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\n\nThe abstract of the paper states the following:\n\n*Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which...",
            "tasks": [],
            "display_name": "DePlot"
        },
        {
            "model_name": "deta",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deta.md",
            "release_date": "2022-12-12",
            "transformers_date": "2023-06-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe DETA model was proposed in [NMS Strikes Back](https://huggingface.co/papers/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl.\nDETA (short for Detection Transformers with Assignment) improves [Deformable DETR](deformable_detr) by replacing the one-to-one bipartite Hungarian matching loss\nwith one-to-many label assignments used in traditional detectors with non-maximum suppression (NMS). This leads to significant gains of up to 2.5 mAP.\n\nThe abstract from the paper is the following:\n\n*Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object...",
            "tasks": [],
            "display_name": "DETA"
        },
        {
            "model_name": "efficientformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\efficientformer.md",
            "release_date": "2022-06-02",
            "transformers_date": "2023-06-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe EfficientFormer model was proposed in [EfficientFormer: Vision Transformers at MobileNet Speed](https://huggingface.co/papers/2206.01191)\nby Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.  EfficientFormer proposes a\ndimension-consistent pure transformer that can be run on mobile devices for dense prediction tasks like image classification, object\ndetection and semantic segmentation.\n\nThe abstract from the paper is the following:\n\n*Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks.\nHowever, due to the massive number of parameters and model design,...",
            "tasks": [],
            "display_name": "EfficientFormer"
        },
        {
            "model_name": "ernie_m",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ernie_m.md",
            "release_date": "2020-12-31",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe ErnieM model was proposed in [ERNIE-M: Enhanced Multilingual Representation by Aligning\nCross-lingual Semantics with Monolingual Corpora](https://huggingface.co/papers/2012.15674)  by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, Haifeng Wang.\n\nThe abstract from the paper is the following:\n\n*Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often...",
            "tasks": [],
            "display_name": "ErnieM"
        },
        {
            "model_name": "flan-t5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\flan-t5.md",
            "release_date": "2022-10-20",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "FLAN-T5 was released in the paper [Scaling Instruction-Finetuned Language Models](https://huggingface.co/papers/2210.11416) - it is an enhanced version of T5 that has been finetuned in a mixture of tasks.\n\nOne can directly use FLAN-T5 weights without finetuning the model:\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n\n>>> inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n>>> outputs = model.generate(**inputs)\n>>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['Pour a cup of bolognese into a large bowl and add the pasta']\n```\n\nFLAN-T5 includes the same improvements as T5 version 1.1 (see [here](https://huggingface.co/docs/transformers/model_doc/t5v1.1) for the full details of the model's improvements.)\n\nGoogle has released the following variants:\n\n-...",
            "tasks": [],
            "display_name": "FLAN-T5"
        },
        {
            "model_name": "flan-ul2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\flan-ul2.md",
            "release_date": "2023-03-03",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Flan-UL2](https://www.yitay.net/blog/flan-ul2-20b) is an encoder decoder model based on the T5 architecture. It uses the same configuration as the [UL2](ul2) model released earlier last year.\nIt was fine tuned using the \"Flan\" prompt tuning and dataset collection. Similar to `Flan-T5`,  one can directly use FLAN-UL2 weights without finetuning the model:\n\nAccording to the original blog here are the notable improvements:\n\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large.\n- The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning.\n- The original UL2 model also had mode switch tokens that was rather mandatory to get good performance. However, they were a little cumbersome as this requires often some changes during inference or finetuning. In this update/change, we continue training UL2 20B for an additional 100k steps (with small batch) to forget...",
            "tasks": [],
            "display_name": "FLAN-UL2"
        },
        {
            "model_name": "gptsan-japanese",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gptsan-japanese.md",
            "release_date": "2023-02-07",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe [GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) model was released in the repository by Toshiyuki Sakamoto (tanreinama).\n\nGPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\nin the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\nfine-tune for translation or summarization.\n\n### Usage example\n\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model.\n\n```python\n>>> from transformers import AutoModel, AutoTokenizer, infer_device\n>>> import torch\n\n>>> device = infer_device()\n>>> tokenizer =...",
            "tasks": [],
            "display_name": "GPTSAN-japanese"
        },
        {
            "model_name": "graphormer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\graphormer.md",
            "release_date": "2021-06-09",
            "transformers_date": "2023-06-20",
            "modality": "graph",
            "modality_name": "Graph Models",
            "modality_color": "#6B7280",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://huggingface.co/papers/2106.05234)  by\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention.\n\nThe abstract from the paper is the following:\n\n*The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive...",
            "tasks": [],
            "display_name": "Graphormer"
        },
        {
            "model_name": "jukebox",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\jukebox.md",
            "release_date": "2020-04-30",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe Jukebox model was proposed in [Jukebox: A generative model for music](https://huggingface.co/papers/2005.00341)\nby Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,\nIlya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditioned on\nan artist, genres and lyrics.\n\nThe abstract from the paper is the following:\n\n*We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate...",
            "tasks": [],
            "display_name": "Jukebox"
        },
        {
            "model_name": "matcha",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\matcha.md",
            "release_date": "2022-12-19",
            "transformers_date": "2023-06-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://huggingface.co/papers/2212.09662), from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.\n\nThe abstract of the paper states the following:\n\n*Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language...",
            "tasks": [],
            "display_name": "MatCha"
        },
        {
            "model_name": "mctct",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mctct.md",
            "release_date": "2021-10-30",
            "transformers_date": "2023-06-20",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "This model is in maintenance mode only, so we won't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n\n\n## Overview\n\nThe M-CTC-T model was proposed in [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. The model is a 1B-param transformer encoder, with a CTC head over 8065 character labels and a language identification head over 60 language ID labels. It is trained on Common Voice (version 6.1, December 2020 release) and VoxPopuli. After training on Common Voice and VoxPopuli, the model is trained on Common Voice only. The labels are unnormalized character-level transcripts (punctuation and capitalization are not removed). The model takes as input Mel filterbank...",
            "tasks": [],
            "display_name": "M-CTC-T"
        },
        {
            "model_name": "mega",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mega.md",
            "release_date": "2022-09-21",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe MEGA model was proposed in [Mega: Moving Average Equipped Gated Attention](https://huggingface.co/papers/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\nMEGA proposes a new approach to self-attention with each encoder layer having a multi-headed exponential moving average in addition to a single head of standard dot-product attention, giving the attention mechanism\nstronger positional biases. This allows MEGA to perform competitively to Transformers on standard benchmarks including LRA\nwhile also having significantly fewer parameters. MEGA's compute efficiency allows it to scale to very long...",
            "tasks": [],
            "display_name": "MEGA"
        },
        {
            "model_name": "mms",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mms.md",
            "release_date": "2023-05-22",
            "transformers_date": "2023-06-20",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://huggingface.co/papers/2305.13516)\nby Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli\n\nThe abstract from the paper is the following:\n\n*Expanding the language coverage of speech technology has the potential to improve access to information for many more people.\nHowever, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000\nlanguages spoken around the world.\nThe Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task.\nThe main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging\nself-supervised learning. We built pre-trained wav2vec 2.0...",
            "tasks": [],
            "display_name": "MMS"
        },
        {
            "model_name": "nat",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nat.md",
            "release_date": "2022-04-14",
            "transformers_date": "2023-06-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nNAT was proposed in [Neighborhood Attention Transformer](https://huggingface.co/papers/2204.07143)\nby Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n\nIt is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n\nThe abstract from the paper is the following:\n\n*We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision.\nNA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a\nlinear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows...",
            "tasks": [],
            "display_name": "Neighborhood Attention Transformer"
        },
        {
            "model_name": "nezha",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nezha.md",
            "release_date": "2019-08-31",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe Nezha model was proposed in [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://huggingface.co/papers/1909.00204) by Junqiu Wei et al.\n\nThe abstract from the paper is the following:\n\n*The pre-trained language models have achieved great successes in various natural language understanding (NLU) tasks\ndue to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora.\nIn this technical report, we present our practice of pre-training language models named NEZHA (NEural contextualiZed\nrepresentation for CHinese lAnguage understanding) on Chinese corpora and finetuning for the Chinese NLU tasks.\nThe...",
            "tasks": [],
            "display_name": "Nezha"
        },
        {
            "model_name": "open-llama",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\open-llama.md",
            "release_date": "2023-04-16",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0.\nYou can do so by running the following command: `pip install -U transformers==4.31.0`.\n\n\n\n\n\nThis model differs from the [OpenLLaMA models](https://huggingface.co/models?search=openllama) on the Hugging Face Hub, which primarily use the [LLaMA](llama) architecture.\n\n\n\n## Overview\n\nThe Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL.\n\nThe model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.\nAnd the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.\n\nThis model was contributed by [s-JoL](https://huggingface.co/s-JoL).\nThe original code was released...",
            "tasks": [],
            "display_name": "Open-Llama"
        },
        {
            "model_name": "openai-gpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\openai-gpt.md",
            "release_date": "2018-06-11",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[GPT (Generative Pre-trained Transformer)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) ([blog post](https://openai.com/index/language-unsupervised/)) focuses on effectively learning text representations and transferring them to tasks. This model trains the Transformer decoder to predict the next word, and then fine-tuned on labeled data.\n\nGPT can generate high-quality text, making it well-suited for a variety of natural language understanding tasks such as textual entailment, question answering, semantic similarity, and document classification.\n\nYou can find all the original GPT checkpoints under the [OpenAI community](https://huggingface.co/openai-community/openai-gpt) organization.\n\n> [!TIP]\n> Click on the GPT models in the right sidebar for more examples of how to apply GPT to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command...",
            "tasks": [],
            "display_name": "GPT"
        },
        {
            "model_name": "qdqbert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qdqbert.md",
            "release_date": "2020-04-20",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe QDQBERT model can be referenced in [Integer Quantization for Deep Learning Inference: Principles and Empirical\nEvaluation](https://huggingface.co/papers/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius\nMicikevicius.\n\nThe abstract from the paper is the following:\n\n*Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by\ntaking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of\nquantization parameters and evaluate their choices on a wide range of neural network models for different application\ndomains, including vision, speech, and language....",
            "tasks": [],
            "display_name": "QDQBERT"
        },
        {
            "model_name": "realm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\realm.md",
            "release_date": "2020-02-10",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe REALM model was proposed in [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It's a\nretrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then\nutilizes retrieved documents to process question answering tasks.\n\nThe abstract from the paper is the following:\n\n*Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks\nsuch as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger...",
            "tasks": [],
            "display_name": "REALM"
        },
        {
            "model_name": "retribert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\retribert.md",
            "release_date": "2020-06-12",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, so we won't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n\n\n## Overview\n\nThe [RetriBERT](https://huggingface.co/yjernite/retribert-base-uncased/tree/main) model was proposed in the blog post [Explain Anything Like I'm Five: A Model for Open Domain Long Form\nQuestion Answering](https://yjernite.github.io/lfqa.html). RetriBERT is a small model that uses either a single or\npair of BERT encoders with lower-dimension projection for dense semantic indexing of text.\n\nThis model was contributed by [yjernite](https://huggingface.co/yjernite). Code to train and use the model can be\nfound [here](https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation).\n\n## RetriBertConfig\n\n[[autodoc]] RetriBertConfig\n\n##...",
            "tasks": [],
            "display_name": "RetriBERT"
        },
        {
            "model_name": "speech_to_text_2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\speech_to_text_2.md",
            "release_date": "2021-04-14",
            "transformers_date": "2023-06-20",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\n  If you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\n  You can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n  \n\n## Overview\n\nThe Speech2Text2 model is used together with [Wav2Vec2](wav2vec2) for Speech Translation models proposed in\n[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) by\nChanghan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n\nSpeech2Text2 is a *decoder-only* transformer model that can be used with any speech *encoder-only*, such as\n[Wav2Vec2](wav2vec2) or [HuBERT](hubert) for Speech-to-Text tasks. Please refer to the\n[SpeechEncoderDecoder](speech-encoder-decoder) class on how to combine Speech2Text2 with any speech *encoder-only*\nmodel.\n\nThis model was contributed by [Patrick von...",
            "tasks": [],
            "display_name": "Speech2Text2"
        },
        {
            "model_name": "t5v1.1",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\t5v1.1.md",
            "release_date": "2020-02-12",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "T5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)\nrepository by Colin Raffel et al. It's an improved version of the original T5 model.\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511).\n\n## Usage tips\n\nOne can directly plug in the weights of T5v1.1 into a T5 model, like so:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-base\")\n```\n\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\n\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://huggingface.co/papers/2002.05202).\n\n- Dropout was turned off in...",
            "tasks": [],
            "display_name": "T5v1.1"
        },
        {
            "model_name": "tapex",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\tapex.md",
            "release_date": "2021-07-16",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n\n\n## Overview\n\nThe TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) by Qian Liu,\nBei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after\nwhich it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.\n\nTAPEX has been fine-tuned on several datasets:\n\n- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\n- [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford...",
            "tasks": [],
            "display_name": "TAPEX"
        },
        {
            "model_name": "trajectory_transformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\trajectory_transformer.md",
            "release_date": "2021-06-03",
            "transformers_date": "2023-06-20",
            "modality": "reinforcement",
            "modality_name": "Reinforcement Learning",
            "modality_color": "#EF4444",
            "description": "This model is in maintenance mode only, so we won't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n\n\n## Overview\n\nThe Trajectory Transformer model was proposed in [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://huggingface.co/papers/2106.02039)  by Michael Janner, Qiyang Li, Sergey Levine.\n\nThe abstract from the paper is the following:\n\n*Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models,\nleveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence\nmodeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards.\nViewed in this way, it is tempting to consider whether high-capacity sequence prediction models...",
            "tasks": [],
            "display_name": "Trajectory Transformer"
        },
        {
            "model_name": "transfo-xl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\transfo-xl.md",
            "release_date": "2019-01-09",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, so we won't accept any new PRs changing its code. This model was deprecated due to security issues linked to `pickle.load`.\n\nWe recommend switching to more recent models for improved security.\n\nIn case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub.\n\nYou will need to set the environment variable `TRUST_REMOTE_CODE` to `True` in order to allow the\nusage of `pickle.load()`:\n\n```python\nimport os\nfrom transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n\nos.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n\ncheckpoint = 'transfo-xl/transfo-xl-wt103'\nrevision = '40a186da79458c9f9de846edfaea79c412137f97'\n\ntokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\nmodel = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n```\n\nIf you run...",
            "tasks": [],
            "display_name": "Transformer XL"
        },
        {
            "model_name": "tvlt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\tvlt.md",
            "release_date": "2022-09-28",
            "transformers_date": "2023-06-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe TVLT model was proposed in [TVLT: Textless Vision-Language Transformer](https://huggingface.co/papers/2209.14156)\nby Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal (the first three authors contributed equally). The Textless Vision-Language Transformer (TVLT) is a model that uses raw visual and audio inputs for vision-and-language representation learning, without using text-specific modules such as tokenization or automatic speech recognition (ASR). It can perform various audiovisual and vision-language tasks like retrieval, question answering, etc.\n\nThe abstract from the paper is the following:\n\n*In this work, we present the Textless Vision-Language Transformer (TVLT), where...",
            "tasks": [],
            "display_name": "TVLT"
        },
        {
            "model_name": "ul2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ul2.md",
            "release_date": "2022-05-10",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The T5 model was presented in [Unifying Language Learning Paradigms](https://huggingface.co/papers/2205.05131) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n\nThe abstract from the paper is the following:\n\n*Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose...",
            "tasks": [],
            "display_name": "UL2"
        },
        {
            "model_name": "van",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\van.md",
            "release_date": "2022-02-20",
            "transformers_date": "2023-06-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\n\n\n\n## Overview\n\nThe VAN model was proposed in [Visual Attention Network](https://huggingface.co/papers/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n\nThis paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations.\n\nThe abstract from the paper is the following:\n\n*While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings...",
            "tasks": [],
            "display_name": "VAN"
        },
        {
            "model_name": "vit_hybrid",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vit_hybrid.md",
            "release_date": "2020-10-22",
            "transformers_date": "2023-06-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n## Overview\n\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://huggingface.co/papers/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose...",
            "tasks": [],
            "display_name": "Hybrid Vision Transformer (ViT Hybrid)"
        },
        {
            "model_name": "xclip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xclip.md",
            "release_date": "2022-08-04",
            "transformers_date": "2023-06-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The X-CLIP model was proposed in [Expanding Language-Image Pretrained Models for General Video Recognition](https://huggingface.co/papers/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\nX-CLIP is a minimal extension of [CLIP](clip) for video. The model consists of a text encoder, a cross-frame vision encoder, a multi-frame integration Transformer, and a video-specific prompt generator.\n\nThe abstract from the paper is the following:\n\n*Contrastive language-image pretraining has shown great success in learning visual-textual joint representation from web-scale data, demonstrating remarkable \"zero-shot\" generalization ability for various image tasks. However, how to effectively expand such new language-image pretraining methods to video domains is still an open problem. In this work, we present a simple yet effective approach that adapts the pretrained language-image models to video recognition directly, instead of...",
            "tasks": [],
            "display_name": "X-CLIP"
        },
        {
            "model_name": "xlm-prophetnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlm-prophetnet.md",
            "release_date": "2020-01-13",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "This model is in maintenance mode only, we don't accept any new PRs changing its code.\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.40.2.\nYou can do so by running the following command: `pip install -U transformers==4.40.2`.\n\n\n\n\n\n\n\n\n\n\n\n\n**DISCLAIMER:** If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title) and assign\n@patrickvonplaten\n\n## Overview\n\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\nZhang, Ming Zhou on 13 Jan, 2020.\n\nXLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of\njust the next token. Its architecture is identical to ProhpetNet, but the model was...",
            "tasks": [],
            "display_name": "XLM-ProphetNet"
        },
        {
            "model_name": "xlm-v",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlm-v.md",
            "release_date": "2023-01-25",
            "transformers_date": "2023-06-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "XLM-V is multilingual language model with a one million token vocabulary trained on 2.5TB of data from Common Crawl (same as XLM-R).\nIt was introduced in the [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://huggingface.co/papers/2301.10472)\npaper by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer and Madian Khabsa.\n\nFrom the abstract of the XLM-V paper:\n\n*Large multilingual language models typically rely on a single vocabulary shared across 100+ languages.\nAs these models have increased in parameter count and depth, vocabulary size has remained largely unchanged.\nThis vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R.\nIn this paper, we introduce a new approach for scaling to very large multilingual vocabularies by\nde-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity\nto achieve sufficient...",
            "tasks": [],
            "display_name": "XLM-V"
        },
        {
            "model_name": "xlsr_wav2vec2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlsr_wav2vec2.md",
            "release_date": "2020-06-24",
            "transformers_date": "2023-06-20",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://huggingface.co/papers/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael\nAuli.\n\nThe abstract from the paper is the following:\n\n*This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw\nwaveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over\nmasked latent speech representations and jointly learns a quantization of the latents shared across languages. The\nresulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly\noutperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction\nof 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to\na...",
            "tasks": [],
            "display_name": "XLSR-Wav2Vec2"
        },
        {
            "model_name": "xls_r",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xls_r.md",
            "release_date": "2021-11-17",
            "transformers_date": "2023-06-20",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The XLS-R model was proposed in [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://huggingface.co/papers/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman\nGoyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n\nThe abstract from the paper is the following:\n\n*This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.\nWe train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128\nlanguages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range\nof tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation\nbenchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into\nEnglish. For speech...",
            "tasks": [],
            "display_name": "XLS-R"
        },
        {
            "model_name": "instructblip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\instructblip.md",
            "release_date": "2023-05-11",
            "transformers_date": "2023-06-26",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The InstructBLIP model was proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://huggingface.co/papers/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\nInstructBLIP leverages the [BLIP-2](blip2) architecture for visual instruction tuning.\n\nThe abstract from the paper is the following:\n\n*General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2...",
            "tasks": [],
            "display_name": "InstructBLIP"
        },
        {
            "model_name": "musicgen",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\musicgen.md",
            "release_date": "2023-06-08",
            "transformers_date": "2023-06-29",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The MusicGen model was proposed in the paper [Simple and Controllable Music Generation](https://huggingface.co/papers/2306.05284)\nby Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre D\u00e9fossez.\n\nMusicGen is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned\non text descriptions or audio prompts. The text descriptions are passed through a frozen text encoder model to obtain a\nsequence of hidden-state representations. MusicGen is then trained to predict discrete audio tokens, or *audio codes*,\nconditioned on these hidden-states. These audio tokens are then decoded using an audio compression model, such as EnCodec,\nto recover the audio waveform.\n\nThrough an efficient token interleaving pattern, MusicGen does not require a self-supervised semantic representation of\nthe text/audio prompts, thus eliminating the need to cascade multiple models to predict a set of codebooks...",
            "tasks": [],
            "display_name": "MusicGen"
        },
        {
            "model_name": "umt5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\umt5.md",
            "release_date": "2023-04-18",
            "transformers_date": "2023-07-03",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The UMT5 model was proposed in [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://huggingface.co/papers/2304.09151) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n\nThe abstract from the paper is the following:\n\n*Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax...",
            "tasks": [],
            "display_name": "UMT5"
        },
        {
            "model_name": "mra",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mra.md",
            "release_date": "2022-07-21",
            "transformers_date": "2023-07-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MRA model was proposed in [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://huggingface.co/papers/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh.\n\nThe abstract from the paper is the following:\n\n*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified sparsity patterns, low-rank basis expansions and combinations thereof. In this paper, we revisit classical Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value in this setting remains underexplored thus far. We show that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a...",
            "tasks": [],
            "display_name": "MRA"
        },
        {
            "model_name": "falcon",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\falcon.md",
            "release_date": "2023-11-28",
            "transformers_date": "2023-07-11",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Falcon](https://huggingface.co/papers/2311.16867) is a family of large language models, available in 7B, 40B, and 180B parameters, as pretrained and instruction tuned variants. This model focuses on scaling pretraining over three categories, performance, data, and hardware. Falcon uses multigroup attention to significantly reduce inference memory requirements and rotary positional embeddings (RoPE). These models are pretrained on [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality and deduplicated 5T token dataset.\n\nYou can find all the original Falcon checkpoints under the [Falcon](https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a) collection.\n\n> [!TIP]\n> Click on the Falcon models in the right sidebar for more examples of how to apply Falcon to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers...",
            "tasks": [],
            "display_name": "Falcon"
        },
        {
            "model_name": "vivit",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vivit.md",
            "release_date": "2021-03-29",
            "transformers_date": "2023-07-11",
            "modality": "video",
            "modality_name": "Video Models",
            "modality_color": "#EC4899",
            "description": "The Vivit model was proposed in [ViViT: A Video Vision Transformer](https://huggingface.co/papers/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, Cordelia Schmid.\nThe paper proposes one of the first successful pure-transformer based set of models for video understanding.\n\nThe abstract from the paper is the following:\n\n*We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage...",
            "tasks": [],
            "display_name": "Video Vision Transformer (ViViT)"
        },
        {
            "model_name": "bark",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bark.md",
            "release_date": "2023-04-09",
            "transformers_date": "2023-07-17",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[Bark](https://huggingface.co/suno/bark) is a transformer-based text-to-speech model proposed by Suno AI in [suno-ai/bark](https://github.com/suno-ai/bark).\n\nBark is made of 4 main models:\n\n- [`BarkSemanticModel`] (also referred to as the 'text' model): a causal auto-regressive transformer model that takes as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.\n- [`BarkCoarseModel`] (also referred to as the 'coarse acoustics' model): a causal autoregressive transformer, that takes as input the results of the [`BarkSemanticModel`] model. It aims at predicting the first two audio codebooks necessary for EnCodec.\n- [`BarkFineModel`] (the 'fine acoustics' model), this time a non-causal autoencoder transformer, which iteratively predicts the last codebooks based on the sum of the previous codebooks embeddings.\n- having predicted all the codebook channels from the [`EncodecModel`], Bark uses it to decode the output audio array.\n\nIt should be noted...",
            "tasks": [],
            "display_name": "Bark"
        },
        {
            "model_name": "dinov2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dinov2.md",
            "release_date": "2023-04-14",
            "transformers_date": "2023-07-18",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[DINOv2](https://huggingface.co/papers/2304.07193) is a vision foundation model that uses [ViT](./vit) as a feature extractor for multiple downstream tasks like image classification and depth estimation. It focuses on stabilizing and accelerating training through techniques like a faster memory-efficient attention, sequence packing, improved stochastic depth, Fully Sharded Data Parallel (FSDP), and model distillation.\n\nYou can find all the original DINOv2 checkpoints under the [Dinov2](https://huggingface.co/collections/facebook/dinov2-6526c98554b3d2576e071ce3) collection.\n\n> [!TIP]\n> Click on the DINOv2 models in the right sidebar for more examples of how to apply DINOv2 to different vision tasks.\n\nThe example below demonstrates how to obtain an image embedding with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"image-classification\",\n    model=\"facebook/dinov2-small-imagenet1k-1-layer\",\n   ...",
            "tasks": [],
            "display_name": "DINOv2"
        },
        {
            "model_name": "llama2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llama2.md",
            "release_date": "2023-07-18",
            "transformers_date": "2023-07-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Llama 2](https://huggingface.co/papers/2307.09288) is a family of large language models, Llama 2 and Llama 2-Chat, available in 7B, 13B, and 70B parameters. The Llama 2 model mostly keeps the same architecture as [Llama](./llama), but it is pretrained on more tokens, doubles the context length, and uses grouped-query attention (GQA) in the 70B model to improve inference.\n\nLlama 2-Chat is trained with supervised fine-tuning (SFT), and reinforcement learning with human feedback (RLHF) - rejection sampling and proximal policy optimization (PPO) - is applied to the fine-tuned model to align the chat model with human preferences.\n\nYou can find all the original Llama 2 checkpoints under the [Llama 2 Family](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b) collection.\n\n> [!TIP]\n> Click on the Llama 2 models in the right sidebar for more examples of how to apply Llama to different language tasks.\n\nThe example below demonstrates how to generate text with...",
            "tasks": [],
            "display_name": "Llama 2"
        },
        {
            "model_name": "pvt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pvt.md",
            "release_date": "2021-02-24",
            "transformers_date": "2023-07-24",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://huggingface.co/papers/2102.12122)\nby Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. The PVT is a type of\nvision transformer that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks. Specifically\nit allows for more fine-grained inputs (4 x 4 pixels per patch) to be used, while simultaneously shrinking the sequence length\nof the Transformer as it deepens - reducing the computational cost. Additionally, a spatial-reduction attention (SRA) layer\nis used to further reduce the resource consumption when learning high-resolution features.\n\nThe abstract from the paper is the following:\n\n*Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a\nsimpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the...",
            "tasks": [],
            "display_name": "Pyramid Vision Transformer (PVT)"
        },
        {
            "model_name": "mpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mpt.md",
            "release_date": "2023-05-05",
            "transformers_date": "2023-07-25",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MPT model was proposed by the [MosaicML](https://www.mosaicml.com/) team and released with multiple sizes and finetuned variants. The MPT models are a series of open source and commercially usable LLMs pre-trained on 1T tokens.\n\nMPT models are GPT-style decoder-only transformers with several improvements: performance-optimized layer implementations, architecture changes that provide greater training stability, and the elimination of context length limits by replacing positional embeddings with ALiBi.\n\n- MPT base: MPT base pre-trained models on next token prediction\n- MPT instruct: MPT base models fine-tuned on instruction based tasks\n- MPT storywriter: MPT base models fine-tuned for 2500 steps on 65k-token excerpts of fiction books contained in the books3 corpus, this enables the model to handle very long sequences\n\nThe original code is available at the  [`llm-foundry`](https://github.com/mosaicml/llm-foundry/tree/main) repository.\n\nRead more about it [in the release...",
            "tasks": [],
            "display_name": "MPT"
        },
        {
            "model_name": "idefics",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\idefics.md",
            "release_date": "2023-06-21",
            "transformers_date": "2023-08-18",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The IDEFICS model was proposed in [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents\n](https://huggingface.co/papers/2306.16527\n) by Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh\n\nThe abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We...",
            "tasks": [],
            "display_name": "IDEFICS"
        },
        {
            "model_name": "pop2piano",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pop2piano.md",
            "release_date": "2022-11-02",
            "transformers_date": "2023-08-21",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover Generation](https://huggingface.co/papers/2211.00895) by Jongho Choi and Kyogu Lee.\n\nPiano covers of pop music are widely enjoyed, but generating them from music is not a trivial task. It requires great\nexpertise with playing piano as well as knowing different characteristics and melodies of a song. With Pop2Piano you\ncan directly generate a cover from a song's audio waveform. It is the first model to directly generate a piano cover\nfrom pop audio without melody and chord extraction modules.\n\nPop2Piano is an encoder-decoder Transformer model based on [T5](https://huggingface.co/papers/1910.10683). The input audio\nis transformed to its waveform and passed to the encoder, which transforms it to a latent representation. The decoder\nuses these latent representations to generate token ids in an autoregressive way. Each token id corresponds to one of four\ndifferent token types: time, velocity, note and 'special'....",
            "tasks": [],
            "display_name": "Pop2Piano"
        },
        {
            "model_name": "code_llama",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\code_llama.md",
            "release_date": "2023-08-24",
            "transformers_date": "2023-08-25",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Code Llama](https://huggingface.co/papers/2308.12950) is a specialized family of large language models based on [Llama 2](./llama2) for coding tasks.  It comes in different flavors - general code, Python-specific, and instruction-following variant - all available in 7B, 13B, 34B, and 70B parameters. Code Llama models can generate, explain, and even fill in missing parts of your code (called \"infilling\"). It can also handle very long contexts with stable generation up to 100k tokens, even though it was trained on sequences of 16K tokens.\n\nYou can find all the original Code Llama checkpoints under the [Code Llama](https://huggingface.co/collections/meta-llama/code-llama-family-661da32d0a9d678b6f55b933) collection.\n\n> [!TIP]\n> Click on the Code Llama models in the right sidebar for more examples of how to apply Code Llama to different coding tasks.\n\nThe example below demonstrates how to generate code with [`Pipeline`], or the [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport...",
            "tasks": [],
            "display_name": "CodeLlama"
        },
        {
            "model_name": "vitdet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vitdet.md",
            "release_date": "2022-03-30",
            "transformers_date": "2023-08-29",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ViTDet model was proposed in [Exploring Plain Vision Transformer Backbones for Object Detection](https://huggingface.co/papers/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.\nVitDet leverages the plain [Vision Transformer](vit) for the task of object detection.\n\nThe abstract from the paper is the following:\n\n*We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones...",
            "tasks": [],
            "display_name": "ViTDet"
        },
        {
            "model_name": "vits",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vits.md",
            "release_date": "2021-06-11",
            "transformers_date": "2023-09-01",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech)](https://huggingface.co/papers/2106.06103) is a end-to-end speech synthesis model, simplifying the traditional two-stage text-to-speech (TTS) systems. It's unique because it directly synthesizes speech from text using variational inference, adversarial learning, and normalizing flows to produce natural and expressive speech with diverse rhythms and intonations.\n\nYou can find all the original VITS checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=mms-tts) organization.\n\n> [!TIP]\n> Click on the VITS models in the right sidebar for more examples of how to apply VITS.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline, set_seed\nfrom scipy.io.wavfile import write\n\nset_seed(555)\n\npipe = pipeline(\n    task=\"text-to-speech\",\n   ...",
            "tasks": [],
            "display_name": "VITS"
        },
        {
            "model_name": "persimmon",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\persimmon.md",
            "release_date": "2023-09-07",
            "transformers_date": "2023-09-12",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Persimmon model was created by [ADEPT](https://www.adept.ai/blog/persimmon-8b), and authored by Erich Elsen, Augustus Odena, Maxwell Nye, Sa\u011fnak Ta\u015f\u0131rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.\n\nThe authors introduced Persimmon-8B, a decoder model based on the classic transformers architecture, with query and key normalization. Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters, released under the Apache license.  Some of the key attributes of Persimmon-8B are long context size (16K), performance, and capabilities for multimodal extensions.\n\nThe authors showcase their approach to model evaluation, focusing on practical text generation, mirroring how users interact with language models. The work also includes a comparative analysis, pitting Persimmon-8B against other prominent models (MPT 7B Instruct and Llama 2 Base 7B 1-Shot), across various evaluation tasks. The results demonstrate Persimmon-8B's competitive...",
            "tasks": [],
            "display_name": "Persimmon"
        },
        {
            "model_name": "bros",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bros.md",
            "release_date": "2021-08-10",
            "transformers_date": "2023-09-15",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The BROS model was proposed in [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://huggingface.co/papers/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.\n\nBROS stands for *BERT Relying On Spatiality*. It is an encoder-only Transformer model that takes a sequence of tokens and their bounding boxes as inputs and outputs a sequence of hidden states. BROS encode relative spatial information instead of using absolute spatial information.\n\nIt is pre-trained with two objectives: a token-masked language modeling objective (TMLM) used in BERT, and a novel area-masked language modeling objective (AMLM)\nIn TMLM, tokens are randomly masked, and the model predicts the masked tokens using spatial information and other unmasked tokens.\nAMLM is a 2D version of TMLM. It randomly masks text tokens and predicts with the same information as TMLM, but it masks text blocks...",
            "tasks": [],
            "display_name": "BROS"
        },
        {
            "model_name": "vitmatte",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vitmatte.md",
            "release_date": "2023-05-24",
            "transformers_date": "2023-09-19",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The ViTMatte model was proposed in [Boosting Image Matting with Pretrained Plain Vision Transformers](https://huggingface.co/papers/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\nViTMatte leverages plain [Vision Transformers](vit) for the task of image matting, which is the process of accurately estimating the foreground object in images and videos.\n\nThe abstract from the paper is the following:\n\n*Recently, plain vision Transformers (ViTs) have shown impressive performance on various computer vision tasks, thanks to their strong modeling capacity and large-scale pretraining. However, they have not yet conquered the problem of image matting. We hypothesize that image matting could also be boosted by ViTs and present a new efficient and robust ViT-based matting system, named ViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with a convolution neck to help ViTs achieve an excellent performance-computation trade-off in matting tasks. (ii)...",
            "tasks": [],
            "display_name": "ViTMatte"
        },
        {
            "model_name": "nougat",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nougat.md",
            "release_date": "2023-08-25",
            "transformers_date": "2023-09-26",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Nougat model was proposed in [Nougat: Neural Optical Understanding for Academic Documents](https://huggingface.co/papers/2308.13418) by\nLukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. Nougat uses the same architecture as [Donut](donut), meaning an image Transformer\nencoder and an autoregressive text Transformer decoder to translate scientific PDFs to markdown, enabling easier access to them.\n\nThe abstract from the paper is the following:\n\n*Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed...",
            "tasks": [],
            "display_name": "Nougat"
        },
        {
            "model_name": "mistral",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mistral.md",
            "release_date": "2023-10-10",
            "transformers_date": "2023-09-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Mistral](https://huggingface.co/papers/2310.06825) is a 7B parameter language model, available as a pretrained and instruction-tuned variant, focused on balancing\nthe scaling costs of large models with performance and efficient inference. This model uses sliding window attention (SWA) trained with a 8K context length and a fixed cache size to handle longer sequences more effectively. Grouped-query attention (GQA) speeds up inference and reduces memory requirements. Mistral also features a byte-fallback BPE tokenizer to improve token handling and efficiency by ensuring characters are never mapped to out-of-vocabulary tokens.\n\nYou can find all the original Mistral checkpoints under the [Mistral AI_](https://huggingface.co/mistralai) organization.\n\n> [!TIP]\n> Click on the Mistral models in the right sidebar for more examples of how to apply Mistral to different language tasks.\n\nThe example below demonstrates how to chat with [`Pipeline`] or the [`AutoModel`], and from the command...",
            "tasks": [],
            "display_name": "Mistral"
        },
        {
            "model_name": "owlv2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\owlv2.md",
            "release_date": "2023-06-16",
            "transformers_date": "2023-10-13",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "OWLv2 was proposed in [Scaling Open-Vocabulary Object Detection](https://huggingface.co/papers/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2 scales up [OWL-ViT](owlvit) using self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. This results in large gains over the previous state-of-the-art for zero-shot object detection.\n\nThe abstract from the paper is the following:\n\n*Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space,...",
            "tasks": [],
            "display_name": "OWLv2"
        },
        {
            "model_name": "fuyu",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\fuyu.md",
            "release_date": "2023-10-17",
            "transformers_date": "2023-10-19",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Fuyu model was created by [ADEPT](https://www.adept.ai/blog/fuyu-8b), and authored by Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sa\u011fnak Ta\u015f\u0131rlar.\n\nThe authors introduced Fuyu-8B, a decoder-only multimodal model based on the classic transformers architecture, with query and key normalization. A linear encoder is added to create multimodal embeddings from image inputs.\n\nBy treating image tokens like text tokens and using a special image-newline character, the model knows when an image line ends. Image positional embeddings are removed. This avoids the need for different training phases for various image resolutions. With 8 billion parameters and licensed under CC-BY-NC, Fuyu-8B is notable for its ability to handle both text and images, its impressive context size of 16K, and its overall performance.\n\n\n\nThe `Fuyu` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use...",
            "tasks": [],
            "display_name": "Fuyu"
        },
        {
            "model_name": "seamless_m4t",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\seamless_m4t.md",
            "release_date": "2023-08-22",
            "transformers_date": "2023-10-23",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The SeamlessM4T model was proposed in [SeamlessM4T \u2014 Massively Multilingual & Multimodal Machine Translation](https://huggingface.co/papers/2308.11596) by the Seamless Communication team from Meta AI.\n\nThis is the **version 1** release of the model. For the updated **version 2** release, refer to the [Seamless M4T v2 docs](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t_v2).\n\nSeamlessM4T is a collection of models designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text.\n\nSeamlessM4T enables multiple tasks without relying on separate models:\n\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR)\n\n[`SeamlessM4TModel`] can perform all the above tasks, but each task also has its own dedicated sub-model.\n\nThe abstract from the paper is the...",
            "tasks": [],
            "display_name": "SeamlessM4T"
        },
        {
            "model_name": "kosmos-2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\kosmos-2.md",
            "release_date": "2023-06-26",
            "transformers_date": "2023-10-30",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The KOSMOS-2 model was proposed in [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://huggingface.co/papers/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.\n\nKOSMOS-2 is a Transformer-based causal language model and is trained using the next-word prediction task on a web-scale\ndataset of grounded image-text pairs [GRIT](https://huggingface.co/datasets/zzliang/GRIT). The spatial coordinates of\nthe bounding boxes in the dataset are converted to a sequence of location tokens, which are appended to their respective\nentity text spans (for example, `a snowman` followed by ``). The data format is\nsimilar to \u201chyperlinks\u201d that connect the object regions in an image to their text span in the corresponding caption.\n\nThe abstract from the paper is the following:\n\n*We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding...",
            "tasks": [],
            "display_name": "KOSMOS-2"
        },
        {
            "model_name": "clvp",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\clvp.md",
            "release_date": "2023-05-12",
            "transformers_date": "2023-11-10",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed in [Better speech synthesis through scaling](https://huggingface.co/papers/2305.07243) by James Betker.\n\nThe abstract from the paper is the following:\n\n*In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise - an expressive, multi-voice text-to-speech system.*\n\nThis model was contributed by [Susnato Dhar](https://huggingface.co/susnato).\nThe original code can be found [here](https://github.com/neonbjb/tortoise-tts).\n\n## Usage tips\n\n1. CLVP is an integral part of the...",
            "tasks": [],
            "display_name": "CLVP"
        },
        {
            "model_name": "phi",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\phi.md",
            "release_date": "2023-06-20",
            "transformers_date": "2023-11-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Phi](https://huggingface.co/papers/2306.11644) is a 1.3B parameter transformer model optimized for Python code generation. It focuses on \"textbook-quality\" training data of code examples, exercises and synthetic Python problems rather than scaling the model size or compute.\n\nYou can find all the original Phi checkpoints under the [Phi-1](https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572) collection.\n\n> [!TIP]\n> Click on the Phi models in the right sidebar for more examples of how to apply Phi to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"microsoft/phi-1.5\", device=0, dtype=torch.bfloat16)\npipeline(\"pipeline('''def print_prime(n): \"\"\" Print all primes between 1 and n\"\"\"''')\")\n\n```\n\n\n\n\n\n```py\nimport torch\nfrom transformers import AutoTokenizer,...",
            "tasks": [],
            "display_name": "Phi"
        },
        {
            "model_name": "patchtst",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\patchtst.md",
            "release_date": "2022-11-27",
            "transformers_date": "2023-11-13",
            "modality": "timeseries",
            "modality_name": "Time Series Models",
            "modality_color": "#F97316",
            "description": "The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://huggingface.co/papers/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n\nAt a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:\n\n![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)\n\nThe abstract from the paper is the following:\n\n*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single...",
            "tasks": [],
            "display_name": "PatchTST"
        },
        {
            "model_name": "tvp",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\tvp.md",
            "release_date": "2023-03-09",
            "transformers_date": "2023-11-22",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The text-visual prompting (TVP) framework was proposed in the paper [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://huggingface.co/papers/2303.04995) by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.\n\nThe abstract from the paper is the following:\n\n*In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call \u2018prompts\u2019) into both visual inputs and textual features of a TVG...",
            "tasks": [],
            "display_name": "TVP"
        },
        {
            "model_name": "univnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\univnet.md",
            "release_date": "2021-06-15",
            "transformers_date": "2023-11-22",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The UnivNet model was proposed in [UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://huggingface.co/papers/2106.07889) by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kin, and Juntae Kim.\nThe UnivNet model is a generative adversarial network (GAN) trained to synthesize high fidelity speech waveforms. The UnivNet model shared in `transformers` is the *generator*, which maps a conditioning log-mel spectrogram and optional noise sequence to a speech waveform (e.g. a vocoder). Only the generator is required for inference. The *discriminator* used to train the `generator` is not implemented.\n\nThe abstract from the paper is the following:\n\n*Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing...",
            "tasks": [],
            "display_name": "UnivNet"
        },
        {
            "model_name": "madlad-400",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\madlad-400.md",
            "release_date": "2023-09-09",
            "transformers_date": "2023-11-28",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "MADLAD-400 models were released in the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://huggingface.co/papers/2309.04662).\n\nThe abstract from the paper is the following:\n\n*We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss\nthe limitations revealed by self-auditing MADLAD-400, and the role data auditing\nhad in the dataset creation process. We then train and release a 10.7B-parameter\nmultilingual machine translation model on 250 billion tokens covering over 450\nlanguages using publicly available data, and find that it is competitive with models\nthat are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot\ntranslation. We make the baseline models 1\navailable to the research community.*\n\nThis model was added by [Juarez Bochi](https://huggingface.co/jbochi). The...",
            "tasks": [],
            "display_name": "MADLAD-400"
        },
        {
            "model_name": "seamless_m4t_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\seamless_m4t_v2.md",
            "release_date": "2023-12-08",
            "transformers_date": "2023-11-30",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The SeamlessM4T-v2 model was proposed in [Seamless: Multilingual Expressive and Streaming Speech Translation](https://huggingface.co/papers/2312.05187) by the Seamless Communication team from Meta AI.\n\nSeamlessM4T-v2 is a collection of models designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text. It is an improvement on the [previous version](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t). For more details on the differences between v1 and v2, refer to section [Difference with SeamlessM4T-v1](#difference-with-seamlessm4t-v1).\n\nSeamlessM4T-v2 enables multiple tasks without relying on separate models:\n\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR)\n\n[`SeamlessM4Tv2Model`] can perform all the above tasks, but each task also has its own...",
            "tasks": [],
            "display_name": "SeamlessM4T-v2"
        },
        {
            "model_name": "patchtsmixer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\patchtsmixer.md",
            "release_date": "2023-06-14",
            "transformers_date": "2023-12-05",
            "modality": "timeseries",
            "modality_name": "Time Series Models",
            "modality_color": "#F97316",
            "description": "The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://huggingface.co/papers/2306.09364) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n\nPatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.\n\nThe abstract from the paper is the following:\n\n*TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP)...",
            "tasks": [],
            "display_name": "PatchTSMixer"
        },
        {
            "model_name": "llava",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llava.md",
            "release_date": "2023-04-17",
            "transformers_date": "2023-12-07",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. In other words, it is an multi-modal version of LLMs fine-tuned for chat / instructions.\n\nThe LLaVa model was proposed in [Visual Instruction Tuning](https://huggingface.co/papers/2304.08485) and improved in [Improved Baselines with Visual Instruction Tuning](https://huggingface.co/papers/2310.03744) by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.\n\nThe abstract from the paper is the following:\n\n*Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple...",
            "tasks": [],
            "display_name": "LLaVa"
        },
        {
            "model_name": "mixtral",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mixtral.md",
            "release_date": "2023-12-11",
            "transformers_date": "2023-12-11",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Mixtral-8x7B](https://huggingface.co/papers/2401.04088) was introduced in the [Mixtral of Experts blogpost](https://mistral.ai/news/mixtral-of-experts/) by Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed.\n\nThe introduction of the blog post says:\n\n*Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts models (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.*\n\nMixtral-8x7B is the second large language...",
            "tasks": [],
            "display_name": "Mixtral"
        },
        {
            "model_name": "vipllava",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vipllava.md",
            "release_date": "2023-12-01",
            "transformers_date": "2023-12-13",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The VipLlava model was proposed in [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://huggingface.co/papers/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.\n\nVipLlava enhances the training protocol of Llava by marking images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\" during training.\n\nThe abstract from the paper is the following:\n\n*While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red...",
            "tasks": [],
            "display_name": "VipLlava"
        },
        {
            "model_name": "fastspeech2_conformer",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\fastspeech2_conformer.md",
            "release_date": "2020-10-26",
            "transformers_date": "2024-01-03",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The FastSpeech2Conformer model was proposed with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://huggingface.co/papers/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.\n\nThe abstract from the original FastSpeech2 paper is the following:\n\n*Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech...",
            "tasks": [],
            "display_name": "FastSpeech2Conformer"
        },
        {
            "model_name": "siglip",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\siglip.md",
            "release_date": "2023-03-27",
            "transformers_date": "2024-01-08",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[SigLIP](https://huggingface.co/papers/2303.15343) is a multimodal image-text model similar to [CLIP](clip). It uses separate image and text encoders to generate representations for both modalities.\n\nUnlike CLIP, SigLIP employs a pairwise sigmoid loss on image-text pairs during training. This training loss eliminates the need for a global view of all pairwise similarities between images and texts within a batch. Consequently, it enables more efficient scaling to larger batch sizes while also delivering superior performance with smaller batch sizes.\n\nYou can find all the original SigLIP checkpoints under the [SigLIP](https://huggingface.co/collections/google/siglip-659d5e62f0ae1a57ae0e83ba) collection.\n\n> [!TIP]\n> Click on the SigLIP models in the right sidebar for more examples of how to apply SigLIP to different image and text tasks.\n\nThe example below demonstrates how to generate similarity scores between texts and image(s) with [`Pipeline`] or the [`AutoModel`]...",
            "tasks": [],
            "display_name": "SigLIP"
        },
        {
            "model_name": "qwen2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen2.md",
            "release_date": "2024-07-15",
            "transformers_date": "2024-01-17",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Qwen2](https://huggingface.co/papers/2407.10671) is a family of large language models (pretrained, instruction-tuned and mixture-of-experts) available in sizes from 0.5B to 72B parameters. The models are built on the Transformer architecture featuring enhancements like group query attention (GQA), rotary positional embeddings (RoPE), a mix of sliding window and full attention, and dual chunk attention with YARN for training stability. Qwen2 models support multiple languages and context lengths up to 131,072 tokens.\n\nYou can find all the official Qwen2 checkpoints under the [Qwen2](https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f) collection.\n\n> [!TIP]\n> Click on the Qwen2 models in the right sidebar for more examples of how to apply Qwen2 to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line using the instruction-tuned models.\n\n\n\n\n```python\nimport torch\nfrom transformers...",
            "tasks": [],
            "display_name": "Qwen2"
        },
        {
            "model_name": "wav2vec2-bert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\wav2vec2-bert.md",
            "release_date": "2023-11-30",
            "transformers_date": "2024-01-18",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The [Wav2Vec2-BERT](https://huggingface.co/papers/2312.05187) model was proposed in [Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/) by the Seamless Communication team from Meta AI.\n\nThis model was pre-trained on 4.5M hours of unlabeled audio data covering more than 143 languages. It requires finetuning to be used for downstream tasks such as Automatic Speech Recognition (ASR), or Audio Classification.\n\nThe official results of the model can be found in Section 3.2.1 of the paper.\n\nThe abstract from the paper is the following:\n\n*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when...",
            "tasks": [],
            "display_name": "Wav2Vec2-BERT"
        },
        {
            "model_name": "depth_anything",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\depth_anything.md",
            "release_date": "2024-01-19",
            "transformers_date": "2024-01-25",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[Depth Anything](https://huggingface.co/papers/2401.10891) is designed to be a foundation model for monocular depth estimation (MDE). It is jointly trained on labeled and ~62M unlabeled images to enhance the dataset. It uses a pretrained [DINOv2](./dinov2) model as an image encoder to inherit its existing rich semantic priors, and [DPT](./dpt) as the decoder. A teacher model is trained on unlabeled images to create pseudo-labels. The student model is trained on a combination of the pseudo-labels and labeled images. To improve the student model's performance, strong perturbations are added to the unlabeled images to challenge the student model to learn more visual knowledge from the image.\n\nYou can find all the original Depth Anything checkpoints under the [Depth Anything](https://huggingface.co/collections/LiheYoung/depth-anything-release-65b317de04eec72abf6b55aa) collection.\n\n> [!TIP]\n> Click on the Depth Anything models in the right sidebar for more examples of how to apply Depth...",
            "tasks": [],
            "display_name": "Depth Anything"
        },
        {
            "model_name": "stablelm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\stablelm.md",
            "release_date": "2023-09-05",
            "transformers_date": "2024-02-14",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "StableLM 3B 4E1T ([blog post](https://stability.ai/news/stable-lm-3b-sustainable-high-performance-language-models-smart-devices)) was proposed in [StableLM 3B 4E1T: Technical Report](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) by Stability AI and is the first model in a series of multi-epoch pre-trained language models.\n\n### Model Details\n\nStableLM 3B 4E1T is a decoder-only base language model pre-trained on 1 trillion tokens of diverse English and code datasets for four epochs.\nThe model architecture is transformer-based with partial Rotary Position Embeddings, SwiGLU activation, LayerNorm, etc.\n\nWe also provide StableLM Zephyr 3B, an instruction fine-tuned version of the model that can be used for chat-based applications.\n\n### Usage Tips\n\n- The architecture is similar to LLaMA but with RoPE applied to 25% of head embedding dimensions, LayerNorm instead of...",
            "tasks": [],
            "display_name": "StableLM"
        },
        {
            "model_name": "gemma",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gemma.md",
            "release_date": "2024-03-13",
            "transformers_date": "2024-02-21",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Gemma](https://huggingface.co/papers/2403.08295) is a family of lightweight language models with pretrained and instruction-tuned variants, available in 2B and 7B parameters. The architecture is based on a transformer decoder-only design. It features Multi-Query Attention, rotary positional embeddings (RoPE), GeGLU activation functions, and RMSNorm layer normalization.\n\nThe instruction-tuned variant was fine-tuned with supervised learning on instruction-following data, followed by reinforcement learning from human feedback (RLHF) to align the model outputs with human preferences.\n\nYou can find all the original Gemma checkpoints under the [Gemma](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b) release.\n\n> [!TIP]\n> Click on the Gemma models in the right sidebar for more examples of how to apply Gemma to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`] class, and from the command...",
            "tasks": [],
            "display_name": "Gemma"
        },
        {
            "model_name": "seggpt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\seggpt.md",
            "release_date": "2023-04-06",
            "transformers_date": "2024-02-26",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The SegGPT model was proposed in [SegGPT: Segmenting Everything In Context](https://huggingface.co/papers/2304.03284) by Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. SegGPT employs a decoder-only Transformer that can generate a segmentation mask given an input image, a prompt image and its corresponding prompt mask. The model achieves remarkable one-shot results with 56.1 mIoU on COCO-20 and 85.6 mIoU on FSS-1000.\n\nThe abstract from the paper is the following:\n\n*We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors....",
            "tasks": [],
            "display_name": "SegGPT"
        },
        {
            "model_name": "starcoder2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\starcoder2.md",
            "release_date": "2024-02-29",
            "transformers_date": "2024-02-28",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "StarCoder2 is a family of open LLMs for code and comes in 3 different sizes with 3B, 7B and 15B parameters. The flagship StarCoder2-15B model is trained on over 4 trillion tokens and 600+ programming languages from The Stack v2. All models use Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and were trained using the Fill-in-the-Middle objective. The models have been released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://huggingface.co/papers/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau\u00df, Naman Jain, Yixuan Su, Xuanli...",
            "tasks": [],
            "display_name": "Starcoder2"
        },
        {
            "model_name": "udop",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\udop.md",
            "release_date": "2022-12-05",
            "transformers_date": "2024-03-04",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The UDOP model was proposed in [Unifying Vision, Text, and Layout for Universal Document Processing](https://huggingface.co/papers/2212.02623) by Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal.\nUDOP adopts an encoder-decoder Transformer architecture based on [T5](t5) for document AI tasks like document image classification, document parsing and document visual question answering.\n\nThe abstract from the paper is the following:\n\nWe propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based...",
            "tasks": [],
            "display_name": "UDOP"
        },
        {
            "model_name": "mamba",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mamba.md",
            "release_date": "2023-12-01",
            "transformers_date": "2024-03-05",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Mamba](https://huggingface.co/papers/2312.00752) is a selective structured state space model (SSMs) designed to work around Transformers computational inefficiency when dealing with long sequences.  It is a completely attention-free architecture, and comprised of a combination of H3 and gated MLP blocks (Mamba block). Mamba's \"content-based reasoning\" allows it to focus on specific parts of an input depending on the current token. Mamba also uses a new hardware-aware parallel algorithm to compensate for the lack of convolutional operations. As a result, Mamba has fast inference and can scale to very long sequences.\n\nYou can find all the original Mamba checkpoints under the [State Space Models](https://huggingface.co/state-spaces) organization.\n\n> [!TIP]\n> This model was contributed by [Molbap](https://huggingface.co/Molbap) and [AntonV](https://huggingface.co/AntonV).\n> Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to different language...",
            "tasks": [],
            "display_name": "Mamba"
        },
        {
            "model_name": "pvt_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pvt_v2.md",
            "release_date": "2021-06-25",
            "transformers_date": "2024-03-13",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://huggingface.co/papers/2106.13797) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. As an improved variant of PVT, it eschews position embeddings, relying instead on positional information encoded through zero-padding and overlapping patch embeddings. This lack of reliance on position embeddings simplifies the architecture, and enables running inference at any resolution without needing to interpolate them.\n\nThe PVTv2 encoder structure has been successfully deployed to achieve state-of-the-art scores in [Segformer](https://huggingface.co/papers/2105.15203) for semantic segmentation, [GLPN](https://huggingface.co/papers/2201.07436) for monocular depth, and [Panoptic Segformer](https://huggingface.co/papers/2109.03814) for panoptic segmentation.\n\nPVTv2 belongs to a family of models called [hierarchical...",
            "tasks": [],
            "display_name": "Pyramid Vision Transformer V2 (PVTv2)"
        },
        {
            "model_name": "cohere",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\cohere.md",
            "release_date": "2024-03-12",
            "transformers_date": "2024-03-15",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "Cohere [Command-R](https://cohere.com/blog/command-r) is a 35B parameter multilingual large language model designed for long context tasks like retrieval-augmented generation (RAG) and calling external APIs and tools. The model is specifically trained for grounded generation and supports both single-step and multi-step tool use. It supports a context length of 128K tokens.\n\nYou can find all the original Command-R checkpoints under the [Command Models](https://huggingface.co/collections/CohereForAI/command-models-67652b401665205e17b192ad) collection.\n\n> [!TIP]\n> Click on the Cohere models in the right sidebar for more examples of how to apply Cohere to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"text-generation\",\n    model=\"CohereForAI/c4ai-command-r-v01\",\n    dtype=torch.float16,\n   ...",
            "tasks": [],
            "display_name": "Cohere"
        },
        {
            "model_name": "musicgen_melody",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\musicgen_melody.md",
            "release_date": "2023-06-08",
            "transformers_date": "2024-03-18",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The MusicGen Melody model was proposed in [Simple and Controllable Music Generation](https://huggingface.co/papers/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre D\u00e9fossez.\n\nMusicGen Melody is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned on text descriptions or audio prompts. The text descriptions are passed through a frozen text encoder model to obtain a sequence of hidden-state representations. MusicGen is then trained to predict discrete audio tokens, or *audio codes*, conditioned on these hidden-states. These audio tokens are then decoded using an audio compression model, such as EnCodec, to recover the audio waveform.\n\nThrough an efficient token interleaving pattern, MusicGen does not require a self-supervised semantic representation of the text/audio prompts, thus eliminating the need to cascade multiple models to predict a set of codebooks (e.g....",
            "tasks": [],
            "display_name": "MusicGen Melody"
        },
        {
            "model_name": "superpoint",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\superpoint.md",
            "release_date": "2017-12-20",
            "transformers_date": "2024-03-19",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[SuperPoint](https://huggingface.co/papers/1712.07629) is the result of self-supervised training of a fully-convolutional network for interest point detection and description. The model is able to detect interest points that are repeatable under homographic transformations and provide a descriptor for each point. Usage on it's own is limited, but it can be used as a feature extractor for other tasks such as homography estimation and image matching.\n\n\n\nYou can find all the original SuperPoint checkpoints under the [Magic Leap Community](https://huggingface.co/magic-leap-community) organization.\n\n> [!TIP]\n> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n>\n> Click on the SuperPoint models in the right sidebar for more examples of how to apply SuperPoint to different computer vision tasks.\n\nThe example below demonstrates how to detect interest points in an image with the [`AutoModel`] class.\n\n\n\n```py\nfrom transformers import AutoImageProcessor,...",
            "tasks": [],
            "display_name": "SuperPoint"
        },
        {
            "model_name": "llava_next",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llava_next.md",
            "release_date": "2024-01-30",
            "transformers_date": "2024-03-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[LLaVA\u2011NeXT](https://llava-vl.github.io/blog/2024-01-30-llava-next/) improves on [Llava](./llava) by increasing the input image resolution by 4x more pixels and supporting 3 aspect ratios (up to 672x672, 336x1344, 1344x336) to better grasp visual details. It is also trained on an improved visual instruction tuning dataset covering more scenarios and applications to improve OCR and common sense reasoning.\n\nYou can find all the original LLaVA\u2011NeXT checkpoints under the [LLaVA-NeXT](https://huggingface.co/collections/llava-hf/llava-next-65f75c4afac77fd37dbbe6cf) collection.\n\n> [!TIP]\n> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n>\n> Click on the LLaVA\u2011NeXT models in the right sidebar for more examples of how to apply Llava-NeXT to different multimodal tasks.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n\n```python\nimport torch  \nfrom transformers import pipeline  \n\npipeline = pipeline( ...",
            "tasks": [],
            "display_name": "LLaVA-NeXT"
        },
        {
            "model_name": "qwen2_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen2_moe.md",
            "release_date": "2024-07-15",
            "transformers_date": "2024-03-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Qwen2MoE](https://huggingface.co/papers/2407.10671) is a Mixture-of-Experts (MoE) variant of [Qwen2](./qwen2), available as a base model and an aligned chat model. It uses SwiGLU activation, group query attention and a mixture of sliding window attention and full attention. The tokenizer can also be adapted to multiple languages and codes.\n\nThe MoE architecture uses upcyled models from the dense language models. For example, Qwen1.5-MoE-A2.7B is upcycled from Qwen-1.8B. It has 14.3B parameters but only 2.7B parameters are activated during runtime.\n\nYou can find all the original checkpoints in the [Qwen1.5](https://huggingface.co/collections/Qwen/qwen15-65c0a2f577b1ecb76d786524) collection.\n\n> [!TIP]\n> Click on the Qwen2MoE models in the right sidebar for more examples of how to apply Qwen2MoE to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import...",
            "tasks": [],
            "display_name": "Qwen2MoE"
        },
        {
            "model_name": "recurrent_gemma",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\recurrent_gemma.md",
            "release_date": "2024-04-11",
            "transformers_date": "2024-04-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Recurrent Gemma model was proposed in [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://huggingface.co/papers/2404.07839) by the Griffin, RLHF and Gemma Teams of Google.\n\nThe abstract from the paper is the following:\n\n*We introduce RecurrentGemma, an open language model which uses Google\u2019s novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.*\n\nTips:\n\n- The original checkpoints can be converted using the conversion script...",
            "tasks": [],
            "display_name": "RecurrentGemma"
        },
        {
            "model_name": "grounding-dino",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\grounding-dino.md",
            "release_date": "2023-03-09",
            "transformers_date": "2024-04-11",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Grounding DINO model was proposed in [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://huggingface.co/papers/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection. The model achieves remarkable results, such as 52.5 AP on COCO zero-shot.\n\nThe abstract from the paper is the following:\n\n*In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide...",
            "tasks": [],
            "display_name": "Grounding DINO"
        },
        {
            "model_name": "idefics2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\idefics2.md",
            "release_date": "2024-05-03",
            "transformers_date": "2024-04-15",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Idefics2 model was proposed in [What matters when building vision-language models?](https://huggingface.co/papers/2405.02246) by L\u00e9o Tronchon, Hugo Laurencon, Victor Sanh. The accompanying blog post can be found [here](https://huggingface.co/blog/idefics2).\n\nIdefics2 is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text\noutputs. The model can answer questions about images, describe visual content, create stories grounded on multiple\nimages, or simply behave as a pure language model without visual inputs. It improves upon IDEFICS-1, notably on\ndocument understanding, OCR, or visual reasoning. Idefics2 is lightweight (8 billion parameters) and treats\nimages in their native aspect ratio and resolution, which allows for varying inference efficiency.\n\nThe abstract from the paper is the following:\n\n*The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers....",
            "tasks": [],
            "display_name": "Idefics2"
        },
        {
            "model_name": "olmo",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\olmo.md",
            "release_date": "2024-02-01",
            "transformers_date": "2024-04-17",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[OLMo](https://huggingface.co/papers/2402.00838) is a 7B-parameter dense language model. It uses SwiGLU activations, non-parametric layer normalization, rotary positional embeddings, and a BPE tokenizer that masks personally identifiable information. It is pretrained on [Dolma](https://huggingface.co/datasets/allenai/dolma), a 3T-token dataset. OLMo was released to provide complete transparency of not just the model weights but the training data, training code, and evaluation code to enable more research on language models.\n\nYou can find all the original OLMo checkpoints under the [OLMo](https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778) collection.\n\n> [!TIP]\n> This model was contributed by [shanearora](https://huggingface.co/shanearora).\n>\n> Click on the OLMo models in the right sidebar for more examples of how to apply OLMo to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`]...",
            "tasks": [],
            "display_name": "OLMo"
        },
        {
            "model_name": "dbrx",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dbrx.md",
            "release_date": "2024-03-27",
            "transformers_date": "2024-04-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "DBRX is a [transformer-based](https://www.isattentionallyouneed.com/) decoder-only large language model (LLM) that was trained using next-token prediction.\nIt uses a *fine-grained* mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input.\nIt was pre-trained on 12T tokens of text and code data.\nCompared to other open MoE models like Mixtral-8x7B and Grok-1, DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, while Mixtral-8x7B and Grok-1 have 8 experts and choose 2.\nThis provides 65x more possible combinations of experts and we found that this improves model quality.\nDBRX uses rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA).\nIt is a BPE based model and uses the GPT-4 tokenizer as described in the [tiktoken](https://github.com/openai/tiktoken) repository.\nWe made these choices based on exhaustive evaluation and scaling...",
            "tasks": [],
            "display_name": "DBRX"
        },
        {
            "model_name": "jamba",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\jamba.md",
            "release_date": "2024-03-28",
            "transformers_date": "2024-04-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Jamba](https://huggingface.co/papers/2403.19887) is a hybrid Transformer-Mamba mixture-of-experts (MoE) language model ranging from 52B to 398B total parameters. This model aims to combine the advantages of both model families, the performance of transformer models and the efficiency and longer context (256K tokens) of state space models (SSMs) like Mamba.\n\nJamba's architecture features a blocks-and-layers approach that allows Jamba to successfully integrate Transformer and Mamba architectures altogether. Each Jamba block contains either an attention or a Mamba layer, followed by a multi-layer perceptron (MLP), producing an overall ratio of one Transformer layer out of every eight total layers. MoE layers are mixed in to increase model capacity.\n\nYou can find all the original Jamba checkpoints under the [AI21](https://huggingface.co/ai21labs) organization.\n\n> [!TIP]\n> Click on the Jamba models in the right sidebar for more examples of how to apply Jamba to different language...",
            "tasks": [],
            "display_name": "Jamba"
        },
        {
            "model_name": "llama3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llama3.md",
            "release_date": "2024-04-18",
            "transformers_date": "2024-04-24",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"dtype\": torch.bfloat16}, device_map=\"auto\")\npipeline(\"Hey how are you doing today?\")\n```\n\n## Overview\n\nThe [Llama3](https://huggingface.co/papers/2407.21783) model was proposed in [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) by the meta AI team.\n\nThe abstract from the blogpost is the following:\n\n*Today, we\u2019re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their class, period. In support of our longstanding open...",
            "tasks": [],
            "display_name": "Llama3"
        },
        {
            "model_name": "phi3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\phi3.md",
            "release_date": "2024-04-22",
            "transformers_date": "2024-04-24",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Phi-3 model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://huggingface.co/papers/2404.14219) by Microsoft.\n\n### Summary\n\nThe abstract from the Phi-3 paper is the following:\n\nWe introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more...",
            "tasks": [],
            "display_name": "Phi-3"
        },
        {
            "model_name": "jetmoe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\jetmoe.md",
            "release_date": "2023-06-07",
            "transformers_date": "2024-05-14",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "**JetMoe-8B** is an 8B Mixture-of-Experts (MoE) language model developed by [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ) and [MyShell](https://myshell.ai/).\nJetMoe project aims to provide a LLaMA2-level performance and efficient language model with a limited budget.\nTo achieve this goal, JetMoe uses a sparsely activated architecture inspired by the [ModuleFormer](https://huggingface.co/papers/2306.04640).\nEach JetMoe block consists of two MoE layers: Mixture of Attention Heads and Mixture of MLP Experts.\nGiven the input tokens, it activates a subset of its experts to process them.\nThis sparse activation schema enables JetMoe to achieve much better training throughput than similar size dense models.\nThe training throughput of JetMoe-8B is around 100B tokens per day on a cluster of 96 H100 GPUs with a straightforward 3-way pipeline parallelism strategy.\n\nThis model was contributed by [Yikang Shen](https://huggingface.co/YikangS).\n\n## JetMoeConfig\n\n[[autodoc]]...",
            "tasks": [],
            "display_name": "JetMoe"
        },
        {
            "model_name": "paligemma",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\paligemma.md",
            "release_date": "2024-07-10",
            "transformers_date": "2024-05-14",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[PaliGemma](https://huggingface.co/papers/2407.07726) is a family of vision-language models (VLMs), combining [SigLIP](./siglip) with the [Gemma](./gemma) 2B model. PaliGemma is available in 3B, 10B, and 28B parameters. The main purpose of PaliGemma is to provide an adaptable base VLM that is easy to transfer to other tasks. The SigLIP vision encoder is a \"shape optimized\" contrastively pretrained [ViT](./vit) that converts an image into a sequence of tokens and prepended to an optional prompt. The Gemma 2B model is used as the decoder. PaliGemma uses full attention on all image and text tokens to maximize its capacity.\n\n[PaliGemma 2](https://huggingface.co/papers/2412.03555) improves on the first model by using Gemma 2 (2B, 9B, and 27B parameter variants) as the decoder. These are available as **pt** or **mix** variants. The **pt** checkpoints are intended for further fine-tuning and the **mix** checkpoints are ready for use out of the box.\n\nYou can find all the original PaliGemma...",
            "tasks": [],
            "display_name": "PaliGemma"
        },
        {
            "model_name": "video_llava",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\video_llava.md",
            "release_date": "2023-11-16",
            "transformers_date": "2024-05-15",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "Video-LLaVa is an open-source multimodal LLM trained by fine-tuning LlamA/Vicuna on multimodal instruction-following data generated by Llava1.5 and VideChat. It is an auto-regressive language model, based on the transformer architecture. Video-LLaVa unifies visual representations to the language feature space, and enables an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n\nThe Video-LLaVA model was proposed in [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://huggingface.co/papers/2311.10122) by Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munang Ning, Peng Jin, Li Yuan.\n\nThe abstract from the paper is the following:\n\n*The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in\nvisual-language understanding. Most existing approaches\nencode images and videos into separate feature spaces,\nwhich are then fed as inputs to large language models.\nHowever, due to the lack of...",
            "tasks": [],
            "display_name": "Video-LLaVA"
        },
        {
            "model_name": "rt_detr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\rt_detr.md",
            "release_date": "2023-04-17",
            "transformers_date": "2024-06-22",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The RT-DETR model was proposed in [DETRs Beat YOLOs on Real-time Object Detection](https://huggingface.co/papers/2304.08069) by Wenyu Lv, Yian Zhao, Shangliang Xu, Jinman Wei, Guanzhong Wang, Cheng Cui, Yuning Du, Qingqing Dang, Yi Liu.\n\nRT-DETR is an object detection model that stands for \"Real-Time DEtection Transformer.\" This model is designed to perform object detection tasks with a focus on achieving real-time performance while maintaining high accuracy. Leveraging the transformer architecture, which has gained significant popularity in various fields of deep learning, RT-DETR processes images to identify and locate multiple objects within them.\n\nThe abstract from the paper is the following:\n\n*Recently, end-to-end transformer-based detectors (DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no...",
            "tasks": [],
            "display_name": "RT-DETR"
        },
        {
            "model_name": "instructblipvideo",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\instructblipvideo.md",
            "release_date": "2023-05-11",
            "transformers_date": "2024-06-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The InstructBLIPVideo is an extension of the models proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://huggingface.co/papers/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\nInstructBLIPVideo uses the same architecture as [InstructBLIP](instructblip) and works with the same checkpoints as [InstructBLIP](instructblip). The only difference is the ability to process videos.\n\nThe abstract from the paper is the following:\n\n*General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less...",
            "tasks": [],
            "display_name": "InstructBlipVideo"
        },
        {
            "model_name": "llava_next_video",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llava_next_video.md",
            "release_date": "2024-05-31",
            "transformers_date": "2024-06-26",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video Understanding Model\n](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) by Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, Chunyuan Li. LLaVa-NeXT-Video improves upon [LLaVa-NeXT](llava_next) by fine-tuning on a mix if video and image dataset thus increasing the model's performance on videos.\n\n[LLaVA-NeXT](llava_next) surprisingly has strong performance in understanding video content in zero-shot fashion with the AnyRes technique that it uses. The AnyRes technique naturally represents a high-resolution image into multiple images. This technique is naturally generalizable to represent videos because videos can be considered as a set of frames (similar to a set of images in LLaVa-NeXT). The current version of LLaVA-NeXT makes use of AnyRes and trains with supervised fine-tuning (SFT) on top of LLaVA-Next on video data to achieves better video...",
            "tasks": [],
            "display_name": "LLaVa-NeXT-Video"
        },
        {
            "model_name": "gemma2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gemma2.md",
            "release_date": "2024-07-31",
            "transformers_date": "2024-06-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Gemma 2](https://huggingface.co/papers/2408.00118) is a family of language models with pretrained and instruction-tuned variants, available in 2B, 9B, 27B parameters. The architecture is similar to the previous Gemma, except it features interleaved local attention (4096 tokens) and global attention (8192 tokens) and grouped-query attention (GQA) to increase inference performance.\n\nThe 2B and 9B models are trained with knowledge distillation, and the instruction-tuned variant was post-trained with supervised fine-tuning and reinforcement learning.\n\nYou can find all the original Gemma 2 checkpoints under the [Gemma 2](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315) collection.\n\n> [!TIP]\n> Click on the Gemma 2 models in the right sidebar for more examples of how to apply Gemma to different language tasks.\n\nThe example below demonstrates how to chat with the model with [`Pipeline`] or the [`AutoModel`] class, and from the command...",
            "tasks": [],
            "display_name": "Gemma2"
        },
        {
            "model_name": "depth_anything_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\depth_anything_v2.md",
            "release_date": "2024-06-13",
            "transformers_date": "2024-07-05",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "Depth Anything V2 was introduced in [the paper of the same name](https://huggingface.co/papers/2406.09414) by Lihe Yang et al. It uses the same architecture as the original [Depth Anything model](depth_anything), but uses synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions.\n\nThe abstract from the paper is the following:\n\n*This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x...",
            "tasks": [],
            "display_name": "Depth Anything V2"
        },
        {
            "model_name": "zoedepth",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\zoedepth.md",
            "release_date": "2023-02-23",
            "transformers_date": "2024-07-08",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[ZoeDepth](https://huggingface.co/papers/2302.12288) is a depth estimation model that combines the generalization performance of relative depth estimation (how far objects are from each other) and metric depth estimation (precise depth measurement on metric scale) from a single image. It is pre-trained on 12 datasets using relative depth and 2 datasets (NYU Depth v2 and KITTI) for metric accuracy. A lightweight head with a metric bin module for each domain is used, and during inference, it automatically selects the appropriate head for each input image with a latent classifier.\n\n\n\nYou can find all the original ZoeDepth checkpoints under the [Intel](https://huggingface.co/Intel?search=zoedepth) organization.\n\nThe example below demonstrates how to estimate depth with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport requests\nimport torch\nfrom transformers import pipeline\nfrom PIL import Image\n\nurl =...",
            "tasks": [],
            "display_name": "ZoeDepth"
        },
        {
            "model_name": "hiera",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\hiera.md",
            "release_date": "2023-06-01",
            "transformers_date": "2024-07-12",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "Hiera was proposed in [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://huggingface.co/papers/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer\n\nThe paper introduces \"Hiera,\" a hierarchical Vision Transformer that simplifies the architecture of modern hierarchical vision transformers by removing unnecessary components without compromising on accuracy or efficiency. Unlike traditional transformers that add complex vision-specific components to improve supervised classification performance, Hiera demonstrates that such additions, often termed \"bells-and-whistles,\" are not essential for high accuracy. By leveraging a strong visual pretext task (MAE) for pretraining, Hiera retains simplicity and achieves superior accuracy and speed both in inference and training across various image and video...",
            "tasks": [],
            "display_name": "Hiera"
        },
        {
            "model_name": "chameleon",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\chameleon.md",
            "release_date": "2024-05-16",
            "transformers_date": "2024-07-17",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models\n](https://huggingface.co/papers/2405.09818) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n\nThe abstract from the paper is the following:\n\n*We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training\napproach from inception, an alignment recipe, and an architectural parameterization tailored for the\nearly-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range\nof tasks, including visual question answering, image captioning, text generation, image generation,...",
            "tasks": [],
            "display_name": "Chameleon"
        },
        {
            "model_name": "mamba2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mamba2.md",
            "release_date": "2024-05-31",
            "transformers_date": "2024-08-06",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Mamba 2](https://huggingface.co/papers/2405.21060) is based on the state space duality (SSD) framework which connects structured state space models (SSMs) and attention variants. It uses a more efficient SSD algorithm that is 2-8x faster than Mamba and modifies the architecture to enable tensor parallelism and a grouped-value attention (GVA) head structure.\n\nYou can find all the original Mamba 2 checkpoints under the [State Space Models](https://huggingface.co/state-spaces) organization, but the examples shown below use [mistralai/Mamba-Codestral-7B-v0.1](https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1) because a Hugging Face implementation isn't supported yet for the original checkpoints.\n\nOther Mamba 2-based architectures include [Bamba](./bamba), [FalconH1](./falcon_h1), and [Zamba2](./zamba2).\n\n> [!TIP]\n> This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\n> Click on the Mamba models in the right sidebar for more examples of how to apply Mamba to...",
            "tasks": [],
            "display_name": "Mamba 2"
        },
        {
            "model_name": "nemotron",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\nemotron.md",
            "release_date": "2024-02-26",
            "transformers_date": "2024-08-06",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The use of this model is governed by the [NVIDIA AI Foundation Models Community License Agreement](https://developer.nvidia.com/downloads/nv-ai-foundation-models-license).\n\n### Description\n\nNemotron-4 is a family of enterprise ready generative text models compatible with [NVIDIA NeMo Framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/).\n\nNVIDIA NeMo is an end-to-end, cloud-native platform to build, customize, and deploy generative AI models anywhere. It includes training and inferencing frameworks, guardrailing toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI. To get access to NeMo Framework, please sign up at [this link](https://developer.nvidia.com/nemo-framework/join).\n\n### References\n\n[Announcement Blog](https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/)\n\n### Model...",
            "tasks": [],
            "display_name": "Nemotron"
        },
        {
            "model_name": "qwen2_audio",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen2_audio.md",
            "release_date": "2024-07-15",
            "transformers_date": "2024-08-08",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Qwen2-Audio is the new model series of large audio-language models from the Qwen team. Qwen2-Audio is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. We introduce two distinct audio interaction modes:\n\n* voice chat: users can freely engage in voice interactions with Qwen2-Audio without text input\n* audio analysis: users could provide audio and text instructions for analysis during the interaction\n\nIt was proposed in [Qwen2-Audio Technical Report](https://huggingface.co/papers/2407.10759) by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou.\n\nThe abstract from the paper is the following:\n\n*We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual...",
            "tasks": [],
            "display_name": "Qwen2Audio"
        },
        {
            "model_name": "falcon_mamba",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\falcon_mamba.md",
            "release_date": "2024-10-07",
            "transformers_date": "2024-08-12",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[FalconMamba](https://huggingface.co/papers/2410.05355) is a 7B large language model, available as pretrained and instruction-tuned variants, based on the [Mamba](./mamba). This model implements a pure Mamba design that focuses on computational efficiency while maintaining strong performance. FalconMamba is significantly faster at inference and requires substantially less memory for long sequence generation. The models are pretrained on a diverse 5.8T token dataset including [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), technical content, code, and mathematical data.\n\nYou can find the official FalconMamba checkpoints in the [FalconMamba 7B](https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a) collection.\n\n> [!TIP]\n> Click on the FalconMamba models in the right sidebar for more examples of how to apply FalconMamba to different language tasks.\n\nThe examples below demonstrate how to generate text with [`Pipeline`], [`AutoModel`],...",
            "tasks": [],
            "display_name": "FalconMamba"
        },
        {
            "model_name": "dac",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dac.md",
            "release_date": "2023-06-11",
            "transformers_date": "2024-08-19",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The DAC model was proposed in [Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN](https://huggingface.co/papers/2306.06546) by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar.\n\nThe Descript Audio Codec (DAC) model is a powerful tool for compressing audio data, making it highly efficient for storage and transmission. By compressing 44.1 KHz audio into tokens at just 8kbps bandwidth, the DAC model enables high-quality audio processing while significantly reducing the data footprint. This is particularly useful in scenarios where bandwidth is limited or storage space is at a premium, such as in streaming applications, remote conferencing, and archiving large audio datasets.\n\nThe abstract from the paper is the following:\n\n*Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional...",
            "tasks": [],
            "display_name": "DAC"
        },
        {
            "model_name": "qwen2_vl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen2_vl.md",
            "release_date": "2024-08-29",
            "transformers_date": "2024-08-26",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [Qwen2-VL](https://huggingface.co/papers/2409.12191) ([blog post](https://qwenlm.github.io/blog/qwen2-vl/)) model is a major update to [Qwen-VL](https://huggingface.co/papers/2308.12966) from the Qwen team at Alibaba Research.\n\nThe abstract from the blog is the following:\n\n*This blog introduces Qwen2-VL, an advanced version of the Qwen-VL model that has undergone significant enhancements over the past year. Key improvements include enhanced image comprehension, advanced video understanding, integrated visual agent functionality, and expanded multilingual support. The model architecture has been optimized for handling arbitrary image resolutions through Naive Dynamic Resolution support and utilizes Multimodal Rotary Position Embedding (M-ROPE) to effectively process both 1D textual and multi-dimensional visual data. This updated model demonstrates competitive performance against leading AI systems like GPT-4o and Claude 3.5 Sonnet in vision-related tasks and ranks highly among...",
            "tasks": [],
            "display_name": "Qwen2-VL"
        },
        {
            "model_name": "granite",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\granite.md",
            "release_date": "2024-08-23",
            "transformers_date": "2024-08-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Granite](https://huggingface.co/papers/2408.13359) is a 3B parameter language model trained with the Power scheduler. Discovering a good learning rate for pretraining large language models is difficult because it depends on so many variables (batch size, number of training tokens, etc.) and it is expensive to perform a hyperparameter search. The Power scheduler is based on a power-law relationship between the variables and their transferability to larger models. Combining the Power scheduler with Maximum Update Parameterization (MUP) allows a model to be pretrained with one set of hyperparameters regardless of all the variables.\n\nYou can find all the original Granite checkpoints under the [IBM-Granite](https://huggingface.co/ibm-granite) organization.\n\n> [!TIP]\n> Click on the Granite models in the right sidebar for more examples of how to apply Granite to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`, and from the...",
            "tasks": [],
            "display_name": "Granite"
        },
        {
            "model_name": "olmoe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\olmoe.md",
            "release_date": "2024-09-03",
            "transformers_date": "2024-09-03",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[OLMoE](https://huggingface.co/papers/2409.02060) is a sparse Mixture-of-Experts (MoE) language model with 7B parameters but only 1B parameters are used per input token. It has similar inference costs as dense models but trains ~3x faster. OLMoE uses fine-grained routing with 64 small experts in each layer and uses a dropless token-based routing algorithm.\n\nYou can find all the original OLMoE checkpoints under the [OLMoE](https://huggingface.co/collections/allenai/olmoe-november-2024-66cf678c047657a30c8cd3da) collection.\n\n> [!TIP]\n> This model was contributed by [Muennighoff](https://huggingface.co/Muennighoff).\n>\n> Click on the OLMoE models in the right sidebar for more examples of how to apply OLMoE to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"text-generation\",\n    model=\"allenai/OLMoE-1B-7B-0125\",\n   ...",
            "tasks": [],
            "display_name": "OLMoE"
        },
        {
            "model_name": "llava_onevision",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llava_onevision.md",
            "release_date": "2024-08-06",
            "transformers_date": "2024-09-05",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The LLaVA-OneVision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://huggingface.co/papers/2408.03326) by \n\n LLaVA-OneVision architecture. Taken from the original paper. \n\nTips:\n\n- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n\n\n\n- Llava-OneVision uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n\n\n\n### Formatting Prompts with Chat Templates  \n\nEach **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor\u2019s `apply_chat_template` method.  \n\n**Important:**  \n\n- You must construct a conversation history \u2014...",
            "tasks": [],
            "display_name": "LLaVA-OneVision"
        },
        {
            "model_name": "pixtral",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\pixtral.md",
            "release_date": "2024-09-17",
            "transformers_date": "2024-09-14",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Pixtral](https://huggingface.co/papers/2410.07073) is a multimodal model trained to understand natural images and documents. It accepts images in their natural resolution and aspect ratio without resizing or padding due to it's 2D RoPE embeddings. In addition, Pixtral has a long 128K token context window for processing a large number of images. Pixtral couples a 400M vision encoder with a 12B Mistral Nemo decoder.\n\n\n\n Pixtral architecture. Taken from the blog post. \n\nYou can find all the original Pixtral checkpoints under the [Mistral AI](https://huggingface.co/mistralai/models?search=pixtral) organization.\n\n> [!TIP]\n> This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ).\n> Click on the Pixtral models in the right sidebar for more examples of how to apply Pixtral to different vision and language tasks.\n\n\n\n\n\n```python\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id =...",
            "tasks": [],
            "display_name": "Pixtral"
        },
        {
            "model_name": "mimi",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mimi.md",
            "release_date": "2024-09-17",
            "transformers_date": "2024-09-18",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[Mimi](huggingface.co/papers/2410.00037) is a neural audio codec model with pretrained and quantized variants, designed for efficient speech representation and compression. The model operates at 1.1 kbps with a 12 Hz frame rate and uses a convolutional encoder-decoder architecture combined with a residual vector quantizer of 16 codebooks. Mimi outputs dual token streams i.e. semantic and acoustic to balance linguistic richness with high fidelity reconstruction. Key features include a causal streaming encoder for low-latency use, dual-path tokenization for flexible downstream generation, and integration readiness with large speech models like Moshi.\n\nYou can find the original Mimi checkpoints under the [Kyutai](https://huggingface.co/kyutai/models?search=mimi) organization.\n\n>[!TIP]\n> This model was contributed by [ylacombe](https://huggingface.co/ylacombe).\n>\n> Click on the Mimi models in the right sidebar for more examples of how to apply Mimi.\n\nThe example below demonstrates how to...",
            "tasks": [],
            "display_name": "Mimi"
        },
        {
            "model_name": "granitemoe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\granitemoe.md",
            "release_date": "2024-08-23",
            "transformers_date": "2024-09-20",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://huggingface.co/papers/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n\nPowerMoE-3B is a 3B sparse Mixture-of-Experts (sMoE) language model trained with the Power learning rate scheduler. It sparsely activates 800M parameters for each token. It is trained on a mix of open-source and proprietary datasets. PowerMoE-3B has shown promising results compared to other dense models with 2x activate parameters across various benchmarks, including natural language multi-choices, code generation, and math reasoning.\n\nThe abstract from the paper is the following:\n\n*Finding the optimal learning rate for language model pretraining is a challenging task.\nThis is not only because there is a complicated correlation between learning rate, batch size, number of training...",
            "tasks": [],
            "display_name": "GraniteMoe"
        },
        {
            "model_name": "idefics3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\idefics3.md",
            "release_date": "2024-08-22",
            "transformers_date": "2024-09-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Idefics3 model was proposed in [Building and better understanding vision-language models: insights and future directions](https://huggingface.co/papers/2408.12637) by Hugo Lauren\u00e7on, Andr\u00e9s Marafioti, Victor Sanh, and L\u00e9o Tronchon.\n\nIdefics3 is an adaptation of the Idefics2 model with three main differences:\n\n- It uses Llama3 for the text model.\n- It uses an updated processing logic for the images.\n- It removes the perceiver.\n\nThe abstract from the paper is the following:\n\n*The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting...",
            "tasks": [],
            "display_name": "Idefics3"
        },
        {
            "model_name": "mllama",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mllama.md",
            "release_date": "2024-09-25",
            "transformers_date": "2024-09-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [Llama 3.2-Vision](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text \\+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image.\n\n**Model Architecture:** Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series...",
            "tasks": [],
            "display_name": "Mllama"
        },
        {
            "model_name": "omdet-turbo",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\omdet-turbo.md",
            "release_date": "2024-03-11",
            "transformers_date": "2024-09-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The OmDet-Turbo model was proposed in [Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head](https://huggingface.co/papers/2403.06892) by Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee. OmDet-Turbo incorporates components from RT-DETR and introduces a swift multimodal fusion module to achieve real-time open-vocabulary object detection capabilities while maintaining high accuracy. The base model achieves performance of up to 100.2 FPS and 53.4 AP on COCO zero-shot.\n\nThe abstract from the paper is the following:\n\n*End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and...",
            "tasks": [],
            "display_name": "OmDet-Turbo"
        },
        {
            "model_name": "phimoe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\phimoe.md",
            "release_date": "2024-04-22",
            "transformers_date": "2024-10-04",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The PhiMoE model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://huggingface.co/papers/2404.14219) by Microsoft.\n\n### Summary\n\nThe abstract from the Phi-3 paper is the following:\n\nWe introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g.,...",
            "tasks": [],
            "display_name": "PhiMoE"
        },
        {
            "model_name": "zamba",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\zamba.md",
            "release_date": "2024-04-16",
            "transformers_date": "2024-10-04",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Zamba](https://huggingface.co/papers/2405.16712) ([blog post](https://www.zyphra.com/post/zamba)) is a large language model (LLM) trained by Zyphra, and made available under an Apache 2.0 license. Please see the [Zyphra Hugging Face](https://huggingface.co/collections/zyphra/) repository for model weights.\n\nThis model was contributed by [pglo](https://huggingface.co/pglo).\n\n## Model details\n\nZamba-7B-v1 is a hybrid between state-space models (Specifically [Mamba](https://github.com/state-spaces/mamba)) and transformer, and was trained using next-token prediction. Zamba uses a shared transformer layer after every 6 mamba blocks. It uses the [Mistral v0.1 tokenizer](https://huggingface.co/mistralai/Mistral-7B-v0.1). We came to this architecture after a series of ablations at small scales. Zamba-7B-v1 was pre-trained on 1T tokens of text and code data.\n\n\n\n## Quick start\n\n### Presequities\n\nZamba requires you use `transformers` version 4.46.0 or higher:\n\n```bash\npip install...",
            "tasks": [],
            "display_name": "Zamba"
        },
        {
            "model_name": "myt5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\myt5.md",
            "release_date": "2024-03-15",
            "transformers_date": "2024-10-06",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The myt5 model was proposed in [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://huggingface.co/papers/2403.10691) by Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer.\nMyT5 (**My**te **T5**) is a multilingual language model based on T5 architecture.\nThe model uses a **m**orphologically-driven **byte** (**MYTE**) representation described in our paper.\n**MYTE** uses codepoints corresponding to morphemes in contrast to characters used in UTF-8 encoding.\nAs a pre-requisite, we used unsupervised morphological segmentation ([Morfessor](https://aclanthology.org/E14-2006.pdf)) to obtain morpheme inventories for 99 languages.\nHowever, the morphological segmentation step is not needed when using the pre-defined morpheme inventory from the hub (see: [Tomli/myt5-base](https://huggingface.co/Tomlim/myt5-base)).\n\nThe abstract from the paper is the following:\n\n*A major consideration in multilingual language...",
            "tasks": [],
            "display_name": "myt5"
        },
        {
            "model_name": "moshi",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\moshi.md",
            "release_date": "2024-09-17",
            "transformers_date": "2024-10-16",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Moshi model was proposed in [Moshi: a speech-text foundation model for real-time dialogue](https://huggingface.co/papers/2410.00037) by Alexandre D\u00e9fossez, Laurent Mazar\u00e9, Manu Orsini, Am\u00e9lie Royer, Patrick P\u00e9rez, Herv\u00e9 J\u00e9gou, Edouard Grave and Neil Zeghidour.\n\nMoshi is a speech-text foundation model that casts spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. Moshi also predicts time-aligned text tokens as a prefix to audio tokens. This \u201cInner Monologue\u201d method significantly improves the linguistic quality of generated speech and provides streaming speech recognition and text-to-speech. As a result, Moshi is the first real-time full-duplex spoken large language...",
            "tasks": [],
            "display_name": "Moshi"
        },
        {
            "model_name": "glm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\glm.md",
            "release_date": "2024-06-18",
            "transformers_date": "2024-10-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "in [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://huggingface.co/papers/2406.12793)\nby GLM Team, THUDM & ZhipuAI.\n\nThe abstract from the paper is the following:\n\n*We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report\nprimarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most\ncapable models that are trained with all the insights and lessons gained from the preceding three generations of\nChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with\na small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment\nis achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human\nfeedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of...",
            "tasks": [],
            "display_name": "GLM"
        },
        {
            "model_name": "olmo2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\olmo2.md",
            "release_date": "2024-12-31",
            "transformers_date": "2024-11-25",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[OLMo2](https://huggingface.co/papers/2501.00656) improves on [OLMo](./olmo) by changing the architecture and training recipes of the original models. This includes excluding all biases to improve training stability, non-parametric layer norm, SwiGLU activation function, rotary positional embeddings, and a modified BPE-based tokenizer that masks personal identifiable information. It is pretrained on [Dolma](https://huggingface.co/datasets/allenai/dolma), a dataset of 3T tokens.\n\nYou can find all the original OLMo2 checkpoints under the [OLMo2](https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc) collection.\n\n> [!TIP]\n> Click on the OLMo2 models in the right sidebar for more examples of how to apply OLMo2 to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"text-generation\",\n   ...",
            "tasks": [],
            "display_name": "OLMo2"
        },
        {
            "model_name": "ijepa",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ijepa.md",
            "release_date": "2023-01-19",
            "transformers_date": "2024-12-05",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[I-JEPA](https://huggingface.co/papers/2301.08243) is a self-supervised learning method that learns semantic image representations by predicting parts of an image from other parts of the image. It compares the abstract representations of the image (rather than pixel level comparisons), which avoids the typical pitfalls of data augmentation bias and pixel-level details that don't capture semantic meaning.\n\nYou can find the original I-JEPA checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=ijepa) organization.\n> [!TIP]\n> This model was contributed by [jmtzt](https://huggingface.co/jmtzt).\n\n\n\n> Click on the I-JEPA models in the right sidebar for more examples of how to apply I-JEPA to different image representation and classification tasks.\n\nThe example below demonstrates how to extract image features with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\nfeature_extractor = pipeline(\n   ...",
            "tasks": [],
            "display_name": "I-JEPA"
        },
        {
            "model_name": "aria",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\aria.md",
            "release_date": "2024-10-08",
            "transformers_date": "2024-12-06",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Aria](https://huggingface.co/papers/2410.05993) is a multimodal mixture-of-experts (MoE) model. The goal of this model is to open-source a training recipe for creating a multimodal native model from scratch. Aria has 3.9B and 3.5B activated parameters per visual and text token respectively. Text is handled by a MoE decoder and visual inputs are handled by a lightweight visual encoder. It is trained in 4 stages, language pretraining, multimodal pretraining, multimodal long-context pretraining, and multimodal post-training.\n\nYou can find all the original Aria checkpoints under the [Aria](https://huggingface.co/rhymes-ai?search_models=aria) organization.\n\n> [!TIP]\n> Click on the Aria models in the right sidebar for more examples of how to apply Aria to different multimodal tasks.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n   ...",
            "tasks": [],
            "display_name": "Aria"
        },
        {
            "model_name": "cohere2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\cohere2.md",
            "release_date": "2024-12-13",
            "transformers_date": "2024-12-13",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Cohere Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model. It is a multilingual model trained on 23 languages and has a context window of 128k. The model features three layers with sliding window attention and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n\nThis model is optimized for speed, cost-performance, and compute resources.\n\nYou can find all the original Command-R checkpoints under the [Command Models](https://huggingface.co/collections/CohereForAI/command-models-67652b401665205e17b192ad) collection.\n\n> [!TIP]\n> Click on the Cohere models in the right sidebar for more examples of how to apply Cohere to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`] class, and from the command...",
            "tasks": [],
            "display_name": "Cohere 2"
        },
        {
            "model_name": "colpali",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\colpali.md",
            "release_date": "2024-06-27",
            "transformers_date": "2024-12-17",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[ColPali](https://huggingface.co/papers/2407.01449) is a model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColPali treats each page as an image. It uses [Paligemma-3B](./paligemma) to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n\nThis model was contributed by [@tonywu71](https://huggingface.co/tonywu71) (ILLUIN Technology) and [@yonigozlan](https://huggingface.co/yonigozlan) (HuggingFace).\n\nYou can find all the original ColPali checkpoints under Vidore's [Hf-native ColVision Models](https://huggingface.co/collections/vidore/hf-native-colvision-models-6755d68fc60a8553acaa96f7) collection.\n\n> [!TIP]\n> Click on the...",
            "tasks": [],
            "display_name": "ColPali"
        },
        {
            "model_name": "falcon3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\falcon3.md",
            "release_date": "2024-12-17",
            "transformers_date": "2024-12-17",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Falcon3](https://falconllm.tii.ae/falcon3/index.html) represents a natural evolution from previous releases, emphasizing expanding the models' science, math, and code capabilities. This iteration includes five base models: Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base. In developing these models, we incorporated several key innovations aimed at improving the models' performances while reducing training costs:\n\nOne pre-training: We conducted a single large-scale pretraining run on the 7B model, using 2048 H100 GPU chips, leveraging 14 trillion tokens featuring web, code, STEM, and curated high-quality and multilingual data.\nDepth up-scaling for improved reasoning: Building on recent studies on the effects of model depth, we upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2TT of high-quality data. This yielded Falcon3-10B-Base which achieves state-of-the-art zero-shot...",
            "tasks": [],
            "display_name": "Falcon3"
        },
        {
            "model_name": "bamba",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bamba.md",
            "release_date": "2024-12-18",
            "transformers_date": "2024-12-19",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Bamba](https://huggingface.co/blog/bamba) is a 9B parameter decoder-only language model built on the [Mamba-2](./mamba2) architecture. It is pretrained in two stages - it starts by training on 2T tokens from the [Dolma v1.7](https://huggingface.co/datasets/allenai/dolma) dataset and then trained on an additional 200B tokens from [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) and [Cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia).\n\nYou can find all the original Bamba checkpoints under the [Bamba](https://huggingface.co/collections/ibm-ai-platform/bamba-674f1388b9bbc98b413c7bab) collection.\n\n> [!TIP]\n> This model was contributed by [ani300](https://github.com/ani300) and [fabianlim](https://github.com/fabianlim).\n>\n> Click on the Bamba models in the right sidebar for more examples of how to apply Bamba to different text generation tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command...",
            "tasks": [],
            "display_name": "Bamba"
        },
        {
            "model_name": "modernbert",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\modernbert.md",
            "release_date": "2024-12-18",
            "transformers_date": "2024-12-19",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[ModernBERT](https://huggingface.co/papers/2412.13663) is a modernized version of [`BERT`] trained on 2T tokens. It brings many improvements to the original architecture such as rotary positional embeddings to support sequences of up to 8192 tokens, unpadding to avoid wasting compute on padding tokens, GeGLU layers, and alternating attention.\n\nYou can find all the original ModernBERT checkpoints under the [ModernBERT](https://huggingface.co/collections/answerdotai/modernbert-67627ad707a4acbf33c41deb) collection.\n\n> [!TIP]\n> Click on the ModernBERT models in the right sidebar for more examples of how to apply ModernBERT to different language tasks.\n\nThe example below demonstrates how to predict the `[MASK]` token with [`Pipeline`], [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"answerdotai/ModernBERT-base\",\n    dtype=torch.float16,\n    device=0\n)\npipeline(\"Plants create [MASK]...",
            "tasks": [],
            "display_name": "ModernBERT"
        },
        {
            "model_name": "dinov2_with_registers",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dinov2_with_registers.md",
            "release_date": "2023-09-28",
            "transformers_date": "2024-12-24",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The DINOv2 with Registers model was proposed in [Vision Transformers Need Registers](https://huggingface.co/papers/2309.16588) by Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski.\n\nThe [Vision Transformer](vit) (ViT) is a transformer encoder model (BERT-like) originally introduced to do supervised image classification on ImageNet.\n\nNext, people figured out ways to make ViT work really well on self-supervised image feature extraction (i.e. learning meaningful features, also called embeddings) on images without requiring any labels. Some example papers here include [DINOv2](dinov2) and [MAE](vit_mae).\n\nThe authors of DINOv2 noticed that ViTs have artifacts in attention maps. It's due to the model using some image patches as \u201cregisters\u201d. The authors propose a fix: just add some new tokens (called \"register\" tokens), which you only use during pre-training (and throw away afterwards). This results in:\n\n- no artifacts\n- interpretable attention maps\n- and improved...",
            "tasks": [],
            "display_name": "DINOv2 with Registers"
        },
        {
            "model_name": "diffllama",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\diffllama.md",
            "release_date": "2024-10-07",
            "transformers_date": "2025-01-07",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The DiffLlama model was proposed in [Differential Transformer](https://huggingface.co/papers/2410.05258) by Kazuma Matsumoto and .\nThis model is combine Llama model and Differential Transformer's Attention.\n\nThe abstract from the paper is the following:\n\n*Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context...",
            "tasks": [],
            "display_name": "DiffLlama"
        },
        {
            "model_name": "textnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\textnet.md",
            "release_date": "2021-11-03",
            "transformers_date": "2025-01-08",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The TextNet model was proposed in [FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation](https://huggingface.co/papers/2111.02394) by Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu. TextNet is a vision backbone useful for text detection tasks. It is the result of neural architecture search (NAS) on backbones with reward function as text detection task (to provide powerful features for text detection).\n\n\n\n TextNet backbone as part of FAST. Taken from the original paper. \n\nThis model was contributed by [Raghavan](https://huggingface.co/Raghavan), [jadechoghari](https://huggingface.co/jadechoghari) and [nielsr](https://huggingface.co/nielsr).\n\n## Usage tips\n\nTextNet is mainly used as a backbone network for the architecture search of text detection. Each stage of the backbone network is comprised of a stride-2 convolution and searchable blocks.\nSpecifically, we present a layer-level candidate set, defined as {conv3\u00d73, conv1\u00d73,...",
            "tasks": [],
            "display_name": "TextNet"
        },
        {
            "model_name": "vitpose",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vitpose.md",
            "release_date": "2022-04-26",
            "transformers_date": "2025-01-08",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[ViTPose](https://huggingface.co/papers/2204.12484) is a vision transformer-based model for keypoint (pose) estimation. It uses a simple, non-hierarchical [ViT](./vit) backbone and a lightweight decoder head. This architecture simplifies model design, takes advantage of transformer scalability, and can be adapted to different training strategies.\n\n[ViTPose++](https://huggingface.co/papers/2212.04246) improves on ViTPose by incorporating a mixture-of-experts (MoE) module in the backbone and using more diverse pretraining data.\n\n\n\nYou can find all ViTPose and ViTPose++ checkpoints under the [ViTPose collection](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335).\n\nThe example below demonstrates pose estimation with the [`VitPoseForPoseEstimation`] class.\n\n```py\nimport torch\nimport requests\nimport numpy as np\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation,...",
            "tasks": [],
            "display_name": "ViTPose"
        },
        {
            "model_name": "emu3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\emu3.md",
            "release_date": "2024-09-27",
            "transformers_date": "2025-01-10",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://huggingface.co/papers/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang.\n\nEmu3 is a multimodal LLM that uses vector quantization to tokenize images into discrete tokens. Discretized image tokens are later fused with text token ids for image and text generation. The model can additionally generate images by predicting image token ids.\n\nThe abstract from the paper is the following:\n\n*While next-token prediction is considered a promising path towards artificial general intelligence, it has struggled to excel in multimodal tasks, which are still dominated by diffusion models (e.g., Stable Diffusion) and compositional approaches (e.g., CLIP...",
            "tasks": [],
            "display_name": "Emu3"
        },
        {
            "model_name": "moonshine",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\moonshine.md",
            "release_date": "2024-10-21",
            "transformers_date": "2025-01-10",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[Moonshine](https://huggingface.co/papers/2410.15608) is an encoder-decoder speech recognition model optimized for real-time transcription and recognizing voice command. Instead of using traditional absolute position embeddings, Moonshine uses Rotary Position Embedding (RoPE) to handle speech with varying lengths without using padding. This improves efficiency during inference, making it ideal for resource-constrained devices.\n\nYou can find all the original Moonshine checkpoints under the [Useful Sensors](https://huggingface.co/UsefulSensors) organization.\n\n> [!TIP]\n> Click on the Moonshine models in the right sidebar for more examples of how to apply Moonshine to different speech recognition tasks.\n\nThe example below demonstrates how to transcribe speech into text with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"automatic-speech-recognition\",\n    model=\"UsefulSensors/moonshine-base\",\n   ...",
            "tasks": [],
            "display_name": "Moonshine"
        },
        {
            "model_name": "helium",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\helium.md",
            "release_date": "2025-01-13",
            "transformers_date": "2025-01-13",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "It supports the following languages: English, French, German, Italian, Portuguese, Spanish.\n\n- **Developed by:** Kyutai\n- **Model type:** Large Language Model\n- **Language(s) (NLP):** English, French, German, Italian, Portuguese, Spanish\n- **License:** CC-BY 4.0\n\n## Evaluation\n\n\n\n#### Testing Data\n\n\n\nThe model was evaluated on MMLU, TriviaQA, NaturalQuestions, ARC Easy & Challenge, Open Book QA, Common Sense QA,\nPhysical Interaction QA, Social Interaction QA, HellaSwag, WinoGrande, Multilingual Knowledge QA, FLORES 200.\n\n#### Metrics\n\n\n\nWe report accuracy on MMLU, ARC, OBQA, CSQA, PIQA, SIQA, HellaSwag, WinoGrande.\nWe report exact match on TriviaQA, NQ and MKQA.\nWe report BLEU on FLORES.\n\n### English Results\n\n| Benchmark | Helium-1 Preview | HF SmolLM2 (1.7B) | Gemma-2 (2.6B) | Llama-3.2 (3B) | Qwen2.5 (1.5B) |\n|--------------|--------|--------|--------|--------|--------|\n| | | | | | |\n| MMLU | 51.2 | 50.4 | 53.1 | 56.6 | 61.0 |\n| NQ   | 17.3 | 15.1 | 17.7 | 22.0 | 13.1 |\n| TQA  |...",
            "tasks": [],
            "display_name": "Helium"
        },
        {
            "model_name": "superglue",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\superglue.md",
            "release_date": "2019-11-26",
            "transformers_date": "2025-01-20",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[SuperGlue](https://huggingface.co/papers/1911.11763) is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. SuperGlue introduces a flexible context aggregation mechanism based on attention, enabling it to reason about the underlying 3D scene and feature assignments jointly. Paired with the [SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and estimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\n\nYou can find all the original SuperGlue checkpoints under the [Magic Leap Community](https://huggingface.co/magic-leap-community) organization.\n\n> [!TIP]\n> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n>\n> Click on the...",
            "tasks": [],
            "display_name": "SuperGlue"
        },
        {
            "model_name": "granitevision",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\granitevision.md",
            "release_date": "2024-12-18",
            "transformers_date": "2025-01-23",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [Granite Vision](https://www.ibm.com/new/announcements/ibm-granite-3-1-powerful-performance-long-context-and-more) model is a variant of [LLaVA-NeXT](llava_next), leveraging a [Granite](granite) language model alongside a [SigLIP](SigLIP) visual encoder. It utilizes multiple concatenated vision hidden states as its image features, similar to [VipLlava](vipllava). It also uses a larger set of image grid pinpoints than the original LlaVa-NeXT models to support additional aspect ratios.\n\nTips:\n\n- This model is loaded into Transformers as an instance of LlaVA-Next. The usage and tips from [LLaVA-NeXT](llava_next) apply to this model as well.\n\n- You can apply the chat template on the tokenizer / processor in the same way as well. Example chat format:\n\n```bash\n\"\\nWhat\u2019s shown in this image?\\n\\nThis image shows a red stop sign.\\nDescribe the image in more details.\\n\\n\"\n```\n\nSample inference:\n\n```python\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration,...",
            "tasks": [],
            "display_name": "Granite Vision"
        },
        {
            "model_name": "qwen2_5_vl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen2_5_vl.md",
            "release_date": "2025-02-19",
            "transformers_date": "2025-01-23",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Qwen2.5-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model, available in 3B, 7B, and 72B parameters, pretrained on 4.1T tokens. The model introduces window attention in the ViT encoder to accelerate training and inference, dynamic FPS sampling on the spatial and temporal dimensions for better video understanding across different sampling rates, and an upgraded MRoPE (multi-resolutional rotary positional encoding) mechanism to better capture and learn temporal dynamics.\n\nYou can find all the original Qwen2.5-VL checkpoints under the [Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5) collection.\n\n> [!TIP]\n> Click on the Qwen2.5-VL models in the right sidebar for more examples of how to apply Qwen2.5-VL to different vision and language tasks.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import...",
            "tasks": [],
            "display_name": "Qwen2.5-VL"
        },
        {
            "model_name": "zamba2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\zamba2.md",
            "release_date": "2024-11-22",
            "transformers_date": "2025-01-27",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Zamba2](https://huggingface.co/papers/2411.15242) is a large language model (LLM) trained by Zyphra, and made available under an Apache 2.0 license. Please see the [Zyphra Hugging Face](https://huggingface.co/collections/zyphra/) repository for model weights.\n\nThis model was contributed by [pglo](https://huggingface.co/pglo).\n\n## Model details\n\n[Zamba2-1.2B](https://www.zyphra.com/post/zamba2-mini), [Zamba2-2.7B](https://www.zyphra.com/post/zamba2-small) and [Zamba2-7B](https://www.zyphra.com/post/zamba2-7b) are hybrid models combining state-space models (Specifically [Mamba2](https://github.com/state-spaces/mamba)) and transformer, and were trained using next-token prediction. Zamba2 uses shared transformer layers after every 6 mamba blocks. It uses the [Mistral v0.1 tokenizer](https://huggingface.co/mistralai/Mistral-7B-v0.1). We came to this architecture after a series of ablations at small scales. Zamba2-1.2B, Zamba2-2.7B and Zamba2-7B were pre-trained on 2T and 3T tokens,...",
            "tasks": [],
            "display_name": "Zamba2"
        },
        {
            "model_name": "got_ocr2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\got_ocr2.md",
            "release_date": "2024-09-03",
            "transformers_date": "2025-01-31",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The GOT-OCR2 model was proposed in [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://huggingface.co/papers/2409.01704) by Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang.\n\nThe abstract from the paper is the following:\n\n*Traditional OCR systems (OCR-1.0) are increasingly unable to meet people\u2019snusage due to the growing demand for intelligent processing of man-made opticalncharacters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle...",
            "tasks": [],
            "display_name": "GOT-OCR2"
        },
        {
            "model_name": "dab-detr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dab-detr.md",
            "release_date": "2022-01-28",
            "transformers_date": "2025-02-04",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The DAB-DETR model was proposed in [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://huggingface.co/papers/2201.12329) by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.\nDAB-DETR is an enhanced variant of Conditional DETR. It utilizes dynamically updated anchor boxes to provide both a reference query point (x, y) and a reference anchor size (w, h), improving cross-attention computation. This new approach achieves 45.7% AP when trained for 50 epochs with a single ResNet-50 model as the backbone.\n\n\n\nThe abstract from the paper is the following:\n\n*We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as queries\nin Transformer decoders and dynamically updates them layer-by-layer. Using box\ncoordinates not only helps using explicit positional priors to improve the...",
            "tasks": [],
            "display_name": "DAB-DETR"
        },
        {
            "model_name": "rt_detr_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\rt_detr_v2.md",
            "release_date": "2024-07-24",
            "transformers_date": "2025-02-06",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The RT-DETRv2 model was proposed in [RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](https://huggingface.co/papers/2407.17140) by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu.\n\nRT-DETRv2 refines RT-DETR by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. These changes enhance flexibility and practicality while maintaining real-time performance.\n\nThe abstract from the paper is the following:\n\n*In this report, we present RT-DETRv2, an improved Real-Time DEtection TRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art real-time detector, RT-DETR, and opens up a set of bag-of-freebies for flexibility and practicality, as well as optimizing the training strategy to achieve enhanced performance. To improve the flexibility, we suggest...",
            "tasks": [],
            "display_name": "RT-DETRv2"
        },
        {
            "model_name": "depth_pro",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\depth_pro.md",
            "release_date": "2024-10-02",
            "transformers_date": "2025-02-10",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The DepthPro model was proposed in [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://huggingface.co/papers/2410.02073) by Aleksei Bochkovskii, Ama\u00ebl Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.\n\nDepthPro is a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. It employs a multi-scale Vision Transformer (ViT)-based architecture, where images are downsampled, divided into patches, and processed using a shared Dinov2 encoder. The extracted patch-level features are merged, upsampled, and refined using a DPT-like fusion stage, enabling precise depth estimation.\n\nThe abstract from the paper is the following:\n\n*We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The...",
            "tasks": [],
            "display_name": "DepthPro"
        },
        {
            "model_name": "granitemoeshared",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\granitemoeshared.md",
            "release_date": "2024-08-23",
            "transformers_date": "2025-02-14",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://huggingface.co/papers/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n\nAdditionally this class GraniteMoeSharedModel adds shared experts for Moe.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"ibm-research/moe-7b-1b-active-shared-experts\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\nmodel.eval()\n\n# change input text as desired\nprompt = \"Write a code to find the maximum value in a list of numbers.\"\n\n# tokenize the text\ninput_tokens = tokenizer(prompt, return_tensors=\"pt\")\n# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into...",
            "tasks": [],
            "display_name": "GraniteMoeShared"
        },
        {
            "model_name": "smolvlm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\smolvlm.md",
            "release_date": "2025-02-20",
            "transformers_date": "2025-02-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[SmolVLM2](https://huggingface.co/papers/2504.05299) ([blog post](https://huggingface.co/blog/smolvlm2)) is an adaptation of the Idefics3 model with two main differences:\n\n- It uses SmolLM2 for the text model.\n- It supports multi-image and video inputs\n\n## Usage tips\n\nInput images are processed either by upsampling (if resizing is enabled) or at their original resolution. The resizing behavior depends on two parameters: do_resize and size.\n\nVideos should not be upsampled.\n\nIf `do_resize` is set to `True`, the model resizes images so that the longest edge is 4*512 pixels by default.\nThe default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 512}` is the default, but you can change it to a different value if needed.\n\nHere's how to control resizing and set a custom size:\n\n```python\nimage_processor = SmolVLMImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 512}, max_image_size=512)\n```\n\nAdditionally, the...",
            "tasks": [],
            "display_name": "SmolVLM"
        },
        {
            "model_name": "siglip2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\siglip2.md",
            "release_date": "2025-02-20",
            "transformers_date": "2025-02-21",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[SigLIP2](https://huggingface.co/papers/2502.14786) is a family of multilingual vision-language encoders that builds on the [SigLIP](./siglip) training recipe. It includes decoder-based pretraining, self-distillation, and masked prediction to improve dense prediction tasks (segmentation, depth estimation, etc.). This model is available in two variants:\n\n- NaFlex supports different resolutions and maintains the native image aspect ratio\n- FixRes supports fixed resolutions and is backwards compatible with [SigLIP](./siglip)\n\nYou can find all the original SigLIP2 checkpoints under the [SigLIP2](https://huggingface.co/collections/google/siglip2-67b5dcef38c175486e240107) collection.\n\n> [!TIP]\n> Click on the SigLIP2 models in the right sidebar for more examples of how to apply SigLIP2 to different image and text tasks.\n\nThe example below demonstrates zero-shot classification with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\nimage =...",
            "tasks": [],
            "display_name": "SigLIP2"
        },
        {
            "model_name": "aya_vision",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\aya_vision.md",
            "release_date": "2025-05-13",
            "transformers_date": "2025-03-04",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Aya Vision](https://huggingface.co/papers/2505.08751) is a family of open-weight multimodal vision-language models from Cohere Labs. It is trained with a synthetic annotation framework that generates high-quality multilingual image captions, improving Aya Vision's generated responses. In addition, a cross-modal model merging technique is used to prevent the model from losing its text capabilities after adding vision capabilities. The model combines a CommandR-7B language model with a SigLIP vision encoder.\n\nYou can find all the original Aya Vision checkpoints under the [Aya Vision](https://huggingface.co/collections/CohereLabs/cohere-labs-aya-vision-67c4ccd395ca064308ee1484) collection.\n\n> [!TIP]\n> This model was contributed by [saurabhdash](https://huggingface.co/saurabhdash) and [yonigozlan](https://huggingface.co/yonigozlan).\n>\n> Click on the Aya Vision models in the right sidebar for more examples of how to apply Aya Vision to different image-to-text tasks.\n\nThe example below...",
            "tasks": [],
            "display_name": "Aya Vision"
        },
        {
            "model_name": "gemma3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gemma3.md",
            "release_date": "2025-03-25",
            "transformers_date": "2025-03-12",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Gemma 3](https://huggingface.co/papers/2503.19786) is a multimodal model with pretrained and instruction-tuned variants, available in 1B, 13B, and 27B parameters. The architecture is mostly the same as the previous Gemma versions. The key differences are alternating 5 local sliding window self-attention layers for every global self-attention layer, support for a longer context length of 128K tokens, and a [SigLip](./siglip) encoder that can \"pan & scan\" high-resolution images to prevent information from disappearing in high resolution images or images with non-square aspect ratios.\n\nThe instruction-tuned variant was post-trained with knowledge distillation and reinforcement learning.\n\nYou can find all the original Gemma 3 checkpoints under the [Gemma 3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d) release.\n\n> [!TIP]\n> Click on the Gemma 3 models in the right sidebar for more examples of how to apply Gemma to different vision and language...",
            "tasks": [],
            "display_name": "Gemma 3"
        },
        {
            "model_name": "mistral3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mistral3.md",
            "release_date": "2025-01-30",
            "transformers_date": "2025-03-18",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Mistral 3](https://mistral.ai/news/mistral-small-3) is a latency optimized model with a lot fewer layers to reduce the time per forward pass. This model adds vision understanding and supports long context lengths of up to 128K tokens without compromising performance.\n\nYou can find the original Mistral 3 checkpoints under the [Mistral AI](https://huggingface.co/mistralai/models?search=mistral-small-3) organization.\n\n> [!TIP]\n> This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez) and [yonigozlan](https://huggingface.co/yonigozlan).\n> Click on the Mistral3 models in the right sidebar for more examples of how to apply Mistral3 to different tasks.\n\nThe example below demonstrates how to generate text for an image with [`Pipeline`] and the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"user\",\n        \"content\":[\n            {\"type\": \"image\",\n            \"image\":...",
            "tasks": [],
            "display_name": "Mistral 3"
        },
        {
            "model_name": "shieldgemma2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\shieldgemma2.md",
            "release_date": "2025-04-01",
            "transformers_date": "2025-03-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The ShieldGemma 2 model was proposed in a [technical report](https://huggingface.co/papers/2504.01081) by Google. ShieldGemma 2, built on [Gemma 3](https://ai.google.dev/gemma/docs/core/model_card_3), is a 4 billion (4B) parameter model that checks the safety of both synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm as defined below:\n\n- No Sexually Explicit content: The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).\n- No Dangerous Content: The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide).\n- No Violence/Gore...",
            "tasks": [],
            "display_name": "ShieldGemma 2"
        },
        {
            "model_name": "prompt_depth_anything",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\prompt_depth_anything.md",
            "release_date": "2024-12-18",
            "transformers_date": "2025-03-21",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The Prompt Depth Anything model was introduced in [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://huggingface.co/papers/2412.14015) by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang.\n\nThe abstract from the paper is as follows:\n\n*Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT...",
            "tasks": [],
            "display_name": "Prompt Depth Anything"
        },
        {
            "model_name": "phi4_multimodal",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\phi4_multimodal.md",
            "release_date": "2025-03-03",
            "transformers_date": "2025-03-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Phi4 Multimodal](https://huggingface.co/papers/2503.01743) is a multimodal model capable of text, image, and speech and audio inputs or any combination of these. It features a mixture of LoRA adapters for handling different inputs, and each input is routed to the appropriate encoder.\n\nYou can find all the original Phi4 Multimodal checkpoints under the [Phi4](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4) collection.\n\n> [!TIP]\n> This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez).\n>\n> Click on the Phi-4 Multimodal in the right sidebar for more examples of how to apply Phi-4 Multimodal to different tasks.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```python\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"microsoft/Phi-4-multimodal-instruct\", dtype=\"auto\", device=0)\n\nprompt = \"Explain the concept of multimodal AI in...",
            "tasks": [],
            "display_name": "Phi4 Multimodal"
        },
        {
            "model_name": "deepseek_v3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deepseek_v3.md",
            "release_date": "2024-12-27",
            "transformers_date": "2025-03-28",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The DeepSeek-V3 model was proposed in [DeepSeek-V3 Technical Report](https://huggingface.co/papers/2412.19437) by DeepSeek-AI Team.\n\nThe abstract from the paper is the following:\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source...",
            "tasks": [],
            "display_name": "DeepSeek-V3"
        },
        {
            "model_name": "qwen3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen3.md",
            "release_date": "2025-04-29",
            "transformers_date": "2025-03-31",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Qwen3](https://huggingface.co/papers/2505.09388) refers to the dense model architecture Qwen3-32B which was released with its mixture of experts variant [Qwen3MoE](qwen3_moe) ([blog post](https://qwenlm.github.io/blog/qwen3/)).\n\n### Model Details\n\nTo be released with the official model launch.\n\n## Usage tips\n\nTo be released with the official model launch.\n\n## Qwen3Config\n\n[[autodoc]] Qwen3Config\n\n## Qwen3Model\n\n[[autodoc]] Qwen3Model\n    - forward\n\n## Qwen3ForCausalLM\n\n[[autodoc]] Qwen3ForCausalLM\n    - forward\n\n## Qwen3ForSequenceClassification\n\n[[autodoc]] Qwen3ForSequenceClassification\n    - forward\n\n## Qwen3ForTokenClassification\n\n[[autodoc]] Qwen3ForTokenClassification\n    - forward\n\n## Qwen3ForQuestionAnswering\n\n[[autodoc]] Qwen3ForQuestionAnswering\n    - forward",
            "tasks": [],
            "display_name": "Qwen3"
        },
        {
            "model_name": "qwen3_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen3_moe.md",
            "release_date": "2025-04-29",
            "transformers_date": "2025-03-31",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Qwen3MoE](https://huggingface.co/papers/2505.09388) refers to the mixture of experts model architecture Qwen3-235B-A22B which was released with its dense variant [Qwen3](qwen3) ([blog post](https://qwenlm.github.io/blog/qwen3/)).\n\n### Model Details\n\nTo be released with the official model launch.\n\n## Usage tips\n\nTo be released with the official model launch.\n\n## Qwen3MoeConfig\n\n[[autodoc]] Qwen3MoeConfig\n\n## Qwen3MoeModel\n\n[[autodoc]] Qwen3MoeModel\n    - forward\n\n## Qwen3MoeForCausalLM\n\n[[autodoc]] Qwen3MoeForCausalLM\n    - forward\n\n## Qwen3MoeForSequenceClassification\n\n[[autodoc]] Qwen3MoeForSequenceClassification\n    - forward\n\n## Qwen3MoeForTokenClassification\n\n[[autodoc]] Qwen3MoeForTokenClassification\n    - forward\n\n## Qwen3MoeForQuestionAnswering\n\n[[autodoc]] Qwen3MoeForQuestionAnswering\n    - forward",
            "tasks": [],
            "display_name": "Qwen3MoE"
        },
        {
            "model_name": "llama4",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\llama4.md",
            "release_date": "2025-04-05",
            "transformers_date": "2025-04-05",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/), developed by Meta, introduces a new auto-regressive Mixture-of-Experts (MoE) architecture.\nThis generation includes two models:\n\n- The highly capable Llama 4 Maverick with 17B active parameters out of ~400B total, with 128 experts.\n- The efficient Llama 4 Scout also  has 17B active parameters out of ~109B total, using just 16 experts.\n-\n\nBoth models leverage early fusion for native multimodality, enabling them to process text and image inputs.\nMaverick and Scout are both trained on up to 40 trillion tokens on data encompassing 200 languages\n(with specific fine-tuning support for 12 languages including Arabic, Spanish, German, and Hindi).\n\nFor deployment, Llama 4 Scout is designed for accessibility, fitting on a single server-grade GPU via\non-the-fly 4-bit or 8-bitint4 quantization, while Maverick is available in BF16 and FP8 formats.\nThese models are released under the custom Llama 4 Community License Agreement,...",
            "tasks": [],
            "display_name": "Llama4"
        },
        {
            "model_name": "glm4",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\glm4.md",
            "release_date": "2024-06-18",
            "transformers_date": "2025-04-09",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The GLM family welcomes new members [GLM-4-0414](https://huggingface.co/papers/2406.12793) series models.\n\nThe **GLM-4-32B-0414** series models, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT\nseries and DeepSeek's V3/R1 series. It also supports very user-friendly local deployment features. GLM-4-32B-Base-0414\nwas pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the\nfoundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference\nalignment for dialogue scenarios. Additionally, using techniques like rejection sampling and reinforcement learning, we\nenhanced the model's performance in instruction following, engineering code, and function calling, thus strengthening\nthe atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in engineering code, Artifact\ngeneration, function calling, search-based Q&A, and report generation....",
            "tasks": [],
            "display_name": "Glm4"
        },
        {
            "model_name": "granite_speech",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\granite_speech.md",
            "release_date": "2025-04-16",
            "transformers_date": "2025-04-11",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The [Granite Speech](https://huggingface.co/papers/2505.08699) model ([blog post](https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras)) is a multimodal language model, consisting of a speech encoder, speech projector, large language model, and LoRA adapter(s). More details regarding each component for the current (Granite 3.2 Speech) model architecture may be found below.\n\n1. Speech Encoder: A [Conformer](https://huggingface.co/papers/2005.08100) encoder trained with Connectionist Temporal Classification (CTC) on character-level targets on ASR corpora. The encoder uses block-attention and self-conditioned CTC from the middle layer.\n\n2. Speech Projector: A query transformer (q-former) operating on the outputs of the last encoder block. The encoder and projector temporally downsample the audio features to be merged into the multimodal embeddings to be processed by the llm.\n\n3. Large Language Model: The Granite Speech model leverages...",
            "tasks": [],
            "display_name": "Granite Speech"
        },
        {
            "model_name": "qwen2_5_omni",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen2_5_omni.md",
            "release_date": "2025-03-26",
            "transformers_date": "2025-04-14",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unified multiple modalities model proposed in [Qwen2.5-Omni Technical Report](https://huggingface.co/papers/2503.20215) from Qwen team, Alibaba Group.\n\nThe abstract from the technical report is the following:\n\n*We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize...",
            "tasks": [],
            "display_name": "Qwen2.5-Omni"
        },
        {
            "model_name": "mlcd",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mlcd.md",
            "release_date": "2024-07-24",
            "transformers_date": "2025-04-15",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The [MLCD](https://huggingface.co/papers/2407.17331) models were released by the DeepGlint-AI team in [unicom](https://github.com/deepglint/unicom), which focuses on building foundational visual models for large multimodal language models using large-scale datasets such as LAION400M and COYO700M, and employs sample-to-cluster contrastive learning to optimize performance. MLCD models are primarily used for multimodal visual large language models, such as LLaVA.\n\n\ud83d\udd25**MLCD-ViT-bigG**\ud83d\udd25 series is the state-of-the-art vision transformer model enhanced with 2D Rotary Position Embedding (RoPE2D), achieving superior performance on document understanding and visual question answering tasks. Developed by DeepGlint AI, this model demonstrates exceptional capabilities in processing complex visual-language interactions.\n\nTips:\n\n- We adopted the official [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT) and the official training dataset...",
            "tasks": [],
            "display_name": "MLCD"
        },
        {
            "model_name": "timesfm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\timesfm.md",
            "release_date": "2023-10-14",
            "transformers_date": "2025-04-16",
            "modality": "timeseries",
            "modality_name": "Time Series Models",
            "modality_color": "#F97316",
            "description": "TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model proposed in [A decoder-only foundation model for time-series forecasting](https://huggingface.co/papers/2310.10688) by Abhimanyu Das, Weihao Kong, Rajat Sen, and  Yichen Zhou. It is a decoder only model that uses non-overlapping patches of time-series data as input and outputs some output patch length prediction in an autoregressive fashion.\n\nThe abstract from the paper is the following:\n\n*Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths...",
            "tasks": [],
            "display_name": "TimesFM"
        },
        {
            "model_name": "janus",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\janus.md",
            "release_date": "2024-10-17",
            "transformers_date": "2025-04-17",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Janus Model was originally proposed in [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://huggingface.co/papers/2410.13848) by DeepSeek AI team and later refined in [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://huggingface.co/papers/2501.17811). Janus is a vision-language model that can generate both image and text output, it can also take both images and text as input.\n\n> [!NOTE]\n> The model doesn't generate both images and text in an interleaved format. The user has to pass a parameter indicating whether to generate text or image.\n\nThe abstract from the original paper is the following:\n\n*In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal...",
            "tasks": [],
            "display_name": "Janus"
        },
        {
            "model_name": "internvl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\internvl.md",
            "release_date": "2025-04-14",
            "transformers_date": "2025-04-18",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The InternVL3 family of Visual Language Models was introduced in [InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models](https://huggingface.co/papers/2504.10479).\n\nThe abstract from the paper is the following:\n\n*We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts,...",
            "tasks": [],
            "display_name": "InternVL"
        },
        {
            "model_name": "bitnet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\bitnet.md",
            "release_date": "2025-04-16",
            "transformers_date": "2025-04-28",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "Trained on a corpus of 4 trillion tokens, this model demonstrates that native 1-bit LLMs can achieve performance comparable to leading open-weight, full-precision models of similar size, while offering substantial advantages in computational efficiency (memory, energy, latency).\n\n\u27a1\ufe0f **Technical Report:** [BitNet b1.58 2B4T Technical Report](https://huggingface.co/papers/2504.12285)\n\n\u27a1\ufe0f **Official Inference Code:** [microsoft/BitNet (bitnet.cpp)](https://github.com/microsoft/BitNet)\n\n## Model Variants\n\nSeveral versions of the model weights are available on Hugging Face:\n\n* [**`microsoft/bitnet-b1.58-2B-4T`**](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T): Contains the packed 1.58-bit weights optimized for efficient inference. **Use this for deployment.**\n\n* [**`microsoft/bitnet-b1.58-2B-4T-bf16`**](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-bf16): Contains the master weights in BF16 format. **Use this only for training or fine-tuning purposes.**\n\n*...",
            "tasks": [],
            "display_name": "BitNet"
        },
        {
            "model_name": "sam_hq",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\sam_hq.md",
            "release_date": "2023-06-02",
            "transformers_date": "2025-04-28",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "SAM-HQ (High-Quality Segment Anything Model) was proposed in [Segment Anything in High Quality](https://huggingface.co/papers/2306.01567) by Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu.\n\nThe model is an enhancement to the original SAM model that produces significantly higher quality segmentation masks while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability.\n\n![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png)\n\nSAM-HQ introduces several key improvements over the original SAM model:\n\n1. High-Quality Output Token: A learnable token injected into SAM's mask decoder for higher quality mask prediction\n2. Global-local Feature Fusion: Combines features from different stages of the model for improved mask details\n3. Training Data: Uses a carefully curated dataset of 44K high-quality masks instead of SA-1B\n4. Efficiency: Adds only...",
            "tasks": [],
            "display_name": "SAM-HQ"
        },
        {
            "model_name": "d_fine",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\d_fine.md",
            "release_date": "2024-10-17",
            "transformers_date": "2025-04-29",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The D-FINE model was proposed in [D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement](https://huggingface.co/papers/2410.13842) by\nYansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu\n\nThe abstract from the paper is the following:\n\n*We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD).\nFDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GO-LSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying...",
            "tasks": [],
            "display_name": "D-FINE"
        },
        {
            "model_name": "hgnet_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\hgnet_v2.md",
            "release_date": "2024-07-01",
            "transformers_date": "2025-04-29",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[HGNetV2](https://github.com/PaddlePaddle/PaddleClas/blob/v2.6.0/docs/zh_CN/models/ImageNet1k/PP-HGNetV2.md) is a next-generation convolutional neural network (CNN) backbone built for optimal accuracy-latency tradeoff on NVIDIA GPUs. Building on the original[HGNet](https://github.com/PaddlePaddle/PaddleClas/blob/v2.6.0/docs/en/models/PP-HGNet_en.md), HGNetV2 delivers high accuracy at fast inference speeds and performs strongly on tasks like image classification, object detection, and segmentation, making it a practical choice for GPU-based computer vision applications.\n\nYou can find all the original HGNet V2 models under the [USTC](https://huggingface.co/ustc-community/models?search=hgnet) organization.\n\n> [!TIP]\n> This model was contributed by [VladOS95-cyber](https://github.com/VladOS95-cyber).\n> Click on the HGNet V2 models in the right sidebar for more examples of how to apply HGNet V2 to different computer vision tasks.\n\nThe example below demonstrates how to classify an image...",
            "tasks": [],
            "display_name": "HGNet-V2"
        },
        {
            "model_name": "granitemoehybrid",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\granitemoehybrid.md",
            "release_date": "2025-05-02",
            "transformers_date": "2025-05-05",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\nmodel.eval()\n\n# change input text as desired\nprompt = \"Write a code to find the maximum value in a list of numbers.\"\n\n# tokenize the text\ninput_tokens = tokenizer(prompt, return_tensors=\"pt\")\n# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# loop over the batch to print, in this example the batch size is 1\nfor i in output:\n    print(i)\n```\n\nThis HF implementation is contributed by [Sukriti Sharma](https://huggingface.co/SukritiSharma) and [Alexander Brooks](https://huggingface.co/abrooks9944).\n\n## Notes\n\n- `GraniteMoeHybridForCausalLM` supports padding-free training which concatenates distinct training examples while still processing inputs as separate batches. It can significantly accelerate inference by...",
            "tasks": [],
            "display_name": "GraniteMoeHybrid"
        },
        {
            "model_name": "csm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\csm.md",
            "release_date": "2025-02-27",
            "transformers_date": "2025-05-07",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The Conversational Speech Model (CSM) is the first open-source contextual text-to-speech model [released by Sesame](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice). It is designed to generate natural-sounding speech with or without conversational context. This context typically consists of multi-turn dialogue between speakers, represented as sequences of text and corresponding spoken audio.\n\n**Model Architecture:**\nCSM is composed of two LLaMA-style auto-regressive transformer decoders: a backbone decoder that predicts the first codebook token and a depth decoder that generates the remaining tokens. It uses the pretrained codec model [Mimi](./mimi), introduced by Kyutai, to encode speech into discrete codebook tokens and decode them back into audio.\n\nThe original csm-1b checkpoint is available under the [Sesame](https://huggingface.co/sesame/csm-1b) organization on Hugging Face.\n\n\n    \n\n\n## Usage Tips\n\n### Without Conversational Context\n\nCSM can be used to simply...",
            "tasks": [],
            "display_name": "Csm"
        },
        {
            "model_name": "falcon_h1",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\falcon_h1.md",
            "release_date": "2025-05-21",
            "transformers_date": "2025-05-21",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The [FalconH1](https://huggingface.co/blog/tiiuae/falcon-h1) model was developed by the TII Pretraining team. A comprehensive research paper covering the architecture, pretraining dynamics, experimental results, and conclusions is forthcoming. You can read more about this series in [this website](https://github.com/tiiuae/Falcon-H1).\n\n## Contributors\n\nThis model was contributed by [DhiyaEddine](https://huggingface.co/DhiyaEddine), [ybelkada](https://huggingface.co/ybelkada), [JingweiZuo](https://huggingface.co/JingweiZuo), [IlyasChahed](https://huggingface.co/IChahed), and [MaksimVelikanov](https://huggingface.co/yellowvm).\nThe original code can be found [here](https://github.com/tiiuae/Falcon-H1).\n\n## FalconH1Config\n\n| Model     | Depth | Dim  | Attn Heads | KV | Mamba Heads | d_head       | d_state | Ctx Len        |\n|-----------|--------|------|------------|----|--------------|--------------|------|-----------------|\n| H1 0.5B   | 36     | 1024 | 8          | 2  | 24           | 64...",
            "tasks": [],
            "display_name": "FalconH1"
        },
        {
            "model_name": "colqwen2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\colqwen2.md",
            "release_date": "2024-06-27",
            "transformers_date": "2025-06-02",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[ColQwen2](https://huggingface.co/papers/2407.01449) is a variant of the [ColPali](./colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](./qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n\nThis model was contributed by [@tonywu71](https://huggingface.co/tonywu71) (ILLUIN Technology) and [@yonigozlan](https://huggingface.co/yonigozlan) (HuggingFace).\n\nYou can find all the original ColPali checkpoints under Vidore's [Hf-native ColVision...",
            "tasks": [],
            "display_name": "ColQwen2"
        },
        {
            "model_name": "minimax",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\minimax.md",
            "release_date": "2025-01-14",
            "transformers_date": "2025-06-04",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The MiniMax-Text-01 model was proposed in [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://huggingface.co/papers/2501.08313) by MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi...",
            "tasks": [],
            "display_name": "MiniMax"
        },
        {
            "model_name": "vjepa2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vjepa2.md",
            "release_date": "2025-06-11",
            "transformers_date": "2025-06-11",
            "modality": "video",
            "modality_name": "Video Models",
            "modality_color": "#EC4899",
            "description": "[V-JEPA 2](https://huggingface.co/papers/2506.09985) ([blog post](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)) is a self-supervised approach to training video encoders developed by FAIR, Meta. Using internet-scale video data, V-JEPA 2 attains state-of-the-art performance on motion understanding and human action anticipation tasks. V-JEPA 2-AC is a latent action-conditioned world model post-trained from V-JEPA 2 (using a small amount of robot trajectory interaction data) that solves robot manipulation tasks without environment-specific data collection or task-specific training or calibration.\n\n\n    \n\n\nYou can find all original V-JEPA2 checkpoints under the [V-JEPA 2](https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6) collection.\n\nThis model was contributed by [koustuvs](https://huggingface.co/koustuvs), [yonigozlan](https://huggingface.co/yonigozlan) and [qubvel](https://huggingface.co/qubvel-hf). The original code can be found...",
            "tasks": [],
            "display_name": "V-JEPA 2"
        },
        {
            "model_name": "lightglue",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\lightglue.md",
            "release_date": "2023-06-23",
            "transformers_date": "2025-06-17",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[LightGlue](https://huggingface.co/papers/2306.13643) is a deep neural network that learns to match local features across images. It revisits multiple design decisions of SuperGlue and derives simple but effective improvements. Cumulatively, these improvements make LightGlue more efficient - in terms of both memory and computation, more accurate, and much easier to train. Similar to [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor), this model consists of matching two sets of local features extracted from two images, with the goal of being faster than SuperGlue. Paired with the [SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and estimate the pose between them.\n\nYou can find all the original LightGlue checkpoints under the [ETH-CVG](https://huggingface.co/ETH-CVG) organization.\n\n> [!TIP]\n> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n>\n> Click on the...",
            "tasks": [],
            "display_name": "LightGlue"
        },
        {
            "model_name": "arcee",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\arcee.md",
            "release_date": "2025-06-18",
            "transformers_date": "2025-06-24",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Arcee](https://www.arcee.ai/blog/deep-dive-afm-4-5b-the-first-arcee-foundational-model) is a decoder-only transformer model based on the Llama architecture with a key modification: it uses ReLU\u00b2 (ReLU-squared) activation in the MLP blocks instead of SiLU, following recent research showing improved training efficiency with squared activations. This architecture is designed for efficient training and inference while maintaining the proven stability of the Llama design.\n\nThe Arcee model is architecturally similar to Llama but uses `x * relu(x)` in MLP layers for improved gradient flow and is optimized for efficiency in both training and inference scenarios.\n\n> [!TIP]\n> The Arcee model supports extended context with RoPE scaling and all standard transformers features including Flash Attention 2, SDPA, gradient checkpointing, and quantization support.\n\nThe example below demonstrates how to generate text with Arcee using [`Pipeline`] or the [`AutoModel`].\n\n\n\n\n```py\nimport torch\nfrom...",
            "tasks": [],
            "display_name": "Arcee"
        },
        {
            "model_name": "dots1",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dots1.md",
            "release_date": "2025-06-06",
            "transformers_date": "2025-06-25",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The `dots.llm1` model was proposed in [dots.llm1 technical report](https://huggingface.co/papers/2506.05767) by rednote-hilab team.\n\nThe abstract from the report is the following:\n\n*Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on high-quality corpus and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints spanning the entire training process, providing...",
            "tasks": [],
            "display_name": "dots.llm1"
        },
        {
            "model_name": "glm4v",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\glm4v.md",
            "release_date": "2025-07-01",
            "transformers_date": "2025-06-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "**GLM-4.1V-9B-Thinking** is a bilingual vision-language model optimized for reasoning, built on GLM-4-9B. It introduces\na \"thinking paradigm\" with reinforcement learning, achieving state-of-the-art results among 10B-class models and\nrivaling 72B-scale models. It supports 64k context, 4K resolution, and arbitrary aspect ratios, with an open-source base\nmodel for further research. You can check our paper [here](https://huggingface.co/papers/2507.01006). and below is a abstract.\n\n*We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal understanding\nand reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework.\nWe first develop a capable vision foundation model with significant potential through large-scale pre-training, which\narguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum\nSampling (RLCS) to unlock the full potential...",
            "tasks": [],
            "display_name": "GLM-4.1V"
        },
        {
            "model_name": "kyutai_speech_to_text",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\kyutai_speech_to_text.md",
            "release_date": "2025-06-17",
            "transformers_date": "2025-06-25",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[Kyutai STT](https://kyutai.org/next/stt) is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai's lab has released two model checkpoints:\n\n- [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\n- [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\n\n\n    \n\n\n## Usage Tips\n\n### Inference\n\n```python\nimport torch\nfrom datasets import load_dataset, Audio\nfrom transformers import infer_device, KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n\n# 1. load the model and the processor\ntorch_device = infer_device()\nmodel_id =...",
            "tasks": [],
            "display_name": "Kyutai Speech-To-Text"
        },
        {
            "model_name": "smollm3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\smollm3.md",
            "release_date": "2025-07-08",
            "transformers_date": "2025-06-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[SmolLM3](https://huggingface.co/blog/smollm3) is a fully open, compact language model designed for efficient deployment while maintaining strong performance. It uses a Transformer decoder architecture with Grouped Query Attention (GQA) to reduce the kv cache, and no RoPE, enabling improved performance on long-context tasks. It is trained using a multi-stage training approach on high-quality public datasets across web, code, and math domains. The model is multilingual and supports very large context lengths. The instruct variant is optimized for reasoning and tool use.\n\n> [!TIP]\n> Click on the SmolLM3 models in the right sidebar for more examples of how to apply SmolLM3 to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line using the instruction-tuned models.\n\n\n\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"text-generation\",\n   ...",
            "tasks": [],
            "display_name": "SmolLM3"
        },
        {
            "model_name": "t5gemma",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\t5gemma.md",
            "release_date": "2025-04-08",
            "transformers_date": "2025-06-25",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "T5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://huggingface.co/papers/2504.06225) by Google. It is a family of encoder-decoder large language models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\n\nT5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the official Gemma 2 models (2B and 9B); and 2) [T5](https://huggingface.co/papers/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\n\nThe...",
            "tasks": [],
            "display_name": "T5Gemma"
        },
        {
            "model_name": "dia",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dia.md",
            "release_date": "2025-04-21",
            "transformers_date": "2025-06-26",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "[Dia](https://github.com/nari-labs/dia) is an open-source text-to-speech (TTS) model (1.6B parameters) developed by [Nari Labs](https://huggingface.co/nari-labs).\nIt can generate highly realistic dialogue from transcript including non-verbal communications such as laughter and coughing.\nFurthermore, emotion and tone control is also possible via audio conditioning (voice cloning).\n\n**Model Architecture:**\nDia is an encoder-decoder transformer based on the original transformer architecture. However, some more modern features such as\nrotational positional embeddings (RoPE) are also included. For its text portion (encoder), a byte tokenizer is utilized while\nfor the audio portion (decoder), a pretrained codec model [DAC](./dac) is used - DAC encodes speech into discrete codebook\ntokens and decodes them back into audio.\n\n## Usage Tips\n\n### Generation with Text\n\n```python\nfrom transformers import AutoProcessor, DiaForConditionalGeneration, infer_device\n\ntorch_device =...",
            "tasks": [],
            "display_name": "Dia"
        },
        {
            "model_name": "gemma3n",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gemma3n.md",
            "release_date": "2025-05-20",
            "transformers_date": "2025-06-26",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Gemma3n](https://developers.googleblog.com/en/introducing-gemma-3n/) is a multimodal model with pretrained and instruction-tuned variants, available in E4B and E2B sizes. While\nlarge portions of the language model architecture are shared with prior Gemma releases, there are many new additions in\nthis model, including [Alternating Updates][altup] (AltUp), [Learned Augmented Residual Layer][laurel] (LAuReL),\n[MatFormer][matformer], Per-Layer Embeddings (PLE), [Activation Sparsity with Statistical Top-k][spark-transformer], and KV cache sharing. The language model uses\na similar attention pattern to [Gemma 3](./gemma3) with alternating 4 local sliding window self-attention layers for\nevery global self-attention layer with a maximum context length of 32k tokens. Gemma 3n introduces\n[MobileNet v5][mobilenetv5] as the vision encoder, using a default resolution of 768x768 pixels, and adds a newly\ntrained audio encoder based on the [Universal Speech Model][usm] (USM) architecture.\n\nThe...",
            "tasks": [],
            "display_name": "Gemma3n"
        },
        {
            "model_name": "eomt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\eomt.md",
            "release_date": "2025-03-24",
            "transformers_date": "2025-06-27",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[The Encoder-only Mask Transformer]((https://www.tue-mps.org/eomt)) (EoMT) model was introduced in the CVPR 2025 Highlight Paper *[Your ViT is Secretly an Image Segmentation Model](https://huggingface.co/papers/2503.19108)* by Tommie Kerssies, Niccol\u00f2 Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, and Daan de Geus.\nEoMT reveals Vision Transformers can perform image segmentation efficiently without task-specific components.\n\nThe abstract from the paper is the following:\n\n*Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be...",
            "tasks": [],
            "display_name": "EoMT"
        },
        {
            "model_name": "aimv2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\aimv2.md",
            "release_date": "2024-11-21",
            "transformers_date": "2025-07-08",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "The AIMv2 model was proposed in [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://huggingface.co/papers/2411.14402) by Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis B\u00e9thune, Zhe Gan, Alexander T Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, Alaaeldin El-Nouby.\n\nThe abstract from the paper is the following:\n\n*We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw...",
            "tasks": [],
            "display_name": "AIMv2"
        },
        {
            "model_name": "doge",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\doge.md",
            "release_date": "2024-12-27",
            "transformers_date": "2025-07-08",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "Doge is a series of small language models based on the [Doge](https://github.com/SmallDoges/small-doge) architecture, aiming to combine the advantages of state-space and self-attention algorithms, calculate dynamic masks from cached value states using the zero-order hold method, and solve the problem of existing mainstream language models getting lost in context. It uses the `wsd_scheduler` scheduler to pre-train on the `smollm-corpus`, and can continue training on new datasets or add sparse activation feedforward networks from stable stage checkpoints.\n\n\n\nAs shown in the figure below, the sequence transformation part of the Doge architecture uses `Dynamic Mask Attention`, which can be understood as using self-attention related to value states during training, and using state-space without past state decay during inference, to solve the problem of existing Transformers or SSMs getting lost in long text. The state transformation part of Doge uses `Cross Domain Mixture of Experts`,...",
            "tasks": [],
            "display_name": "Doge"
        },
        {
            "model_name": "deepseek_v2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deepseek_v2.md",
            "release_date": "2024-05-07",
            "transformers_date": "2025-07-09",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The DeepSeek-V2 model was proposed in [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://huggingface.co/papers/2405.04434) by DeepSeek-AI Team.\n\nThe abstract from the paper is the following:\nWe present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum...",
            "tasks": [],
            "display_name": "DeepSeek-V2"
        },
        {
            "model_name": "lfm2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\lfm2.md",
            "release_date": "2025-07-10",
            "transformers_date": "2025-07-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment.\n\nThe models are available in three sizes (350M, 700M, and 1.2B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n\n## Architecture\n\nThe architecture consists of 16 blocks total: 10 double-gated short-range convolution blocks and 6 blocks of grouped query attention. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates, allowing for \"liquid\" dynamics that can adapt in real-time. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without...",
            "tasks": [],
            "display_name": "LFM2"
        },
        {
            "model_name": "perception_lm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\perception_lm.md",
            "release_date": "2025-04-17",
            "transformers_date": "2025-07-11",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [PerceptionLM](https://huggingface.co/papers/2504.13180) model was proposed in [PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding](https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding/) by Jang Hyun Cho et al. It's a fully open, reproducible model for transparent research in image and video understanding. PLM consists of\na vision encoder with a small scale (<8B parameters) LLM decoder.\n\nThe abstract from the paper is the following:\n\n*Vision-language models are integral to computer vision research, yet many high-performing models\nremain closed-source, obscuring their data, design and training recipe. The research community\nhas responded by using distillation from black-box models to label training data, achieving strong\nbenchmark results, at the cost of measurable scientific progress. However, without knowing the details\nof the teacher model and its data sources, scientific progress remains...",
            "tasks": [],
            "display_name": "PerceptionLM"
        },
        {
            "model_name": "modernbert-decoder",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\modernbert-decoder.md",
            "release_date": "2024-12-18",
            "transformers_date": "2025-07-15",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "ModernBERT Decoder has the same architecture as [ModernBERT](https://huggingface.co/papers/2412.13663) but it is trained from scratch with a causal language modeling objective from the [Ettin paper](https://huggingface.co/papers/2507.11412). This allows for using the same architecture to compare encoders and decoders. This model is the decoder architecture implementation of ModernBERT, designed for autoregressive text generation tasks.\n\nModernBERT Decoder uses sliding window attention and rotary positional embeddings for efficiency and to handle longer sequences.\n\nYou can find all the original ModernBERT Decoder checkpoints under the [jhu-clsp](https://huggingface.co/collections/jhu-clsp/encoders-vs-decoders-the-ettin-suite-686303e16142257eed8e6aeb) collection.\n\n> [!TIP]\n> This model was contributed by [orionw](https://huggingface.co/orionweller).\n>\n> Click on the ModernBERT Decoder models in the right sidebar for more examples of how to apply ModernBERT Decoder to different text...",
            "tasks": [],
            "display_name": "ModernBERT Decoder"
        },
        {
            "model_name": "voxtral",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\voxtral.md",
            "release_date": "2025-07-15",
            "transformers_date": "2025-07-18",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "Voxtral is an upgrade of [Ministral 3B and Mistral Small 3B](https://mistral.ai/news/ministraux), extending its language capabilities with audio input support. It is designed to handle tasks such as speech transcription, translation, and audio understanding.\n\nYou can read more in Mistral's [realease blog post](https://mistral.ai/news/voxtral).\n\nThe model is available in two checkpoints:\n\n- 3B: [mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)\n- 24B: [mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)\n\n## Key Features\n\nVoxtral builds on Ministral-3B by adding audio processing capabilities:\n\n- **Transcription mode**: Includes a dedicated mode for speech transcription. By default, Voxtral detects the spoken language and transcribes it accordingly.  \n- **Long-form context**: With a 32k token context window, Voxtral can process up to 30 minutes of audio for transcription or 40 minutes for broader audio...",
            "tasks": [],
            "display_name": "Voxtral"
        },
        {
            "model_name": "ernie4_5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ernie4_5.md",
            "release_date": "2025-06-30",
            "transformers_date": "2025-07-21",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Ernie 4.5 model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\nThis family of models contains multiple different architectures and model sizes. This model in specific targets the base text\nmodel without mixture of experts (moe) with 0.3B parameters in total. It uses the standard [Llama](./llama) at its core.\n\nOther models from the family can be found at [Ernie 4.5 Moe](./ernie4_5_moe).\n\n\n    \n\n\n## Usage Tips\n\n### Generate text\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-0.3B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    dtype=torch.bfloat16,\n)\n\n# prepare the model input\ninputs = tokenizer(\"Hey, are you conscious? Can you talk to me?\", return_tensors=\"pt\")\nprompt = \"Hey, are you conscious? Can you talk to...",
            "tasks": [],
            "display_name": "Ernie 4.5"
        },
        {
            "model_name": "ernie4_5_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ernie4_5_moe.md",
            "release_date": "2025-06-30",
            "transformers_date": "2025-07-21",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Ernie 4.5 Moe model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\nThis family of models contains multiple different architectures and model sizes. This model in specific targets the base text\nmodel with mixture of experts (moe) - one with 21B total, 3B active parameters and another one with 300B total, 47B active parameters.\nIt uses the standard [Llama](./llama) at its core combined with a specialized MoE based on [Mixtral](./mixtral) with additional shared\nexperts.\n\nOther models from the family can be found at [Ernie 4.5](./ernie4_5).\n\n\n    \n\n\n## Usage Tips\n\n### Generate text\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-PT\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    dtype=torch.bfloat16,\n)\n\n# prepare the...",
            "tasks": [],
            "display_name": "Ernie 4.5 Moe"
        },
        {
            "model_name": "glm4_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\glm4_moe.md",
            "release_date": "2025-07-28",
            "transformers_date": "2025-07-21",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The [**GLM-4.5**](https://huggingface.co/papers/2508.06471) series models are foundation models designed for intelligent agents, MoE variants are documented here as Glm4Moe.\n\nGLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\n\nAs demonstrated in our comprehensive...",
            "tasks": [],
            "display_name": "Glm4Moe"
        },
        {
            "model_name": "efficientloftr",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\efficientloftr.md",
            "release_date": "2024-03-07",
            "transformers_date": "2025-07-22",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[EfficientLoFTR](https://huggingface.co/papers/2403.04765) is an efficient detector-free local feature matching method that produces semi-dense matches across images with sparse-like speed. It builds upon the original [LoFTR](https://huggingface.co/papers/2104.00680) architecture but introduces significant improvements for both efficiency and accuracy. The key innovation is an aggregated attention mechanism with adaptive token selection that makes the model ~2.5\u00d7 faster than LoFTR while achieving higher accuracy. EfficientLoFTR can even surpass state-of-the-art efficient sparse matching pipelines like [SuperPoint](./superpoint) + [LightGlue](./lightglue) in terms of speed, making it suitable for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction.\n\n> [!TIP]\n> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n>\n> Click on the EfficientLoFTR models in the right sidebar for more examples of how to apply...",
            "tasks": [],
            "display_name": "EfficientLoFTR"
        },
        {
            "model_name": "deepseek_vl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deepseek_vl.md",
            "release_date": "2024-03-08",
            "transformers_date": "2025-07-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Deepseek-VL](https://huggingface.co/papers/2403.05525) was introduced by the DeepSeek AI team. It is a vision-language model (VLM) designed to process both text and images for generating contextually relevant responses. The model leverages [LLaMA](./llama) as its text encoder, while [SigLip](./siglip) is used for encoding images.\n\nYou can find all the original Deepseek-VL checkpoints under the [DeepSeek-community](https://huggingface.co/deepseek-community) organization.\n\n> [!TIP]\n> Click on the Deepseek-VL models in the right sidebar for more examples of how to apply Deepseek-VL to different vision and language tasks.\n\nThe example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"image-text-to-text\",\n    model=\"deepseek-community/deepseek-vl-1.3b-chat\",\n    device=0,\n    dtype=torch.float16\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n       ...",
            "tasks": [],
            "display_name": "DeepseekVL"
        },
        {
            "model_name": "deepseek_vl_hybrid",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\deepseek_vl_hybrid.md",
            "release_date": "2024-03-08",
            "transformers_date": "2025-07-25",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Deepseek-VL-Hybrid](https://huggingface.co/papers/2403.05525) was introduced by the DeepSeek AI team. It is a vision-language model (VLM) designed to process both text and images for generating contextually relevant responses. The model leverages [LLaMA](./llama) as its text encoder, while [SigLip](./siglip) is used for encoding low-resolution images and [SAM (Segment Anything Model)](./sam) is incorporated to handle high-resolution image encoding, enhancing the model's ability to process fine-grained visual details. Deepseek-VL-Hybrid is a variant of Deepseek-VL that uses [SAM (Segment Anything Model)](./sam) to handle high-resolution image encoding.\n\nYou can find all the original Deepseek-VL-Hybrid checkpoints under the [DeepSeek-community](https://huggingface.co/deepseek-community) organization.\n\n> [!TIP]\n> Click on the Deepseek-VL-Hybrid models in the right sidebar for more examples of how to apply Deepseek-VL-Hybrid to different vision and language tasks.\n\nThe example below...",
            "tasks": [],
            "display_name": "DeepseekVLHybrid"
        },
        {
            "model_name": "xlstm",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xlstm.md",
            "release_date": "2024-05-07",
            "transformers_date": "2025-07-25",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The xLSTM model was proposed in [xLSTM: Extended Long Short-Term Memory](https://huggingface.co/papers/2405.04517) by Maximilian Beck*, Korbinian P\u00f6ppel*, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter and Sepp Hochreiter.\nxLSTM updates the original LSTM architecture to be competitive with Transformer models by introducing exponential gating, matrix memory expansion, and parallelizable training and ingestion.\n\nThe [7B model](https://huggingface.co/NX-AI/xLSTM-7b) variant was trained by the xLSTM team Maximilian Beck, Korbinian P\u00f6ppel, Phillip Lippe, Richard Kurle, Patrick Blies, Sebastian B\u00f6ck and Sepp Hochreiter at NXAI.\n\nThe abstract from the paper is the following:\n\n*In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they...",
            "tasks": [],
            "display_name": "xLSTM"
        },
        {
            "model_name": "evolla",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\evolla.md",
            "release_date": "2025-01-05",
            "transformers_date": "2025-07-26",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Evolla model was proposed in [Decoding the Molecular Language of Proteins with Evolla](https://doi.org/10.1101/2025.01.05.630192) by [Zhou et al.](https://doi.org/10.1101/2025.01.05.630192).\n\nEvolla is an advanced 80-billion-parameter protein-language generative model designed to decode the molecular language of proteins. It integrates information from protein sequences, structures, and user queries to generate precise and contextually nuanced insights into protein function. Trained on an unprecedented AI-generated dataset of 546 million protein question-answer pairs and 150 billion word tokens, Evolla significantly advances research in proteomics and functional genomics, providing expert-level insights and shedding light on the molecular logic encoded in proteins.\n\nThe abstract from the paper is the following:\n\n*Proteins, nature's intricate molecular machines, are the products of billions of years of evolution and play fundamental roles in sustaining life. Yet, deciphering their...",
            "tasks": [],
            "display_name": "Evolla"
        },
        {
            "model_name": "exaone4",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\exaone4.md",
            "release_date": "2025-07-15",
            "transformers_date": "2025-07-26",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "**[EXAONE 4.0](https://github.com/LG-AI-EXAONE/EXAONE-4.0)** model is the language model, which integrates a **Non-reasoning mode** and **Reasoning mode** to achieve both the excellent usability of [EXAONE 3.5](https://github.com/LG-AI-EXAONE/EXAONE-3.5) and the advanced reasoning abilities of [EXAONE Deep](https://github.com/LG-AI-EXAONE/EXAONE-Deep). To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended\nto support Spanish in addition to English and Korean.\n\nThe EXAONE 4.0 model series consists of two sizes: a mid-size **32B** model optimized for high performance, and a small-size **1.2B** model designed for on-device applications.\n\nIn the EXAONE 4.0 architecture, we apply new architectural changes compared to previous EXAONE models as below:\n\n1. **Hybrid Attention**: For the 32B model, we adopt hybrid attention scheme, which combines *Local attention (sliding window attention)* with...",
            "tasks": [],
            "display_name": "EXAONE 4"
        },
        {
            "model_name": "cohere2_vision",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\cohere2_vision.md",
            "release_date": "2025-07-31",
            "transformers_date": "2025-07-31",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "Command A Vision ([blog post](https://cohere.com/blog/command-a-vision)) is a state-of-the-art multimodal model designed to seamlessly integrate visual and textual information for a wide range of applications. By combining advanced computer vision techniques with natural language processing capabilities, Command A Vision enables users to analyze, understand, and generate insights from both visual and textual data.\n\nThe model excels at tasks including image captioning, visual question answering, document understanding, and chart understanding. This makes it a versatile tool for AI practitioners. Its ability to process complex visual and textual inputs makes it useful in settings where text-only representations are imprecise or unavailable, like real-world image understanding and graphics-heavy document processing.\n\nCommand A Vision is built upon a robust architecture that leverages the latest advancements in VLMs. It's highly performant and efficient, even when dealing with large-scale...",
            "tasks": [],
            "display_name": "Command A Vision"
        },
        {
            "model_name": "mm-grounding-dino",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\mm-grounding-dino.md",
            "release_date": "2024-01-04",
            "transformers_date": "2025-08-01",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[MM Grounding DINO](https://huggingface.co/papers/2401.02361) model was proposed in [An Open and Comprehensive Pipeline for Unified Object Grounding and Detection](https://huggingface.co/papers/2401.02361) by Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang>.\n\nMM Grounding DINO improves upon the [Grounding DINO](https://huggingface.co/docs/transformers/model_doc/grounding-dino) by improving the contrastive class head and removing the parameter sharing in the decoder, improving zero-shot detection performance on both COCO (50.6(+2.2) AP) and LVIS (31.9(+11.8) val AP and 41.4(+12.6) minival AP).\n\nYou can find all the original MM Grounding DINO checkpoints under the [MM Grounding DINO](https://huggingface.co/collections/openmmlab-community/mm-grounding-dino-688cbde05b814c4e2832f9df) collection. This model also supports LLMDet inference. You can find LLMDet checkpoints under the...",
            "tasks": [],
            "display_name": "MM Grounding DINO"
        },
        {
            "model_name": "gpt_oss",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\gpt_oss.md",
            "release_date": "2025-08-05",
            "transformers_date": "2025-08-05",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The GptOss model was proposed in [blog post](https://openai.com/index/introducing-gpt-oss/) by .\n\n\nThe abstract from the paper is the following:\n\n**\n\nTips:\n- **Attention Sinks with Flex Attention**: When using flex attention, attention sinks require special handling. Unlike with standard attention implementations where sinks can be added directly to attention scores, flex attention `score_mod` function operates on individual score elements rather than the full attention matrix. Therefore, attention sinks renormalization have to be applied after the flex attention computations by renormalizing the outputs using the log-sum-exp (LSE) values returned by flex attention.\n\n\n\n\nThis model was contributed by [INSERT YOUR HF USERNAME HERE](https://huggingface.co/).\nThe original code can be found [here]().\n\n## GptOssConfig\n\n[[autodoc]] GptOssConfig\n\n## GptOssModel\n\n[[autodoc]] GptOssModel\n    - forward\n\n## GptOssForCausalLM\n\n[[autodoc]] GptOssForCausalLM\n    - forward\n\n##...",
            "tasks": [],
            "display_name": "GptOss"
        },
        {
            "model_name": "glm4v_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\glm4v_moe.md",
            "release_date": "2025-07-28",
            "transformers_date": "2025-08-08",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "Vision-language models (VLMs) have become a key cornerstone of intelligent systems. As real-world AI tasks grow increasingly complex, VLMs urgently need to enhance reasoning capabilities beyond basic multimodal perception \u2014 improving accuracy, comprehensiveness, and intelligence \u2014 to enable complex problem solving, long-context understanding, and multimodal agents.\n\nThrough our open-source work, we aim to explore the technological frontier together with the community while empowering more developers to create exciting and innovative applications.\n\n[GLM-4.5V](https://huggingface.co/papers/2508.06471) ([Github repo](https://github.com/zai-org/GLM-V)) is based on ZhipuAI\u2019s next-generation flagship text foundation model GLM-4.5-Air (106B parameters, 12B active).  It continues the technical approach of [GLM-4.1V-Thinking](https://huggingface.co/papers/2507.01006), achieving SOTA performance among models of the same scale on 42 public vision-language benchmarks.  It covers common tasks such...",
            "tasks": [],
            "display_name": "Glm4vMoe"
        },
        {
            "model_name": "dinov3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\dinov3.md",
            "release_date": "2025-08-13",
            "transformers_date": "2025-08-14",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "[DINOv3](https://huggingface.co/papers/2508.10104) is a family of versatile vision foundation models that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models.\n\nYou can find all the original DINOv3 checkpoints under the [DINOv3](https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009) collection.\n\n> [!TIP]\n> Click on the DINOv3 models in the right sidebar for more examples of how to apply DINOv3 to different vision tasks.\n\nThe example below demonstrates how to obtain an image embedding with [`Pipeline`] or the [`AutoModel`] class.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"image-feature-extraction\",\n    model=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\n   ...",
            "tasks": [],
            "display_name": "DINOv3"
        },
        {
            "model_name": "sam2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\sam2.md",
            "release_date": "2024-07-29",
            "transformers_date": "2025-08-14",
            "modality": "vision",
            "modality_name": "Vision Models",
            "modality_color": "#06B6D4",
            "description": "SAM2 (Segment Anything Model 2) was proposed in [Segment Anything in Images and Videos](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/) by Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R\u00e4dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll\u00e1r, Christoph Feichtenhofer.\n\nThe model can be used to predict segmentation masks of any object of interest given an input image or video, and input points or bounding boxes.\n\n![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam2_header.gif)\n\nThe abstract from the paper is the following:\n\n*We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction,...",
            "tasks": [],
            "display_name": "SAM2"
        },
        {
            "model_name": "sam2_video",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\sam2_video.md",
            "release_date": "2024-07-29",
            "transformers_date": "2025-08-14",
            "modality": "video",
            "modality_name": "Video Models",
            "modality_color": "#EC4899",
            "description": "SAM2 (Segment Anything Model 2) was proposed in [Segment Anything in Images and Videos](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/) by Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R\u00e4dle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Doll\u00e1r, Christoph Feichtenhofer.\n\nThe model can be used to predict segmentation masks of any object of interest given an input image or video, and input points or bounding boxes.\n\n![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam2_header.gif)\n\nThe abstract from the paper is the following:\n\n*We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction,...",
            "tasks": [],
            "display_name": "SAM2 Video"
        },
        {
            "model_name": "xcodec",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\xcodec.md",
            "release_date": "2024-08-30",
            "transformers_date": "2025-08-15",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "The X-Codec model was proposed in [Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model](https://huggingface.co/papers/2408.17175) by Zhen Ye,\u00a0Peiwen Sun,\u00a0Jiahe Lei,\u00a0Hongzhan Lin,\u00a0Xu Tan,\u00a0Zheqi Dai,\u00a0Qiuqiang Kong,\u00a0Jianyi Chen,\u00a0Jiahao Pan,\u00a0Qifeng Liu,\u00a0Yike Guo,\u00a0Wei Xue.\n\nThe X-Codec model is a neural audio codec that integrates semantic information from self-supervised models (e.g., HuBERT) alongside traditional acoustic information. This enables:\n\n- **Music continuation**: Better modeling of musical semantics yields more coherent continuations.\n- **Text-to-Sound Synthesis**: X-Codec captures semantic alignment between text prompts and generated audio.\n- **Semantic aware audio tokenization**: X-Codec is used as an audio tokenizer in the YuE lyrics to song generation model.\n\nThe abstract of the paper states the following:\n\n*Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The...",
            "tasks": [],
            "display_name": "X-Codec"
        },
        {
            "model_name": "ovis2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ovis2.md",
            "release_date": "2024-05-31",
            "transformers_date": "2025-08-18",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The [Ovis2](https://github.com/AIDC-AI/Ovis) is an updated version of the [Ovis](https://huggingface.co/papers/2405.20797) model developed by the AIDC-AI team at Alibaba International Digital Commerce Group.\n\nOvis2 is the latest advancement in multi-modal large language models (MLLMs), succeeding Ovis1.6. It retains the architectural design of the Ovis series, which focuses on aligning visual and textual embeddings, and introduces major improvements in data curation and training methods.\n\n\n\n Ovis2 architecture.\n\nThis model was contributed by [thisisiron](https://huggingface.co/thisisiron).\n\n## Usage example\n\n```python\n\nfrom PIL import Image\nimport requests\nimport torch\nfrom torchvision import io\nfrom typing import Dict\nfrom transformers.image_utils import load_images, load_video\nfrom transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor, infer_device\n\ndevice = f\"{infer_device()}:0\"\n\nmodel = AutoModelForVision2Seq.from_pretrained(\n    \"thisisiron/Ovis2-2B-hf\",\n   ...",
            "tasks": [],
            "display_name": "Ovis2"
        },
        {
            "model_name": "kosmos2_5",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\kosmos2_5.md",
            "release_date": "2023-09-20",
            "transformers_date": "2025-08-19",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Kosmos-2.5 model was proposed in [KOSMOS-2.5: A Multimodal Literate Model](https://huggingface.co/papers/2309.11419/) by Microsoft.\n\nThe abstract from the paper is the following:\n\n*We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with...",
            "tasks": [],
            "display_name": "KOSMOS-2.5"
        },
        {
            "model_name": "florence2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\florence2.md",
            "release_date": "2024-06-16",
            "transformers_date": "2025-08-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Florence-2](https://huggingface.co/papers/2311.06242) is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages the FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\n\nYou can find all the original Florence-2 checkpoints under the [Florence-2](https://huggingface.co/models?other=florence-2) collection.\n\n> [!TIP]\n> This model was contributed by [ducviet00](https://huggingface.co/ducviet00).\n> Click on the Florence-2 models in the right sidebar for more examples of how to apply Florence-2 to different vision and language tasks.\n\nThe example below demonstrates how to...",
            "tasks": [],
            "display_name": "Florence-2"
        },
        {
            "model_name": "metaclip_2",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\metaclip_2.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-08-20",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "MetaCLIP 2 is a replication of the original CLIP model trained on 300+ languages. It achieves state-of-the-art (SOTA) results on multilingual benchmarks (e.g., XM3600, CVQA, Babel\u2011ImageNet), surpassing previous SOTA such as [mSigLIP](siglip) and [SigLIP\u20112](siglip2). The authors show that English and non-English worlds can mutually benefit and elevate each other.\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/facebookresearch/MetaCLIP).\n\nYou can find all the MetaCLIP 2 checkpoints under the [Meta](https://huggingface.co/facebook/models?search=metaclip-2) organization.\n\n> [!TIP]\n> Click on the MetaCLIP 2 models in the right sidebar for more examples of how to apply MetaCLIP 2 to different image and language tasks.\n\nThe example below demonstrates how to calculate similarity scores between multiple text descriptions and an image with [`Pipeline`] or the [`AutoModel`] class. Usage of the MetaCLIP 2 models is...",
            "tasks": [],
            "display_name": "MetaCLIP 2"
        },
        {
            "model_name": "hunyuan_v1_dense",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\hunyuan_v1_dense.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-08-22",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[[autodoc]] HunYuanDenseV1ForSequenceClassification\n    - forward",
            "tasks": [],
            "display_name": "HunYuanDenseV1"
        },
        {
            "model_name": "hunyuan_v1_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\hunyuan_v1_moe.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-08-22",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-22.*\n\n# HunYuanMoEV1\n\n## Overview\n\nTo be released with the official model launch.\n\n### Model Details\n\nTo be released with the official model launch.\n\n## Usage tips\n\nTo be released with the official model launch.\n\n## HunYuanMoEV1Config\n\n[[autodoc]] HunYuanMoEV1Config\n\n## HunYuanMoEV1Model\n\n[[autodoc]] HunYuanMoEV1Model\n    - forward\n\n## HunYuanMoEV1ForCausalLM\n\n[[autodoc]] HunYuanMoEV1ForCausalLM\n    - forward\n\n## HunYuanMoEV1ForSequenceClassification\n\n[[autodoc]] HunYuanMoEV1ForSequenceClassification\n    - forward",
            "tasks": [],
            "display_name": "HunYuanMoEV1"
        },
        {
            "model_name": "seed_oss",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\seed_oss.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-08-22",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "*This model was released on {release_date} and added to Hugging Face Transformers on 2025-08-22.*\n\n# SeedOss\n\n## Overview\n\nTo be released with the official model launch.\n\n### Model Details\n\nTo be released with the official model launch.\n\n## Usage tips\n\nTo be released with the official model launch.\n\n## SeedOssConfig\n\n[[autodoc]] SeedOssConfig\n\n## SeedOssModel\n\n[[autodoc]] SeedOssModel\n    - forward\n\n## SeedOssForCausalLM\n\n[[autodoc]] SeedOssForCausalLM\n    - forward\n\n## SeedOssForSequenceClassification\n\n[[autodoc]] SeedOssForSequenceClassification\n    - forward\n\n## SeedOssForTokenClassification\n\n[[autodoc]] SeedOssForTokenClassification\n    - forward\n\n## SeedOssForQuestionAnswering\n\n[[autodoc]] SeedOssForQuestionAnswering\n    - forward",
            "tasks": [],
            "display_name": "SeedOss"
        },
        {
            "model_name": "apertus",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\apertus.md",
            "release_date": "2025-09-02",
            "transformers_date": "2025-08-28",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Apertus](https://www.swiss-ai.org) is a family of large language models from the Swiss AI Initiative.\n\n> [!TIP]\n> Coming soon\n\nThe example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"text-generation\",\n    model=\"swiss-ai/Apertus-8B\",\n    dtype=torch.bfloat16,\n    device=0\n)\npipeline(\"Plants create energy through a process known as\")\n```\n\n\n\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"swiss-ai/Apertus-8B\",\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"swiss-ai/Apertus-8B\",\n    dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation=\"sdpa\"\n)\ninput_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n\noutput = model.generate(**input_ids)\nprint(tokenizer.decode(output[0],...",
            "tasks": [],
            "display_name": "Apertus"
        },
        {
            "model_name": "qwen3_next",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen3_next.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-09-10",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency.\nThe series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\n\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.\n- **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers \u2014 drastically reducing FLOPs per token while preserving model capacity.\n- **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference.\n- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.\n\nBuilt on this architecture, we trained and open-sourced Qwen3-Next-80B-A3B \u2014 80B total parameters, only 3B active \u2014...",
            "tasks": [],
            "display_name": "Qwen3 Next"
        },
        {
            "model_name": "ministral",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\ministral.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-09-11",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[Ministral](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) is a 8B parameter language model that extends the Mistral architecture with alternating attention pattern. Unlike Mistral, that uses either full attention or sliding window attention consistently, Ministral alternates between full attention and sliding window attention layers, in a pattern of 1 full attention layer followed by 3 sliding window attention layers. This allows for a 128K context length support.\n\nThis architecture turns out to coincide with Qwen2, with the main difference being the presence of biases in attention projections in Ministral.\n\nYou can find the Ministral checkpoints under the [Mistral AI](https://huggingface.co/mistralai) organization.\n\n## Usage\n\nThe example below demonstrates how to use Ministral for text generation:\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n\n>>> model =...",
            "tasks": [],
            "display_name": "Ministral"
        },
        {
            "model_name": "vaultgemma",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\vaultgemma.md",
            "release_date": "2016-07-01",
            "transformers_date": "2025-09-12",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[VaultGemma](https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf) is a text-only decoder model\nderived from [Gemma 2](https://huggingface.co/docs/transformers/en/model_doc/gemma2), notably it drops the norms after\nthe Attention and MLP blocks, and uses full attention for all layers instead of alternating between full attention and\nlocal sliding attention. VaultGemma is available as a pretrained model with 1B parameters that uses a 1024 token\nsequence length.\n\nVaultGemma was trained from scratch with sequence-level differential privacy (DP). Its training data includes the same\nmixture as the [Gemma 2 models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315),\nconsisting of a number of documents of varying lengths. Additionally, it is trained using\n[DP stochastic gradient descent (DP-SGD)](https://huggingface.co/papers/1607.00133) and provides a\n(\u03b5 \u2264 2.0, \u03b4 \u2264 1.1e-10)-sequence-level DP guarantee, where a sequence consists of 1024...",
            "tasks": [],
            "display_name": "VaultGemma"
        },
        {
            "model_name": "qwen3_vl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen3_vl.md",
            "release_date": "2025-09-23",
            "transformers_date": "2025-09-15",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. Building upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignment\u2014evolving from T-RoPE to text timestamp alignment for more precise temporal grounding. These innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\n\nModel usage\n\n\n\n\n```py\nimport torch\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-VL\",\n...",
            "tasks": [],
            "display_name": "Qwen3-VL"
        },
        {
            "model_name": "qwen3_vl_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen3_vl_moe.md",
            "release_date": "2025-02-19",
            "transformers_date": "2025-09-15",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. Building upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignment\u2014evolving from T-RoPE to text timestamp alignment for more precise temporal grounding. These innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\n\nModel usage\n\n\n\n\n```py\nimport torch\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n   ...",
            "tasks": [],
            "display_name": "Qwen3-VL-Moe"
        },
        {
            "model_name": "olmo3",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\olmo3.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-09-16",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "Olmo3 is an improvement on [OLMo2](./olmo2). More details will be released on *soon*.\n\n> [!TIP]\n> Click on the OLMo3 models in the right sidebar for more examples of how to apply OLMo3 to different language tasks.\n\nThe example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n\n\n\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    task=\"text-generation\",\n    model=\"allenai/TBA\",\n    dtype=torch.bfloat16,\n    device=0,\n)\n\nresult = pipe(\"Plants create energy through a process known as\")\nprint(result)\n```\n\n\n\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"allenai/TBA\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"allenai/TBA\",\n    dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation=\"sdpa\"\n)\ninput_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n\noutput =...",
            "tasks": [],
            "display_name": "OLMo3"
        },
        {
            "model_name": "longcat_flash",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\longcat_flash.md",
            "release_date": "2025-09-01",
            "transformers_date": "2025-09-17",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The LongCatFlash model was proposed in [LongCat-Flash Technical Report](https://huggingface.co/papers/2509.01322) by the Meituan LongCat Team.\nLongCat-Flash is a 560B parameter Mixture-of-Experts (MoE) model that activates 18.6B-31.3B parameters dynamically (average ~27B). The model features a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and advanced reasoning capabilities.\n\nThe abstract from the paper is the following:\n\n*We present LongCat-Flash, a 560 billion parameter Mixture-of-Experts (MoE) language model featuring a dynamic computation mechanism that activates 18.6B-31.3B parameters based on context (average ~27B). The model incorporates a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and demonstrates strong performance across multiple benchmarks including 89.71% accuracy on MMLU and exceptional agentic tool use capabilities.*\n\nTips:\n\n- LongCat-Flash uses a unique shortcut-connected MoE architecture that...",
            "tasks": [],
            "display_name": "LongCatFlash"
        },
        {
            "model_name": "flex_olmo",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\flex_olmo.md",
            "release_date": "2025-07-09",
            "transformers_date": "2025-09-18",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets.\n\nYou can find all the original FlexOlmo checkpoints under the [FlexOlmo](https://huggingface.co/collections/allenai/flexolmo-68471177a386b6e20a54c55f) collection.\n\n> [!TIP]\n> Click on the FlexOlmo models...",
            "tasks": [],
            "display_name": "FlexOlmo"
        },
        {
            "model_name": "lfm2_vl",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\lfm2_vl.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-09-18",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "[LFM2-VL](https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models) first series of vision-language foundation models developed by [Liquid AI](https://liquid.ai/). These multimodal models are designed for low-latency and device-aware deployment. LFM2-VL extends the LFM2 family of open-weight Liquid Foundation Models (LFMs) into the vision-language space, supporting both text and image inputs with variable resolutions.\n\n## Architecture\n\nLFM2-VL consists of three main components: a language model backbone, a vision encoder, and a multimodal projector. LFM2-VL builds upon the LFM2 backbone, inheriting from either LFM2-1.2B (for LFM2-VL-1.6B) or LFM2-350M (for LFM2-VL-450M). For the vision tower, LFM2-VL uses SigLIP2 NaFlex encoders to convert input images into token sequences. Two variants are implemented:\n\n* Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\n* Base (86M) for fast image processing for LFM2-VL-450M\n\nThe encoder processes images at...",
            "tasks": [],
            "display_name": "LFM2-VL"
        },
        {
            "model_name": "blt",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\blt.md",
            "release_date": "2024-12-13",
            "transformers_date": "2025-09-19",
            "modality": "text",
            "modality_name": "Text Models",
            "modality_color": "#F59E0B",
            "description": "The BLT model was proposed in [Byte Latent Transformer: Patches Scale Better Than Tokens](https://huggingface.co/papers/2412.09871) by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman\u2020, Srinivasan Iyer.\nBLT is a byte-level LLM that achieves tokenization-level performance through entropy-based dynamic patching.\n\nThe abstract from the paper is the following:\n\n*We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference\nefficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating\nmore compute and model capacity where increased data complexity demands it. We present the first...",
            "tasks": [],
            "display_name": "Byte Lantet Transformer (BLT)"
        },
        {
            "model_name": "qwen3_omni_moe",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\qwen3_omni_moe.md",
            "release_date": "2025-03-26",
            "transformers_date": "2025-09-21",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The Qwen3-Omni-MOE model is a unified multiple modalities model proposed in [Qwen3-Omni Technical Report](https://huggingface.co/papers/2509.17765) from Qwen team, Alibaba Group.\n\nThe abstract from the technical report is the following:\n\n*We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports...",
            "tasks": [],
            "display_name": "Qwen3-Omni-MOE"
        },
        {
            "model_name": "parakeet",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\parakeet.md",
            "release_date": "{release_date}",
            "transformers_date": "2025-09-25",
            "modality": "audio",
            "modality_name": "Audio Models",
            "modality_color": "#8B5CF6",
            "description": "Parakeet models, [introduced by NVIDIA NeMo](https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/), are models that combine a [Fast Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#fast-conformer) encoder with connectionist temporal classification (CTC), recurrent neural network transducer (RNNT) or token and duration transducer (TDT) decoder for automatic speech recognition.\n\n**Model Architecture**\n\n- **Fast Conformer Encoder**: A linearly scalable Conformer architecture that processes mel-spectrogram features and reduces sequence length through subsampling. This is more efficient version of the Conformer Encoder found in [FastSpeech2Conformer](./fastspeech2_conformer.md) (see [`ParakeetEncoder`] for the encoder implementation and details).\n- [**ParakeetForCTC**](#parakeetforctc): a Fast Conformer Encoder + a CTC decoder\n  - **CTC Decoder**: Simple but effective decoder consisting...",
            "tasks": [],
            "display_name": "Parakeet"
        },
        {
            "model_name": "edgetam",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\edgetam.md",
            "release_date": "2025-01-13",
            "transformers_date": "2025-09-29",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The EdgeTAM model was proposed in [EdgeTAM: On-Device Track Anything Model](https://huggingface.co/papers/2501.07256) Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran.\n\nEdgeTAM is an efficient adaptation of SAM 2 that introduces a 2D Spatial Perceiver architecture to optimize memory attention mechanisms for real-time video segmentation on mobile devices.\n\nThe abstract from the paper is the following:\n\n*On top of Segment Anything Model (SAM), SAM 2 further extends its capability from image to video inputs through a memory bank mechanism and obtains a remarkable performance compared with previous methods, making it a foundation model for video segmentation task. In this paper, we aim at making SAM 2 much more efficient so that it even runs on mobile devices while maintaining a comparable performance. Despite several works optimizing SAM for better efficiency, we find they are not...",
            "tasks": [],
            "display_name": "EdgeTAM"
        },
        {
            "model_name": "edgetam_video",
            "file_path": "F:\\Online class\\Reinvent\\Phase 7\\Transformer timeline vis\\data\\transformers_repo\\transformers\\docs\\source\\en\\model_doc\\edgetam_video.md",
            "release_date": "2025-01-13",
            "transformers_date": "2025-09-29",
            "modality": "multimodal",
            "modality_name": "Multimodal Models",
            "modality_color": "#10B981",
            "description": "The EdgeTAM model was proposed in [EdgeTAM: On-Device Track Anything Model](https://huggingface.co/papers/2501.07256) Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran.\n\nEdgeTAM is an efficient adaptation of SAM 2 that introduces a 2D Spatial Perceiver architecture to optimize memory attention mechanisms for real-time video segmentation on mobile devices.\n\nThe abstract from the paper is the following:\n\n*On top of Segment Anything Model (SAM), SAM 2 further extends its capability from image to video inputs through a memory bank mechanism and obtains a remarkable performance compared with previous methods, making it a foundation model for video segmentation task. In this paper, we aim at making SAM 2 much more efficient so that it even runs on mobile devices while maintaining a comparable performance. Despite several works optimizing SAM for better efficiency, we find they are not...",
            "tasks": [],
            "display_name": "EdgeTAMVideo"
        }
    ],
    "total_count": 414
}